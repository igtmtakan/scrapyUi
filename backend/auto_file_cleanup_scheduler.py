#!/usr/bin/env python3
"""
è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼
å®šæœŸçš„ã«JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã€ã‚·ã‚¹ãƒ†ãƒ ã®æ€§èƒ½ã‚’ç¶­æŒ
"""
import os
import sys
import time
import schedule
import logging
import subprocess
from datetime import datetime, timedelta
from pathlib import Path
import glob

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('auto_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutoFileCleanupScheduler:
    """è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self):
        self.scrapyui_root = '/home/igtmtakan/workplace/python/scrapyUI'
        self.backend_path = os.path.join(self.scrapyui_root, 'backend')
        self.projects_path = os.path.join(self.scrapyui_root, 'scrapy_projects')
        self.tool_path = os.path.join(self.backend_path, 'jsonl_file_manager.py')
        
        # è¨­å®šï¼ˆæ¥µã‚ã¦ç©æ¥µçš„ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
        self.max_file_lines = 500    # æœ€å¤§è¡Œæ•°ï¼ˆæ¥µã‚ã¦å°ã•ãï¼‰
        self.keep_sessions = 1       # ä¿æŒã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°ï¼ˆæœ€æ–°ã®ã¿ï¼‰
        self.max_file_age_days = 30  # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¤§ä¿å­˜æ—¥æ•°
        self.cleanup_interval_hours = 1  # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—é–“éš”ï¼ˆ1æ™‚é–“æ¯ï¼‰
        
        logger.info(f"ğŸ¤– Auto File Cleanup Scheduler initialized")
        logger.info(f"   ğŸ“ Projects path: {self.projects_path}")
        logger.info(f"   ğŸ”§ Tool path: {self.tool_path}")
        logger.info(f"   ğŸ“Š Max lines: {self.max_file_lines:,}")
        logger.info(f"   ğŸ“… Keep sessions: {self.keep_sessions}")
        logger.info(f"   â° Cleanup interval: {self.cleanup_interval_hours}h")

    def find_jsonl_files(self):
        """ã™ã¹ã¦ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢"""
        jsonl_files = []
        
        try:
            # scrapy_projectsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            pattern = os.path.join(self.projects_path, "**", "*.jsonl")
            files = glob.glob(pattern, recursive=True)
            
            for file_path in files:
                try:
                    stat = os.stat(file_path)
                    line_count = self._count_file_lines(file_path)
                    
                    jsonl_files.append({
                        'path': file_path,
                        'size': stat.st_size,
                        'lines': line_count,
                        'modified': datetime.fromtimestamp(stat.st_mtime),
                        'age_days': (datetime.now() - datetime.fromtimestamp(stat.st_mtime)).days
                    })
                except Exception as e:
                    logger.warning(f"âš ï¸ Error analyzing {file_path}: {e}")
                    
            logger.info(f"ğŸ“„ Found {len(jsonl_files)} JSONL files")
            return jsonl_files
            
        except Exception as e:
            logger.error(f"âŒ Error finding JSONL files: {e}")
            return []

    def _count_file_lines(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’åŠ¹ç‡çš„ã«ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return sum(1 for _ in f)
        except Exception:
            return 0

    def analyze_files(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        files = self.find_jsonl_files()
        
        if not files:
            logger.info("ğŸ“„ No JSONL files found")
            return
        
        total_size = sum(f['size'] for f in files)
        total_lines = sum(f['lines'] for f in files)
        large_files = [f for f in files if f['lines'] > self.max_file_lines]
        old_files = [f for f in files if f['age_days'] > self.max_file_age_days]
        
        logger.info(f"ğŸ“Š File Analysis Report:")
        logger.info(f"   ğŸ“„ Total files: {len(files)}")
        logger.info(f"   ğŸ“ Total lines: {total_lines:,}")
        logger.info(f"   ğŸ’¾ Total size: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)")
        logger.info(f"   ğŸ“ˆ Large files (>{self.max_file_lines:,} lines): {len(large_files)}")
        logger.info(f"   ğŸ“… Old files (>{self.max_file_age_days} days): {len(old_files)}")
        
        if large_files:
            logger.info(f"ğŸ” Large files details:")
            for f in large_files[:5]:  # æœ€å¤§5ã¤ã¾ã§è¡¨ç¤º
                logger.info(f"   ğŸ“„ {f['path']}: {f['lines']:,} lines, {f['size']:,} bytes")

    def cleanup_large_files(self):
        """å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        files = self.find_jsonl_files()
        large_files = [f for f in files if f['lines'] > self.max_file_lines]
        
        if not large_files:
            logger.info("âœ… No large files to cleanup")
            return
        
        logger.info(f"ğŸ§¹ Starting cleanup of {len(large_files)} large files")
        
        for file_info in large_files:
            try:
                self._cleanup_file(file_info['path'])
            except Exception as e:
                logger.error(f"âŒ Error cleaning up {file_info['path']}: {e}")

    def _cleanup_file(self, file_path):
        """å€‹åˆ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if not os.path.exists(self.tool_path):
                logger.error(f"âŒ Cleanup tool not found: {self.tool_path}")
                return
            
            cmd = [
                sys.executable, self.tool_path, file_path,
                '--clean', '--keep-sessions', str(self.keep_sessions)
            ]
            
            logger.info(f"ğŸ”§ Cleaning up: {file_path}")
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
            
            if result.returncode == 0:
                logger.info(f"âœ… Successfully cleaned up: {file_path}")
                # çµæœã®è©³ç´°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²
                for line in result.stdout.split('\n'):
                    if 'Reduced from' in line or 'items' in line:
                        logger.info(f"   ğŸ“Š {line.strip()}")
            else:
                logger.error(f"âŒ Cleanup failed for {file_path}")
                logger.error(f"   Error: {result.stderr}")
                
        except subprocess.TimeoutExpired:
            logger.error(f"â° Cleanup timeout for {file_path}")
        except Exception as e:
            logger.error(f"âŒ Error cleaning up {file_path}: {e}")

    def remove_old_files(self):
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤"""
        files = self.find_jsonl_files()
        old_files = [f for f in files if f['age_days'] > self.max_file_age_days]
        
        if not old_files:
            logger.info("âœ… No old files to remove")
            return
        
        logger.info(f"ğŸ—‘ï¸ Removing {len(old_files)} old files")
        
        for file_info in old_files:
            try:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‰Šé™¤ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã¯ä¿æŒï¼‰
                if 'backup_' in file_info['path']:
                    os.remove(file_info['path'])
                    logger.info(f"ğŸ—‘ï¸ Removed old backup: {file_info['path']}")
                else:
                    logger.info(f"âš ï¸ Skipping main file: {file_info['path']}")
                    
            except Exception as e:
                logger.error(f"âŒ Error removing {file_info['path']}: {e}")

    def run_full_cleanup(self):
        """å®Œå…¨ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸš€ Starting full cleanup cycle")
        start_time = time.time()
        
        try:
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ
            self.analyze_files()
            
            # 2. å¤§ããªãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            self.cleanup_large_files()
            
            # 3. å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
            self.remove_old_files()
            
            elapsed = time.time() - start_time
            logger.info(f"âœ… Full cleanup completed in {elapsed:.1f}s")
            
        except Exception as e:
            logger.error(f"âŒ Error in full cleanup: {e}")

    def start_scheduler(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã‚’é–‹å§‹"""
        logger.info(f"â° Starting scheduler with {self.cleanup_interval_hours}h interval")
        
        # å³åº§ã«1å›å®Ÿè¡Œ
        self.run_full_cleanup()
        
        # å®šæœŸå®Ÿè¡Œã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«
        schedule.every(self.cleanup_interval_hours).hours.do(self.run_full_cleanup)
        
        # æ¯æ—¥åˆå‰3æ™‚ã«å®Œå…¨ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        schedule.every().day.at("03:00").do(self.run_full_cleanup)
        
        logger.info("ğŸ”„ Scheduler started. Press Ctrl+C to stop.")
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)  # 1åˆ†é–“éš”ã§ãƒã‚§ãƒƒã‚¯
        except KeyboardInterrupt:
            logger.info("â¹ï¸ Scheduler stopped by user")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Auto File Cleanup Scheduler')
    parser.add_argument('--analyze', action='store_true', help='Analyze files only')
    parser.add_argument('--cleanup', action='store_true', help='Run cleanup once')
    parser.add_argument('--start', action='store_true', help='Start scheduler')
    parser.add_argument('--max-lines', type=int, default=10000, help='Max lines per file')
    parser.add_argument('--keep-sessions', type=int, default=5, help='Sessions to keep')
    
    args = parser.parse_args()
    
    scheduler = AutoFileCleanupScheduler()
    scheduler.max_file_lines = args.max_lines
    scheduler.keep_sessions = args.keep_sessions
    
    if args.analyze:
        scheduler.analyze_files()
    elif args.cleanup:
        scheduler.run_full_cleanup()
    elif args.start:
        scheduler.start_scheduler()
    else:
        parser.print_help()

if __name__ == "__main__":
    main()
