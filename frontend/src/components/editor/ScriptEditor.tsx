'use client'

import React, { useRef, useEffect, useState } from 'react'
import Editor from '@monaco-editor/react'
import { Play, FileText, Settings, Bug, Download, FileCode, Copy, X, RotateCcw } from 'lucide-react'

interface ScriptEditorProps {
  value: string
  onChange: (value: string) => void
  language?: string
  theme?: 'vs-dark' | 'light'
  readOnly?: boolean
  onSave?: () => void
  onRun?: () => void
  onTest?: () => void
  fileName?: string
  className?: string
}

export function ScriptEditor({
  value,
  onChange,
  language = 'python',
  theme = 'vs-dark',
  readOnly = false,
  onSave,
  onRun,
  onTest,
  fileName = 'spider.py',
  className = ''
}: ScriptEditorProps) {
  // Props „Çí„É≠„Ç∞Âá∫Âäõ
  console.log('üìù ScriptEditor props:', {
    fileName,
    valueLength: value?.length || 0,
    valuePreview: value?.substring(0, 100) + (value?.length > 100 ? '...' : ''),
    isEmpty: value === '',
    language
  });
  const editorRef = useRef<any>(null)
  const [isLoading, setIsLoading] = useState(true)
  const [errors, setErrors] = useState<any[]>([])
  const [showSettingsSnippet, setShowSettingsSnippet] = useState(false)
  const [showRestoreConfirm, setShowRestoreConfirm] = useState(false)

  // „Éï„Ç°„Ç§„É´„Çø„Ç§„Éó„ÇíÂà§ÂÆö
  const getFileType = () => {
    const name = fileName.toLowerCase()
    if (name.includes('scrapy.cfg')) return 'scrapy_cfg'
    if (name.includes('__init__.py')) return 'init'
    if (name.includes('settings')) return 'settings'
    if (name.includes('items')) return 'items'
    if (name.includes('pipelines')) return 'pipelines'
    if (name.includes('middlewares')) return 'middlewares'
    return 'spider'
  }

  // „Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàêÔºà„Éï„Ç°„Ç§„É´„Çø„Ç§„Éó„Å´Âøú„Åò„Å¶Ôºâ
  const generateSnippet = () => {
    const fileType = getFileType()

    // „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂêç„ÇíÊ≠£„Åó„ÅèÂèñÂæó
    let projectName = fileName.replace('.py', '').toLowerCase()

    // scrapy.cfg„ÅÆÂ†¥Âêà„ÅØ„ÄÅURL„Åã„Çâ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂêç„ÇíÂèñÂæó
    if (fileType === 'scrapy_cfg') {
      // URL„Éë„É©„É°„Éº„Çø„Åã„ÇâprojectPath„ÇíÂèñÂæó
      const urlParams = new URLSearchParams(window.location.search)
      const projectPath = urlParams.get('projectPath')
      if (projectPath) {
        projectName = projectPath.toLowerCase()
      } else {
        // „Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ: demo_demoomochaproject
        projectName = 'demo_demoomochaproject'
      }
    }

    const capitalizedProjectName = projectName.charAt(0).toUpperCase() + projectName.slice(1)

    switch (fileType) {
      case 'scrapy_cfg':
        return generateScrapyCfgSnippet(projectName)
      case 'init':
        return generateInitSnippet(projectName)
      case 'settings':
        return generateSettingsSnippet(projectName)
      case 'items':
        return generateItemsSnippet(projectName, capitalizedProjectName)
      case 'pipelines':
        return generatePipelinesSnippet(projectName, capitalizedProjectName)
      case 'middlewares':
        return generateMiddlewaresSnippet(projectName, capitalizedProjectName)
      default:
        return generateSpiderSnippet(projectName, capitalizedProjectName)
    }
  }

  // scrapy.cfg„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateScrapyCfgSnippet = (projectName: string) => {
    return `# Automatically created by: scrapy startproject
#
# For more information about the [deploy] section see:
# https://scrapyd.readthedocs.io/en/latest/deploy.html

[settings]
default = ${projectName}.settings

[deploy]
#url = http://localhost:6800/
project = ${projectName}
`
  }

  // __init__.py„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateInitSnippet = (projectName: string) => {
    // „Éï„Ç°„Ç§„É´„Éë„Çπ„Å´Âü∫„Å•„ÅÑ„Å¶„Çπ„Éã„Éö„ÉÉ„Éà„ÇíÂà§ÂÆö
    const filePath = fileName.toLowerCase()

    if (filePath.includes('/spiders/__init__.py') || filePath.includes('\\spiders\\__init__.py')) {
      // spiders/__init__.pyÁî®„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà
      return `# This package will contain the spiders of your Scrapy project
#
# Please refer to the documentation for information on how to create and manage
# your spiders.
`
    } else {
      // „Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„ÅÆ__init__.pyÁî®„ÅÆ„Çπ„Éã„Éö„ÉÉ„ÉàÔºàÈÄöÂ∏∏„ÅØÁ©∫Ôºâ
      return `# ${projectName} package
`
    }
  }

  // settings.py„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateSettingsSnippet = (projectName: string) => {
    return `# Scrapy settings for ${projectName} project
#
# For simplicity, this file contains only settings considered important or
# commonly used. You can find more settings consulting the documentation:
#
#     https://docs.scrapy.org/en/latest/topics/settings.html
#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html

BOT_NAME = '${projectName}'

SPIDER_MODULES = ['${projectName}.spiders']
NEWSPIDER_MODULE = '${projectName}.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure maximum concurrent requests performed by Scrapy (default: 16)
#CONCURRENT_REQUESTS = 32

# Configure a delay for requests for the same website (default: 0)
# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay
# See also autothrottle settings and docs
#DOWNLOAD_DELAY = 3
# The download delay setting will honor only one of:
#CONCURRENT_REQUESTS_PER_DOMAIN = 16
#CONCURRENT_REQUESTS_PER_IP = 16

# Disable cookies (enabled by default)
#COOKIES_ENABLED = False

# Disable Telnet Console (enabled by default)
#TELNETCONSOLE_ENABLED = False

# Override the default request headers:
#DEFAULT_REQUEST_HEADERS = {
#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#   'Accept-Language': 'en',
#}

# Enable or disable spider middlewares
# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html
#SPIDER_MIDDLEWARES = {
#    '${projectName}.middlewares.${projectName.charAt(0).toUpperCase() + projectName.slice(1)}SpiderMiddleware': 543,
#}

# Enable or disable downloader middlewares
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html
#DOWNLOADER_MIDDLEWARES = {
#    '${projectName}.middlewares.${projectName.charAt(0).toUpperCase() + projectName.slice(1)}DownloaderMiddleware': 543,
#}

# Enable or disable extensions
# See https://docs.scrapy.org/en/latest/topics/extensions.html
#EXTENSIONS = {
#    'scrapy.extensions.telnet.TelnetConsole': None,
#}

# Configure item pipelines
# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html
#ITEM_PIPELINES = {
#    '${projectName}.pipelines.${projectName.charAt(0).toUpperCase() + projectName.slice(1)}Pipeline': 300,
#}

# Enable and configure the AutoThrottle extension (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/autothrottle.html
#AUTOTHROTTLE_ENABLED = True
# The initial download delay
#AUTOTHROTTLE_START_DELAY = 1
# The maximum download delay to be set in case of high latencies
#AUTOTHROTTLE_MAX_DELAY = 60
# The average number of requests Scrapy should be sending in parallel to
# each remote server
#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
# Enable showing throttling stats for every response received:
#AUTOTHROTTLE_DEBUG = False

# Enable and configure HTTP caching (disabled by default)
# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
#HTTPCACHE_ENABLED = True
#HTTPCACHE_EXPIRATION_SECS = 0
#HTTPCACHE_DIR = 'httpcache'
#HTTPCACHE_IGNORE_HTTP_CODES = []
#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'

# Set settings whose default value is deprecated to a future-proof value
FEED_EXPORT_ENCODING = 'utf-8'

# Custom settings (commented out for standard Scrapy configuration)
# DOWNLOAD_DELAY = 1
# RANDOMIZE_DOWNLOAD_DELAY = 0.5
# CONCURRENT_REQUESTS = 16
# CONCURRENT_REQUESTS_PER_DOMAIN = 8
# CONCURRENT_REQUESTS_PER_IP = 16
# COOKIES_ENABLED = True
# TELNETCONSOLE_ENABLED = False
# RETRY_ENABLED = True
# RETRY_TIMES = 2
# RETRY_HTTP_CODES = [500, 502, 503, 504, 408, 429]
# DEFAULT_REQUEST_HEADERS = {
#     'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
#     'Accept-Language': 'ja',
#     'User-Agent': '${projectName} (+http://www.yourdomain.com)',
# }
# HTTPCACHE_ENABLED = True
# HTTPCACHE_DIR = 'httpcache'
# HTTPCACHE_EXPIRATION_SECS = 86400  # 1 day

# ========================================
# Scrapy-Playwright Settings (Optional)
# ========================================
# Uncomment the following settings to enable Playwright support

# Download handlers for Playwright
#DOWNLOAD_HANDLERS = {
#    'http': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
#    'https': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
#}

# Playwright browser configuration
#PLAYWRIGHT_BROWSER_TYPE = 'chromium'  # Options: 'chromium', 'firefox', 'webkit'
#PLAYWRIGHT_LAUNCH_OPTIONS = {
#    'headless': True,
#    'args': ['--no-sandbox', '--disable-dev-shm-usage']
#}

# Playwright page configuration
#PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT = 30000  # 30 seconds
#PLAYWRIGHT_ABORT_REQUEST = lambda req: req.resource_type in ['image', 'stylesheet', 'font']

# Playwright context configuration
#PLAYWRIGHT_CONTEXTS = {
#    'default': {
#        'viewport': {'width': 1920, 'height': 1080},
#        'user_agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
#    },
#    'mobile': {
#        'viewport': {'width': 375, 'height': 667},
#        'user_agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15'
#    }
#}

# Playwright process configuration
#PLAYWRIGHT_PROCESS_REQUEST_TIMEOUT = 30
#PLAYWRIGHT_MAX_CONTEXTS = 15
#PLAYWRIGHT_MAX_PAGES_PER_CONTEXT = 1

# Proxy settings (optional - configure as needed)
# DOWNLOADER_MIDDLEWARES = {
#     'scrapy_proxies.RandomProxy': 350,
# }`
  }

  // items.py„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateItemsSnippet = (projectName: string, capitalizedProjectName: string) => {
    return `# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy
from itemloaders.processors import TakeFirst, MapCompose, Join
from w3lib.html import remove_tags


def clean_text(value):
    """„ÉÜ„Ç≠„Çπ„Éà„Çí„ÇØ„É™„Éº„Éã„É≥„Ç∞„Åô„ÇãÈñ¢Êï∞"""
    if value:
        return value.strip().replace('\\n', ' ').replace('\\r', ' ')
    return value


def convert_price(value):
    """‰æ°Ê†ºÊñáÂ≠óÂàó„ÇíÊï∞ÂÄ§„Å´Â§âÊèõ„Åô„ÇãÈñ¢Êï∞"""
    if value:
        # Êï∞Â≠ó‰ª•Â§ñ„ÅÆÊñáÂ≠ó„ÇíÈô§Âéª„Åó„Å¶Êï∞ÂÄ§„Å´Â§âÊèõ
        import re
        price_str = re.sub(r'[^0-9.]', '', value)
        try:
            return float(price_str)
        except ValueError:
            return 0.0
    return 0.0


class ${capitalizedProjectName}Item(scrapy.Item):
    """Âü∫Êú¨ÁöÑ„Å™„Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞„Ç¢„Ç§„ÉÜ„É†"""

    # Âü∫Êú¨ÊÉÖÂ†±
    title = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    description = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    url = scrapy.Field(
        output_processor=TakeFirst()
    )

    # Êó•ÊôÇÊÉÖÂ†±
    published_date = scrapy.Field(
        output_processor=TakeFirst()
    )

    scraped_date = scrapy.Field(
        output_processor=TakeFirst()
    )


class ProductItem(${capitalizedProjectName}Item):
    """ÂïÜÂìÅÊÉÖÂ†±Áî®„ÅÆ„Ç¢„Ç§„ÉÜ„É†"""

    # ÂïÜÂìÅÂõ∫Êúâ„ÅÆÊÉÖÂ†±
    product_name = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    price = scrapy.Field(
        input_processor=MapCompose(convert_price),
        output_processor=TakeFirst()
    )

    original_price = scrapy.Field(
        input_processor=MapCompose(convert_price),
        output_processor=TakeFirst()
    )

    discount_rate = scrapy.Field(
        output_processor=TakeFirst()
    )

    availability = scrapy.Field(
        input_processor=MapCompose(clean_text),
        output_processor=TakeFirst()
    )

    brand = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    category = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    rating = scrapy.Field(
        output_processor=TakeFirst()
    )

    review_count = scrapy.Field(
        output_processor=TakeFirst()
    )

    image_urls = scrapy.Field()

    images = scrapy.Field()


class NewsItem(${capitalizedProjectName}Item):
    """„Éã„É•„Éº„ÇπË®ò‰∫ãÁî®„ÅÆ„Ç¢„Ç§„ÉÜ„É†"""

    # „Éã„É•„Éº„ÇπÂõ∫Êúâ„ÅÆÊÉÖÂ†±
    headline = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    author = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    content = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text),
        output_processor=TakeFirst()
    )

    tags = scrapy.Field(
        input_processor=MapCompose(remove_tags, clean_text)
    )

    source = scrapy.Field(
        input_processor=MapCompose(clean_text),
        output_processor=TakeFirst()
    )
`
  }

  // pipelines.py„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generatePipelinesSnippet = (projectName: string, capitalizedProjectName: string) => {
    return `# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html

import json
import sqlite3
import logging
from datetime import datetime, timezone
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem


class ${capitalizedProjectName}Pipeline:
    """Âü∫Êú¨ÁöÑ„Å™„Ç¢„Ç§„ÉÜ„É†„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def process_item(self, item, spider):
        """„Ç¢„Ç§„ÉÜ„É†„ÇíÂá¶ÁêÜ„Åô„Çã"""
        adapter = ItemAdapter(item)

        # ÂøÖÈ†à„Éï„Ç£„Éº„É´„Éâ„ÅÆÊ§úË®º
        if not adapter.get('title'):
            raise DropItem(f"Missing title in {item}")

        # „Çπ„ÇØ„É¨„Ç§„Éî„É≥„Ç∞Êó•ÊôÇ„ÇíËøΩÂä†
        adapter['scraped_date'] = datetime.now(timezone.utc).isoformat()

        return item


class ValidationPipeline:
    """„Éá„Éº„ÇøÊ§úË®º„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def process_item(self, item, spider):
        """„Ç¢„Ç§„ÉÜ„É†„ÅÆÊ§úË®º„ÇíË°å„ÅÜ"""
        adapter = ItemAdapter(item)

        # URL„ÅÆÊ§úË®º
        if adapter.get('url') and not adapter['url'].startswith(('http://', 'https://')):
            raise DropItem(f"Invalid URL in {item}")

        # „ÉÜ„Ç≠„Çπ„Éà„Éï„Ç£„Éº„É´„Éâ„ÅÆÈï∑„ÅïÂà∂Èôê
        if adapter.get('title') and len(adapter['title']) > 500:
            adapter['title'] = adapter['title'][:500] + '...'

        if adapter.get('description') and len(adapter['description']) > 2000:
            adapter['description'] = adapter['description'][:2000] + '...'

        return item


class DuplicatesPipeline:
    """ÈáçË§áÈô§Âéª„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        """ÈáçË§á„Ç¢„Ç§„ÉÜ„É†„ÇíÈô§Âéª„Åô„Çã"""
        adapter = ItemAdapter(item)

        # URL„Çí„É¶„Éã„Éº„ÇØ„Ç≠„Éº„Å®„Åó„Å¶‰ΩøÁî®
        item_id = adapter.get('url')
        if item_id in self.ids_seen:
            raise DropItem(f"Duplicate item found: {item}")
        else:
            self.ids_seen.add(item_id)
            return item


class JsonWriterPipeline:
    """JSON„Éï„Ç°„Ç§„É´Âá∫Âäõ„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def open_spider(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÈñãÂßãÊôÇ„Å´„Éï„Ç°„Ç§„É´„ÇíÈñã„Åè"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.file = open(f'{spider.name}_{timestamp}.json', 'w', encoding='utf-8')

    def close_spider(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÁµÇ‰∫ÜÊôÇ„Å´„Éï„Ç°„Ç§„É´„ÇíÈñâ„Åò„Çã"""
        self.file.close()

    def process_item(self, item, spider):
        """„Ç¢„Ç§„ÉÜ„É†„ÇíJSON„Éï„Ç°„Ç§„É´„Å´Êõ∏„ÅçËæº„ÇÄ"""
        line = json.dumps(ItemAdapter(item).asdict(), ensure_ascii=False) + "\\n"
        self.file.write(line)
        return item


class SQLitePipeline:
    """SQLite„Éá„Éº„Çø„Éô„Éº„Çπ‰øùÂ≠ò„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def __init__(self, sqlite_db):
        self.sqlite_db = sqlite_db

    @classmethod
    def from_crawler(cls, crawler):
        """Ë®≠ÂÆö„Åã„Çâ„Éá„Éº„Çø„Éô„Éº„Çπ„Éë„Çπ„ÇíÂèñÂæó"""
        db_settings = crawler.settings.getdict("DATABASE")
        if db_settings:
            sqlite_db = db_settings.get('sqlite_db', 'scrapy_data.db')
        else:
            sqlite_db = 'scrapy_data.db'
        return cls(sqlite_db=sqlite_db)

    def open_spider(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÈñãÂßãÊôÇ„Å´„Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂ö„ÇíÈñã„Åè"""
        self.connection = sqlite3.connect(self.sqlite_db)
        self.cursor = self.connection.cursor()

        # „ÉÜ„Éº„Éñ„É´‰ΩúÊàê
        self.cursor.execute('''
            CREATE TABLE IF NOT EXISTS items (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                description TEXT,
                url TEXT UNIQUE,
                scraped_date TEXT,
                spider_name TEXT
            )
        ''')
        self.connection.commit()

    def close_spider(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÁµÇ‰∫ÜÊôÇ„Å´„Éá„Éº„Çø„Éô„Éº„ÇπÊé•Á∂ö„ÇíÈñâ„Åò„Çã"""
        self.connection.close()

    def process_item(self, item, spider):
        """„Ç¢„Ç§„ÉÜ„É†„Çí„Éá„Éº„Çø„Éô„Éº„Çπ„Å´‰øùÂ≠ò"""
        adapter = ItemAdapter(item)

        try:
            self.cursor.execute('''
                INSERT OR REPLACE INTO items (title, description, url, scraped_date, spider_name)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                adapter.get('title'),
                adapter.get('description'),
                adapter.get('url'),
                adapter.get('scraped_date'),
                spider.name
            ))
            self.connection.commit()
            logging.info(f"Item saved to database: {adapter.get('title')}")
        except sqlite3.Error as e:
            logging.error(f"Error saving item to database: {e}")
            raise DropItem(f"Error saving item: {e}")

        return item


class ImagesPipeline:
    """ÁîªÂÉè„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ„Éë„Ç§„Éó„É©„Ç§„É≥"""

    def process_item(self, item, spider):
        """ÁîªÂÉèURL„ÇíÂá¶ÁêÜ„Åô„Çã"""
        adapter = ItemAdapter(item)

        # ÁîªÂÉèURL„ÅÆÊ≠£Ë¶èÂåñ
        if adapter.get('image_urls'):
            normalized_urls = []
            for url in adapter['image_urls']:
                if url.startswith('//'):
                    url = 'https:' + url
                elif url.startswith('/'):
                    # Áõ∏ÂØæURL„ÇíÁµ∂ÂØæURL„Å´Â§âÊèõ
                    base_url = adapter.get('url', '')
                    if base_url:
                        from urllib.parse import urljoin
                        url = urljoin(base_url, url)
                normalized_urls.append(url)
            adapter['image_urls'] = normalized_urls

        return item
`
  }

  // middlewares.py„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateMiddlewaresSnippet = (projectName: string, capitalizedProjectName: string) => {
    return `# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

import random
import logging
from scrapy import signals
from scrapy.http import HtmlResponse
from scrapy.downloadermiddlewares.retry import RetryMiddleware
from scrapy.utils.response import response_status_message


class ${capitalizedProjectName}SpiderMiddleware:
    """„Çπ„Éë„Ç§„ÉÄ„Éº„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        """„Çπ„Éë„Ç§„ÉÄ„Éº„ÅÆÂÖ•Âäõ„ÇíÂá¶ÁêÜ"""
        return None

    def process_spider_output(self, response, result, spider):
        """„Çπ„Éë„Ç§„ÉÄ„Éº„ÅÆÂá∫Âäõ„ÇíÂá¶ÁêÜ"""
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        """„Çπ„Éë„Ç§„ÉÄ„Éº„ÅÆ‰æãÂ§ñ„ÇíÂá¶ÁêÜ"""
        pass

    def process_start_requests(self, start_requests, spider):
        """ÈñãÂßã„É™„ÇØ„Ç®„Çπ„Éà„ÇíÂá¶ÁêÜ"""
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÈñãÂßãÊôÇ„ÅÆÂá¶ÁêÜ"""
        spider.logger.info('Spider opened: %s' % spider.name)


class ${capitalizedProjectName}DownloaderMiddleware:
    """„ÉÄ„Ç¶„É≥„É≠„Éº„ÉÄ„Éº„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        """„É™„ÇØ„Ç®„Çπ„Éà„ÇíÂá¶ÁêÜ"""
        return None

    def process_response(self, request, response, spider):
        """„É¨„Çπ„Éù„É≥„Çπ„ÇíÂá¶ÁêÜ"""
        return response

    def process_exception(self, request, exception, spider):
        """‰æãÂ§ñ„ÇíÂá¶ÁêÜ"""
        pass

    def spider_opened(self, spider):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÈñãÂßãÊôÇ„ÅÆÂá¶ÁêÜ"""
        spider.logger.info('Spider opened: %s' % spider.name)


class UserAgentMiddleware:
    """„É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    def __init__(self):
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0',
        ]

    def process_request(self, request, spider):
        """„É©„É≥„ÉÄ„É†„Å™„É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíË®≠ÂÆö"""
        ua = random.choice(self.user_agents)
        request.headers['User-Agent'] = ua
        return None


class ProxyMiddleware:
    """„Éó„É≠„Ç≠„Ç∑„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    def __init__(self):
        self.proxies = [
            # „Éó„É≠„Ç≠„Ç∑„É™„Çπ„Éà„Çí„Åì„Åì„Å´ËøΩÂä†
            # 'http://proxy1:port',
            # 'http://proxy2:port',
        ]

    def process_request(self, request, spider):
        """„É©„É≥„ÉÄ„É†„Å™„Éó„É≠„Ç≠„Ç∑„ÇíË®≠ÂÆö"""
        if self.proxies:
            proxy = random.choice(self.proxies)
            request.meta['proxy'] = proxy
        return None


class CustomRetryMiddleware(RetryMiddleware):
    """„Ç´„Çπ„Çø„É†„É™„Éà„É©„Ç§„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    def __init__(self, settings):
        super().__init__(settings)
        self.retry_http_codes = set(int(x) for x in settings.getlist('RETRY_HTTP_CODES'))

    def process_response(self, request, response, spider):
        """„É¨„Çπ„Éù„É≥„Çπ„Å´Âü∫„Å•„ÅÑ„Å¶„É™„Éà„É©„Ç§„ÇíÂà§ÂÆö"""
        if request.meta.get('dont_retry', False):
            return response


class LoggingMiddleware:
    """„É≠„Ç∞Âá∫Âäõ„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    def process_request(self, request, spider):
        """„É™„ÇØ„Ç®„Çπ„Éà„Çí„É≠„Ç∞„Å´Ë®òÈå≤"""
        spider.logger.info(f'Processing request: {request.url}')
        return None

    def process_response(self, request, response, spider):
        """„É¨„Çπ„Éù„É≥„Çπ„Çí„É≠„Ç∞„Å´Ë®òÈå≤"""
        spider.logger.info(f'Response received: {response.status} for {request.url}')
        return response


# ========================================
# Playwright Middlewares (Optional)
# ========================================
# Uncomment the following middlewares to enable Playwright support

#import asyncio
#from scrapy_playwright.page import PageCoroutine

#class PlaywrightUserAgentMiddleware:
#    """PlaywrightÁî®„ÅÆ„É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„É≠„Éº„ÉÜ„Éº„Ç∑„Éß„É≥„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""
#
#    def __init__(self):
#        self.user_agents = [
#            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
#        ]
#
#    def process_request(self, request, spider):
#        """„É©„É≥„ÉÄ„É†„Å™„É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÇíË®≠ÂÆö"""
#        if request.meta.get('playwright'):
#            ua = random.choice(self.user_agents)
#            request.meta.setdefault('playwright_context_kwargs', {})
#            request.meta['playwright_context_kwargs']['user_agent'] = ua
#        return None

#class PlaywrightDelayMiddleware:
#    """PlaywrightÁî®„ÅÆÈÅÖÂª∂„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""
#
#    def __init__(self, delay_min=1, delay_max=3):
#        self.delay_min = delay_min
#        self.delay_max = delay_max
#
#    @classmethod
#    def from_crawler(cls, crawler):
#        settings = crawler.settings
#        delay_min = settings.getfloat('PLAYWRIGHT_DELAY_MIN', 1)
#        delay_max = settings.getfloat('PLAYWRIGHT_DELAY_MAX', 3)
#        return cls(delay_min, delay_max)
#
#    def process_request(self, request, spider):
#        """Playwright„É™„ÇØ„Ç®„Çπ„Éà„ÅÆÂ†¥Âêà„ÅÆ„ÅøÈÅÖÂª∂„ÇíÈÅ©Áî®"""
#        if request.meta.get('playwright'):
#            delay = random.uniform(self.delay_min, self.delay_max)
#            request.meta.setdefault('playwright_page_coroutines', [])
#            request.meta['playwright_page_coroutines'].append(
#                PageCoroutine('wait_for_timeout', delay * 1000)
#            )
#        return None

#class PlaywrightScrollMiddleware:
#    """PlaywrightÁî®„ÅÆ„Çπ„ÇØ„É≠„Éº„É´„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""
#
#    def process_request(self, request, spider):
#        """„Éö„Éº„Ç∏„ÇíÊúÄ‰∏ãÈÉ®„Åæ„Åß„Çπ„ÇØ„É≠„Éº„É´"""
#        if request.meta.get('playwright') and request.meta.get('scroll_to_bottom'):
#            request.meta.setdefault('playwright_page_coroutines', [])
#            request.meta['playwright_page_coroutines'].extend([
#                PageCoroutine('evaluate', '''
#                    async () => {
#                        await new Promise((resolve) => {
#                            let totalHeight = 0;
#                            const distance = 100;
#                            const timer = setInterval(() => {
#                                const scrollHeight = document.body.scrollHeight;
#                                window.scrollBy(0, distance);
#                                totalHeight += distance;
#                                if(totalHeight >= scrollHeight){
#                                    clearInterval(timer);
#                                    resolve();
#                                }
#                            }, 100);
#                        });
#                    }
#                '''),
#                PageCoroutine('wait_for_timeout', 1000)
#            ])
#        return None

        if response.status in self.retry_http_codes:
            reason = response_status_message(response.status)
            return self._retry(request, reason, spider) or response

        return response

    def process_exception(self, request, exception, spider):
        """‰æãÂ§ñ„Å´Âü∫„Å•„ÅÑ„Å¶„É™„Éà„É©„Ç§„ÇíÂà§ÂÆö"""
        if isinstance(exception, self.EXCEPTIONS_TO_RETRY) and not request.meta.get('dont_retry', False):
            return self._retry(request, exception, spider)


class LoggingMiddleware:
    """„É≠„Ç∞Âá∫Âäõ„Éü„Éâ„É´„Ç¶„Çß„Ç¢"""

    def process_request(self, request, spider):
        """„É™„ÇØ„Ç®„Çπ„Éà„Çí„É≠„Ç∞Âá∫Âäõ"""
        spider.logger.info(f'Processing request: {request.url}')
        return None

    def process_response(self, request, response, spider):
        """„É¨„Çπ„Éù„É≥„Çπ„Çí„É≠„Ç∞Âá∫Âäõ"""
        spider.logger.info(f'Received response: {response.status} for {request.url}')
        return response

    def process_exception(self, request, exception, spider):
        """‰æãÂ§ñ„Çí„É≠„Ç∞Âá∫Âäõ"""
        spider.logger.error(f'Exception occurred: {exception} for {request.url}')
        pass
`
  }

  // spiderÁî®„Çπ„Éã„Éö„ÉÉ„ÉàÁîüÊàê
  const generateSpiderSnippet = (projectName: string, capitalizedProjectName: string) => {
    return `import scrapy
from scrapy import Request
from ${projectName}.items import ${capitalizedProjectName}Item

# Playwright imports (uncomment when using Playwright)
# from scrapy_playwright.page import PageCoroutine


class ${capitalizedProjectName}Spider(scrapy.Spider):
    """${capitalizedProjectName} Spider"""

    name = '${projectName}'
    allowed_domains = ['example.com']
    start_urls = ['https://example.com']

    # „Ç´„Çπ„Çø„É†Ë®≠ÂÆö
    custom_settings = {
        'DOWNLOAD_DELAY': 1,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
    }

    def parse(self, response):
        """„É°„Ç§„É≥„ÅÆ„Éë„Éº„ÇπÈñ¢Êï∞"""
        # „Éö„Éº„Ç∏ÂÜÖ„ÅÆ„É™„É≥„ÇØ„ÇíÊäΩÂá∫
        links = response.css('a::attr(href)').getall()

        for link in links:
            if link:
                # Áõ∏ÂØæURL„ÇíÁµ∂ÂØæURL„Å´Â§âÊèõ
                absolute_url = response.urljoin(link)
                yield Request(
                    url=absolute_url,
                    callback=self.parse_item,
                    meta={'page_url': response.url}
                )

        # Ê¨°„ÅÆ„Éö„Éº„Ç∏„Å∏„ÅÆ„É™„É≥„ÇØ„ÇíÂá¶ÁêÜ
        next_page = response.css('a.next::attr(href)').get()
        if next_page:
            yield Request(
                url=response.urljoin(next_page),
                callback=self.parse
            )

    def parse_item(self, response):
        """ÂÄãÂà•„Ç¢„Ç§„ÉÜ„É†„ÅÆ„Éë„Éº„Çπ"""
        item = ${capitalizedProjectName}Item()

        # Âü∫Êú¨ÊÉÖÂ†±„ÇíÊäΩÂá∫
        item['title'] = response.css('h1::text').get()
        item['description'] = response.css('.description::text').get()
        item['url'] = response.url

        # „É°„ÇøÊÉÖÂ†±„ÇíËøΩÂä†
        item['page_url'] = response.meta.get('page_url')

        # „Éá„Éê„ÉÉ„Ç∞ÊÉÖÂ†±„ÇíÂá∫Âäõ
        self.logger.info(f'Scraped item: {item["title"]} from {response.url}')

        yield item

    def parse_with_css(self, response):
        """CSS„Çª„É¨„ÇØ„Çø„Éº„Çí‰ΩøÁî®„Åó„Åü„Éë„Éº„Çπ‰æã"""
        items = response.css('.item')

        for item_selector in items:
            item = ${capitalizedProjectName}Item()

            item['title'] = item_selector.css('.title::text').get()
            item['description'] = item_selector.css('.description::text').get()
            item['url'] = item_selector.css('a::attr(href)').get()

            if item['url']:
                item['url'] = response.urljoin(item['url'])

            yield item

    def parse_with_xpath(self, response):
        """XPath„Çí‰ΩøÁî®„Åó„Åü„Éë„Éº„Çπ‰æã"""
        items = response.xpath('//div[@class="item"]')

        for item_selector in items:
            item = ${capitalizedProjectName}Item()

            item['title'] = item_selector.xpath('.//h2/text()').get()
            item['description'] = item_selector.xpath('.//p[@class="description"]/text()').get()
            item['url'] = item_selector.xpath('.//a/@href').get()

            if item['url']:
                item['url'] = response.urljoin(item['url'])

            yield item

    def start_requests(self):
        """ÈñãÂßã„É™„ÇØ„Ç®„Çπ„Éà„Çí„Ç´„Çπ„Çø„Éû„Ç§„Ç∫"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }

        for url in self.start_urls:
            yield Request(
                url=url,
                headers=headers,
                callback=self.parse,
                meta={'download_timeout': 30}
            )

    def closed(self, reason):
        """„Çπ„Éë„Ç§„ÉÄ„ÉºÁµÇ‰∫ÜÊôÇ„ÅÆÂá¶ÁêÜ"""
        self.logger.info(f'Spider closed: {reason}')


# ========================================
# Playwright Spider Examples (Optional)
# ========================================
# Uncomment the following class to enable Playwright support

# class ${capitalizedProjectName}PlaywrightSpider(scrapy.Spider):
#     """PlaywrightÂØæÂøú„Çπ„Éë„Ç§„ÉÄ„Éº"""
#
#     name = '${projectName}_playwright'
#     allowed_domains = ['example.com']
#     start_urls = ['https://example.com']
#
#     # PlaywrightÁî®„ÅÆ„Ç´„Çπ„Çø„É†Ë®≠ÂÆö
#     custom_settings = {
#         'DOWNLOAD_HANDLERS': {
#             'http': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
#             'https': 'scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler',
#         },
#         'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
#         'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
#         'PLAYWRIGHT_LAUNCH_OPTIONS': {
#             'headless': True,
#         },
#     }
#
#     def start_requests(self):
#         """PlaywrightÁî®„ÅÆÈñãÂßã„É™„ÇØ„Ç®„Çπ„Éà"""
#         for url in self.start_urls:
#             yield scrapy.Request(
#                 url=url,
#                 meta={
#                     'playwright': True,
#                     'playwright_page_coroutines': [
#                         PageCoroutine('wait_for_selector', 'body'),
#                         PageCoroutine('wait_for_timeout', 2000),
#                     ],
#                 },
#                 callback=self.parse
#             )
#
#     def parse(self, response):
#         """Playwright„É¨„Çπ„Éù„É≥„Çπ„ÅÆ„Éë„Éº„Çπ"""
#         # „Éö„Éº„Ç∏„ÅåÂÆåÂÖ®„Å´Ë™≠„ÅøËæº„Åæ„Çå„ÅüÂæå„ÅÆÂá¶ÁêÜ
#         items = response.css('.item')
#
#         for item_selector in items:
#             item = ${capitalizedProjectName}Item()
#             item['title'] = item_selector.css('.title::text').get()
#             item['description'] = item_selector.css('.description::text').get()
#             item['url'] = response.url
#             yield item
#
#         # Ê¨°„ÅÆ„Éö„Éº„Ç∏„Å∏„ÅÆ„É™„É≥„ÇØÔºàJavaScriptÂá¶ÁêÜ„ÅåÂøÖË¶Å„Å™Â†¥ÂêàÔºâ
#         next_page_button = response.css('button.next-page')
#         if next_page_button:
#             yield scrapy.Request(
#                 url=response.url,
#                 meta={
#                     'playwright': True,
#                     'playwright_page_coroutines': [
#                         PageCoroutine('click', 'button.next-page'),
#                         PageCoroutine('wait_for_selector', '.item'),
#                         PageCoroutine('wait_for_timeout', 1000),
#                     ],
#                 },
#                 callback=self.parse,
#                 dont_filter=True
#             )
#
#     def parse_with_scroll(self, response):
#         """„Çπ„ÇØ„É≠„Éº„É´„ÅåÂøÖË¶Å„Å™„Éö„Éº„Ç∏„ÅÆÂá¶ÁêÜ"""
#         yield scrapy.Request(
#             url=response.url,
#             meta={
#                 'playwright': True,
#                 'playwright_page_coroutines': [
#                     PageCoroutine('evaluate', '''
#                         async () => {
#                             await new Promise((resolve) => {
#                                 let totalHeight = 0;
#                                 const distance = 100;
#                                 const timer = setInterval(() => {
#                                     const scrollHeight = document.body.scrollHeight;
#                                     window.scrollBy(0, distance);
#                                     totalHeight += distance;
#                                     if(totalHeight >= scrollHeight){
#                                         clearInterval(timer);
#                                         resolve();
#                                     }
#                                 }, 100);
#                             });
#                         }
#                     '''),
#                     PageCoroutine('wait_for_timeout', 2000),
#                 ],
#             },
#             callback=self.parse_scrolled_content
#         )
#
#     def parse_scrolled_content(self, response):
#         """„Çπ„ÇØ„É≠„Éº„É´Âæå„ÅÆ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„ÇíÂá¶ÁêÜ"""
#         items = response.css('.dynamic-item')
#         for item_selector in items:
#             item = ${capitalizedProjectName}Item()
#             item['title'] = item_selector.css('.title::text').get()
#             item['url'] = response.url
#             yield item
`
  }

  const copySnippet = () => {
    const snippet = generateSnippet()
    navigator.clipboard.writeText(snippet)
  }

  const restoreDefaultCode = () => {
    setShowRestoreConfirm(true)
  }

  const confirmRestoreDefaultCode = () => {
    const defaultCode = generateSnippet()
    onChange(defaultCode)
    setShowRestoreConfirm(false)
  }

  const downloadSnippet = () => {
    const snippet = generateSnippet()
    const fileType = getFileType()
    const projectName = fileName.replace(/\.(py|cfg)$/, '').replace('__init__', 'init').toLowerCase()
    const blob = new Blob([snippet], { type: 'text/plain' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url

    // „Éï„Ç°„Ç§„É´Êã°ÂºµÂ≠ê„ÇíÈÅ©Âàá„Å´Ë®≠ÂÆö
    let extension = '.py'
    if (fileType === 'scrapy_cfg') {
      extension = '.cfg'
    } else if (fileType === 'init') {
      extension = '.py'
    }

    a.download = `${projectName}_${fileType}${extension}`

    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
  }

  const getSnippetTitle = () => {
    const fileType = getFileType()
    const filePath = fileName.toLowerCase()

    switch (fileType) {
      case 'scrapy_cfg': return 'Scrapy.cfg Snippet'
      case 'init':
        if (filePath.includes('/spiders/__init__.py') || filePath.includes('\\spiders\\__init__.py')) {
          return 'Spiders __init__.py Snippet'
        } else {
          return '__init__.py Snippet'
        }
      case 'settings': return 'Settings.py Snippet'
      case 'items': return 'Items.py Snippet'
      case 'pipelines': return 'Pipelines.py Snippet'
      case 'middlewares': return 'Middlewares.py Snippet'
      default: return 'Spider.py Snippet'
    }
  }

  const getSnippetDescription = () => {
    const fileType = getFileType()
    const filePath = fileName.toLowerCase()

    switch (fileType) {
      case 'scrapy_cfg':
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„É´„Éº„Éà„ÅÆ scrapy.cfg „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: '„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂêç„Å®settings„É¢„Ç∏„É•„Éº„É´„ÅÆË®≠ÂÆö„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: 'ScrapyÊ®ôÊ∫ñ„ÅÆ„Éó„É≠„Ç∏„Çß„ÇØ„ÉàË®≠ÂÆö„Éï„Ç°„Ç§„É´„Åß„Åô'
        }
      case 'init':
        if (filePath.includes('/spiders/__init__.py') || filePath.includes('\\spiders\\__init__.py')) {
          return {
            usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çíspiders„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆ __init__.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
            config: '„Çπ„Éë„Ç§„ÉÄ„Éº„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆÂàùÊúüÂåñ„Éï„Ç°„Ç§„É´„Åß„Åô',
            standard: 'ScrapyÊ®ôÊ∫ñ„ÅÆ„Çπ„Éë„Ç§„ÉÄ„Éº„Éë„ÉÉ„Ç±„Éº„Ç∏Ë®≠ÂÆö„Åß„Åô'
          }
        } else {
          return {
            usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ __init__.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
            config: '„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Éë„ÉÉ„Ç±„Éº„Ç∏„ÅÆÂàùÊúüÂåñ„Éï„Ç°„Ç§„É´„Åß„Åô',
            standard: 'PythonÊ®ôÊ∫ñ„ÅÆ„Éë„ÉÉ„Ç±„Éº„Ç∏ÂàùÊúüÂåñ„Éï„Ç°„Ç§„É´„Åß„Åô'
          }
        }
      case 'settings':
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ settings.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: 'ScrapyÊ®ôÊ∫ñË®≠ÂÆö„Å®PlaywrightË®≠ÂÆöÔºà„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„ÉàÔºâ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: 'ScrapyÊ®ôÊ∫ñË®≠ÂÆö„ÅÆ„Åø„ÅåÊúâÂäπ„Åß„ÄÅPlaywrightË®≠ÂÆö„ÅØ„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„Éà„Åï„Çå„Å¶„ÅÑ„Åæ„Åô'
        }
      case 'items':
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ items.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: 'Âü∫Êú¨„Ç¢„Ç§„ÉÜ„É†„ÄÅÂïÜÂìÅ„Ç¢„Ç§„ÉÜ„É†„ÄÅ„Éã„É•„Éº„Çπ„Ç¢„Ç§„ÉÜ„É†„ÅÆ„ÇØ„É©„Çπ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: 'ItemLoader„Å®„Éó„É≠„Çª„ÉÉ„Çµ„Éº„Çí‰ΩøÁî®„Åó„Åü„Éá„Éº„Çø„ÇØ„É™„Éº„Éã„É≥„Ç∞Ê©üËÉΩ‰ªò„Åç„Åß„Åô'
        }
      case 'pipelines':
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ pipelines.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: 'Ê§úË®º„ÄÅÈáçË§áÈô§Âéª„ÄÅJSONÂá∫Âäõ„ÄÅSQLite‰øùÂ≠ò„Å™„Å©„ÅÆ„Éë„Ç§„Éó„É©„Ç§„É≥„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: 'settings.py„ÅÆITEM_PIPELINES„Å´ËøΩÂä†„Åó„Å¶ÊúâÂäπÂåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ'
        }
      case 'middlewares':
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„Çí„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆ middlewares.py „Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: '„É¶„Éº„Ç∂„Éº„Ç®„Éº„Ç∏„Çß„É≥„Éà„ÄÅ„Éó„É≠„Ç≠„Ç∑„ÄÅ„É™„Éà„É©„Ç§„ÄÅPlaywright„Éü„Éâ„É´„Ç¶„Çß„Ç¢Ôºà„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„ÉàÔºâ„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: 'settings.py„ÅÆMIDDLEWARES„Å´ËøΩÂä†„Åó„Å¶ÊúâÂäπÂåñ„Åó„Å¶„Åè„Å†„Åï„ÅÑ'
        }
      default:
        return {
          usage: '„Åì„ÅÆ„Çπ„Éã„Éö„ÉÉ„Éà„ÇíÊñ∞„Åó„ÅÑ„Çπ„Éë„Ç§„ÉÄ„Éº„Éï„Ç°„Ç§„É´„Å´„Ç≥„Éî„Éº„Åó„Å¶„Åè„Å†„Åï„ÅÑ',
          config: 'CSS„ÄÅXPath„ÄÅ„Ç´„Çπ„Çø„É†„É™„ÇØ„Ç®„Çπ„Éà„ÄÅPlaywright„Çπ„Éë„Ç§„ÉÄ„ÉºÔºà„Ç≥„É°„É≥„Éà„Ç¢„Ç¶„ÉàÔºâ„ÅÆÂÆüË£Ö‰æã„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô',
          standard: '„Éó„É≠„Ç∏„Çß„ÇØ„Éà„ÅÆitems„Å®„ÅÆÈÄ£Êê∫„Å®PlaywrightÂØæÂøú„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô'
        }
    }
  }

  const handleEditorDidMount = (editor: any, monaco: any) => {
    editorRef.current = editor
    setIsLoading(false)

    // PythonË®ÄË™ûË®≠ÂÆö„ÅÆÊã°Âºµ
    if (language === 'python') {
      monaco.languages.setLanguageConfiguration('python', {
        comments: {
          lineComment: '#',
          blockComment: ['"""', '"""']
        },
        brackets: [
          ['{', '}'],
          ['[', ']'],
          ['(', ')']
        ],
        autoClosingPairs: [
          { open: '{', close: '}' },
          { open: '[', close: ']' },
          { open: '(', close: ')' },
          { open: '"', close: '"' },
          { open: "'", close: "'" }
        ],
        surroundingPairs: [
          { open: '{', close: '}' },
          { open: '[', close: ']' },
          { open: '(', close: ')' },
          { open: '"', close: '"' },
          { open: "'", close: "'" }
        ]
      })

      // ScrapyÂõ∫Êúâ„ÅÆ„Ç≠„Éº„ÉØ„Éº„Éâ„Å®„ÇØ„É©„Çπ„ÅÆËøΩÂä†
      monaco.languages.registerCompletionItemProvider('python', {
        provideCompletionItems: (model: any, position: any) => {
          const suggestions = [
            {
              label: 'scrapy.Spider',
              kind: monaco.languages.CompletionItemKind.Class,
              insertText: 'scrapy.Spider',
              documentation: 'Base Spider class'
            },
            {
              label: 'start_urls',
              kind: monaco.languages.CompletionItemKind.Property,
              insertText: 'start_urls = []',
              documentation: 'List of URLs to start crawling from'
            },
            {
              label: 'parse',
              kind: monaco.languages.CompletionItemKind.Method,
              insertText: [
                'def parse(self, response):',
                '    """Parse the response and extract data."""',
                '    pass'
              ].join('\n'),
              documentation: 'Default callback method for parsing responses'
            },
            {
              label: 'scrapy.Request',
              kind: monaco.languages.CompletionItemKind.Class,
              insertText: 'scrapy.Request(url, callback=self.parse)',
              documentation: 'Create a new request'
            },
            {
              label: 'response.css',
              kind: monaco.languages.CompletionItemKind.Method,
              insertText: 'response.css("selector")',
              documentation: 'CSS selector for extracting data'
            },
            {
              label: 'response.xpath',
              kind: monaco.languages.CompletionItemKind.Method,
              insertText: 'response.xpath("//xpath")',
              documentation: 'XPath selector for extracting data'
            },
            {
              label: 'yield',
              kind: monaco.languages.CompletionItemKind.Keyword,
              insertText: 'yield',
              documentation: 'Yield items or requests'
            }
          ]
          return { suggestions }
        }
      })
    }

    // „Ç≠„Éº„Éú„Éº„Éâ„Ç∑„Éß„Éº„Éà„Ç´„ÉÉ„Éà
    editor.addCommand(monaco.KeyMod.CtrlCmd | monaco.KeyCode.Enter, () => {
      onRun?.()
    })
  }

  const handleEditorChange = (value: string | undefined) => {
    if (value !== undefined) {
      onChange(value)
    }
  }

  const formatCode = () => {
    if (editorRef.current) {
      editorRef.current.getAction('editor.action.formatDocument').run()
    }
  }

  const downloadCode = () => {
    const blob = new Blob([value], { type: 'text/plain' })
    const url = URL.createObjectURL(blob)
    const a = document.createElement('a')
    a.href = url
    a.download = fileName
    document.body.appendChild(a)
    a.click()
    document.body.removeChild(a)
    URL.revokeObjectURL(url)
  }

  return (
    <div className={`flex flex-col h-full bg-gray-900 ${className}`}>
      {/* „Éï„Ç°„Ç§„É´Âêç„Å®„É°„Ç§„É≥„Ç¢„ÇØ„Ç∑„Éß„É≥„Éú„Çø„É≥ */}
      <div className="flex items-center justify-between p-3 bg-gray-800 border-b border-gray-700">
        <div className="flex items-center space-x-2">
          <FileText className="w-4 h-4 text-gray-400" />
          <span className="text-sm text-gray-300">{fileName}</span>
        </div>

        <div className="flex items-center space-x-2">
          {onTest && (
            <button
              onClick={onTest}
              className="px-3 py-2 bg-yellow-600 hover:bg-yellow-700 text-white rounded transition-colors flex items-center space-x-1"
              title="Quick Test"
            >
              <Bug className="w-4 h-4" />
              <span className="text-sm">Test</span>
            </button>
          )}

          {onRun && (
            <button
              onClick={onRun}
              className="px-3 py-2 bg-green-600 hover:bg-green-700 text-white rounded transition-colors flex items-center space-x-1"
              title="Run (Ctrl+Enter)"
            >
              <Play className="w-4 h-4" />
              <span className="text-sm">Run</span>
            </button>
          )}
        </div>
      </div>

      {/* „ÉÑ„Éº„É´„Éê„Éº */}
      <div className="flex items-center justify-end p-2 bg-gray-750 border-b border-gray-700">
        <div className="flex items-center space-x-1">
          <button
            onClick={formatCode}
            className="p-2 text-gray-400 hover:text-white hover:bg-gray-700 rounded transition-colors"
            title="Format Code (Shift+Alt+F)"
          >
            <Settings className="w-4 h-4" />
          </button>

          <button
            onClick={downloadCode}
            className="p-2 text-gray-400 hover:text-white hover:bg-gray-700 rounded transition-colors"
            title="Download"
          >
            <Download className="w-4 h-4" />
          </button>

          <button
            onClick={() => setShowSettingsSnippet(true)}
            className="p-2 text-gray-400 hover:text-white hover:bg-gray-700 rounded transition-colors"
            title={getSnippetTitle()}
          >
            <FileCode className="w-4 h-4" />
          </button>

          <button
            onClick={restoreDefaultCode}
            className="p-2 text-gray-400 hover:text-white hover:bg-gray-700 rounded transition-colors"
            title="Restore Default Code"
          >
            <RotateCcw className="w-4 h-4" />
          </button>
        </div>
      </div>

      {/* „Ç®„Éá„Ç£„Çø„Éº */}
      <div className="flex-1 relative">
        {isLoading && (
          <div className="absolute inset-0 flex items-center justify-center bg-gray-900">
            <div className="text-gray-400">Loading editor...</div>
          </div>
        )}

        <Editor
          height="100%"
          language={language}
          theme={theme}
          value={value}
          onChange={handleEditorChange}
          onMount={handleEditorDidMount}
          options={{
            readOnly,
            minimap: { enabled: true },
            fontSize: 14,
            lineNumbers: 'on',
            roundedSelection: false,
            scrollBeyondLastLine: false,
            automaticLayout: true,
            tabSize: 4,
            insertSpaces: true,
            wordWrap: 'on',
            folding: true,
            foldingStrategy: 'indentation',
            showFoldingControls: 'always',
            bracketPairColorization: { enabled: true },
            guides: {
              bracketPairs: true,
              indentation: true
            },
            suggest: {
              showKeywords: true,
              showSnippets: true,
              showClasses: true,
              showFunctions: true,
              showVariables: true
            },
            quickSuggestions: {
              other: true,
              comments: false,
              strings: false
            },
            parameterHints: {
              enabled: true
            },
            hover: {
              enabled: true
            }
          }}
        />
      </div>

      {/* „Ç®„É©„ÉºË°®Á§∫ */}
      {errors.length > 0 && (
        <div className="p-3 bg-red-900 border-t border-red-700">
          <div className="flex items-center space-x-2 mb-2">
            <Bug className="w-4 h-4 text-red-400" />
            <span className="text-sm text-red-300 font-medium">Errors</span>
          </div>
          <div className="space-y-1">
            {errors.map((error, index) => (
              <div key={index} className="text-sm text-red-200">
                Line {error.line}: {error.message}
              </div>
            ))}
          </div>
        </div>
      )}

      {/* Settings.py „Çπ„Éã„Éö„ÉÉ„Éà„É¢„Éº„ÉÄ„É´ */}
      {showSettingsSnippet && (
        <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50">
          <div className="bg-gray-800 rounded-lg shadow-xl max-w-4xl w-full mx-4 max-h-[90vh] flex flex-col">
            {/* „Éò„ÉÉ„ÉÄ„Éº */}
            <div className="flex items-center justify-between p-4 border-b border-gray-700">
              <div className="flex items-center space-x-2">
                <FileCode className="w-5 h-5 text-blue-400" />
                <h3 className="text-lg font-semibold text-white">{getSnippetTitle()}</h3>
              </div>
              <div className="flex items-center space-x-2">
                <button
                  onClick={copySnippet}
                  className="px-3 py-1.5 bg-blue-600 hover:bg-blue-700 text-white rounded text-sm transition-colors flex items-center space-x-1"
                  title="Copy to Clipboard"
                >
                  <Copy className="w-4 h-4" />
                  <span>Copy</span>
                </button>
                <button
                  onClick={downloadSnippet}
                  className="px-3 py-1.5 bg-green-600 hover:bg-green-700 text-white rounded text-sm transition-colors flex items-center space-x-1"
                  title="Download File"
                >
                  <Download className="w-4 h-4" />
                  <span>Download</span>
                </button>
                <button
                  onClick={() => setShowSettingsSnippet(false)}
                  className="p-1.5 text-gray-400 hover:text-white hover:bg-gray-700 rounded transition-colors"
                  title="Close"
                >
                  <X className="w-4 h-4" />
                </button>
              </div>
            </div>

            {/* „Ç≥„É≥„ÉÜ„É≥„ÉÑ */}
            <div className="flex-1 overflow-hidden">
              <Editor
                height="70vh"
                language={getFileType() === 'scrapy_cfg' ? 'ini' : 'python'}
                theme="vs-dark"
                value={generateSnippet()}
                options={{
                  readOnly: true,
                  minimap: { enabled: false },
                  fontSize: 13,
                  lineNumbers: 'on',
                  scrollBeyondLastLine: false,
                  automaticLayout: true,
                  wordWrap: 'on',
                  folding: true,
                  bracketPairColorization: { enabled: true },
                  guides: {
                    bracketPairs: true,
                    indentation: true
                  }
                }}
              />
            </div>

            {/* „Éï„ÉÉ„Çø„Éº */}
            <div className="p-4 border-t border-gray-700 bg-gray-750">
              <div className="text-sm text-gray-400">
                <p className="mb-1">
                  <strong className="text-gray-300">üìù ‰ΩøÁî®ÊñπÊ≥ï:</strong> {getSnippetDescription().usage}
                </p>
                <p className="mb-1">
                  <strong className="text-gray-300">‚öôÔ∏è Ë®≠ÂÆö:</strong> {getSnippetDescription().config}
                </p>
                <p>
                  <strong className="text-gray-300">üîß Ê®ôÊ∫ñÊ∫ñÊã†:</strong> {getSnippetDescription().standard}
                </p>
              </div>
            </div>
          </div>
        </div>
      )}

      {/* „Éá„Éï„Ç©„É´„Éà„Ç≥„Éº„ÉâÂæ©ÊóßÁ¢∫Ë™ç„ÉÄ„Ç§„Ç¢„É≠„Ç∞ */}
      {showRestoreConfirm && (
        <div className="fixed inset-0 bg-black bg-opacity-50 flex items-center justify-center z-50">
          <div className="bg-gray-800 rounded-lg shadow-xl max-w-md w-full mx-4">
            {/* „Éò„ÉÉ„ÉÄ„Éº */}
            <div className="flex items-center justify-between p-4 border-b border-gray-700">
              <div className="flex items-center space-x-2">
                <RotateCcw className="w-5 h-5 text-orange-400" />
                <h3 className="text-lg font-semibold text-white">„Éá„Éï„Ç©„É´„Éà„Ç≥„Éº„ÉâÂæ©Êóß</h3>
              </div>
            </div>

            {/* „Ç≥„É≥„ÉÜ„É≥„ÉÑ */}
            <div className="p-4">
              <p className="text-gray-300 mb-4">
                ÁèæÂú®„ÅÆ„Ç≥„Éº„Éâ„ÇíÂâäÈô§„Åó„Å¶„ÄÅ„Éá„Éï„Ç©„É´„Éà„ÅÆ{getSnippetTitle().replace(' Snippet', '')}„Ç≥„Éº„Éâ„Å´Âæ©Êóß„Åó„Åæ„Åô„ÅãÔºü
              </p>
              <div className="bg-yellow-900 border border-yellow-700 rounded p-3 mb-4">
                <div className="flex items-center space-x-2">
                  <span className="text-yellow-400">‚ö†Ô∏è</span>
                  <span className="text-yellow-200 text-sm font-medium">Ê≥®ÊÑè</span>
                </div>
                <p className="text-yellow-200 text-sm mt-1">
                  „Åì„ÅÆÊìç‰Ωú„ÅØÂÖÉ„Å´Êàª„Åõ„Åæ„Åõ„Çì„ÄÇÁèæÂú®„ÅÆÂ§âÊõ¥ÂÜÖÂÆπ„ÅØÂ§±„Çè„Çå„Åæ„Åô„ÄÇ
                </p>
              </div>
            </div>

            {/* „Éï„ÉÉ„Çø„Éº */}
            <div className="flex items-center justify-end space-x-3 p-4 border-t border-gray-700">
              <button
                onClick={() => setShowRestoreConfirm(false)}
                className="px-4 py-2 text-gray-300 hover:text-white hover:bg-gray-700 rounded transition-colors"
              >
                „Ç≠„É£„É≥„Çª„É´
              </button>
              <button
                onClick={confirmRestoreDefaultCode}
                className="px-4 py-2 bg-orange-600 hover:bg-orange-700 text-white rounded transition-colors flex items-center space-x-2"
              >
                <RotateCcw className="w-4 h-4" />
                <span>Âæ©Êóß„Åô„Çã</span>
              </button>
            </div>
          </div>
        </div>
      )}
    </div>
  )
}
