#!/usr/bin/env python3
"""
scrapy crawlã‚³ãƒãƒ³ãƒ‰ + watchdogç›£è¦–ã®å®Ÿè£…
backend/app/services/scrapy_watchdog_monitor.py
"""
import asyncio
import subprocess
import json
import sqlite3
import threading
import time
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
import uuid
import os
import sys

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install watchdog")


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, monitor):
        self.monitor = monitor
        self.last_size = 0

    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return

        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self._handle_file_change,
                daemon=True
            ).start()

    def _handle_file_change(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®å‡¦ç†ï¼ˆDBæŒ¿å…¥æœ‰åŠ¹åŒ–ï¼‰"""
        try:
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            print(f"ğŸ”„ watchdogç›£è¦–ã§DBæŒ¿å…¥ã‚’å®Ÿè¡Œã—ã¾ã™")

            # æ–°ã—ã„è¡Œã‚’DBæŒ¿å…¥å‡¦ç†
            if self.monitor.jsonl_file_path.exists():
                current_size = self.monitor.jsonl_file_path.stat().st_size
                print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {self.monitor.last_file_size} â†’ {current_size}")

                # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
                if current_size > self.monitor.last_file_size:
                    with open(self.monitor.jsonl_file_path, 'r', encoding='utf-8') as f:
                        f.seek(self.monitor.last_file_size)
                        new_content = f.read()

                    # æ–°ã—ã„è¡Œã‚’å‡¦ç†
                    new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
                    print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                    if new_lines:
                        # ãƒãƒ«ã‚¯DBæŒ¿å…¥å‡¦ç†
                        successful_inserts = self.monitor._bulk_insert_items_threading(new_lines)
                        self.monitor.processed_lines += successful_inserts
                        print(f"ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.monitor.processed_lines}")

                self.monitor.last_file_size = current_size

                # WebSocketé€šçŸ¥ã‚’é€ä¿¡
                if self.monitor.websocket_callback:
                    try:
                        import requests
                        response = requests.post(
                            'http://localhost:8000/api/tasks/internal/websocket-notify',
                            json={
                                'type': 'file_update',
                                'task_id': self.monitor.task_id,
                                'file_lines': self.monitor.processed_lines,
                                'message': 'ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ¤œå‡ºãƒ»DBæŒ¿å…¥å®Œäº†'
                            },
                            timeout=5
                        )
                        if response.status_code == 200:
                            print(f"ğŸ“¡ WebSocketé€šçŸ¥é€ä¿¡å®Œäº†")
                    except Exception as ws_error:
                        print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")


class ScrapyWatchdogMonitor:
    """scrapy crawl + watchdogç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self,
                 task_id: str,
                 project_path: str,
                 spider_name: str,
                 db_path: str = "backend/database/scrapy_ui.db",
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.project_path = Path(project_path)
        self.spider_name = spider_name
        self.db_path = db_path
        self.websocket_callback = websocket_callback

        # ç›£è¦–çŠ¶æ…‹
        self.is_monitoring = False
        self.observer = None
        self.loop = None
        self.processed_lines = 0
        self.last_file_size = 0

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.jsonl_file_path = self.project_path / f"results_{task_id}.jsonl"

        # Scrapyãƒ—ãƒ­ã‚»ã‚¹
        self.scrapy_process = None

    def _generate_data_hash_improved(self, item_data: dict) -> str:
        """item_typeã‚’è€ƒæ…®ã—ãŸæ”¹å–„ã•ã‚ŒãŸãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆå…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¯¾å¿œï¼‰"""
        try:
            # item_typeã«å¿œã˜ã¦é©åˆ‡ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é¸æŠ
            item_type = item_data.get('item_type', 'unknown')

            if item_type == 'ranking_product':
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°å•†å“ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'ranking_position': item_data.get('ranking_position'),
                    'page_number': item_data.get('page_number'),
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'source_url': item_data.get('source_url')
                }
            elif item_type == 'ranking_product_detail':
                # å•†å“è©³ç´°ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'description': item_data.get('description'),
                    'detail_scraped_at': item_data.get('detail_scraped_at')
                }
            elif item_type == 'test_product':
                # ãƒ†ã‚¹ãƒˆå•†å“ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'price': item_data.get('price'),
                    'test_id': item_data.get('test_id')
                }
            elif item_type == 'test_product_detail':
                # ãƒ†ã‚¹ãƒˆå•†å“è©³ç´°ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'description': item_data.get('description'),
                    'test_id': item_data.get('test_id')
                }
            else:
                # ãã®ä»–ã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                hash_data = item_data.copy()

            # è¾æ›¸ã‚’ã‚½ãƒ¼ãƒˆã—ã¦JSONæ–‡å­—åˆ—ã«å¤‰æ›
            hash_string = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥
            data_str = json.dumps(item_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()

    async def execute_spider_with_monitoring(self,
                                           settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """watchdogç›£è¦–ä»˜ãã§scrapy crawlã‚’å®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {self.spider_name}")

            # ç¾åœ¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä¿å­˜
            self.loop = asyncio.get_event_loop()

            # 1. watchdogç›£è¦–ã‚’é–‹å§‹
            await self._start_watchdog_monitoring()

            # 2. scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            scrapy_task = asyncio.create_task(
                self._execute_scrapy_crawl(settings)
            )

            # 3. ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã¾ã§å¾…æ©Ÿ
            scrapy_result = await scrapy_task

            # 4. å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç›£è¦–åœæ­¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç¢ºå®Ÿã«å‡¦ç†ï¼‰
            await asyncio.sleep(2)
            self._stop_watchdog_monitoring()

            # 5. æœ€çµ‚çš„ãªçµæœå‡¦ç†
            await self._process_remaining_lines()

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            final_success = scrapy_result['success'] or (self.processed_lines > 0)

            return {
                'success': final_success,
                'process_success': scrapy_result.get('process_success', scrapy_result['success']),
                'data_success': self.processed_lines > 0,
                'task_id': self.task_id,
                'items_processed': self.processed_lines,
                'scrapy_result': scrapy_result,
                'jsonl_file': str(self.jsonl_file_path)
            }

        except Exception as e:
            self._stop_watchdog_monitoring()
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }

    async def _start_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        # watchdogç„¡åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯
        if self._is_watchdog_disabled():
            print(f"ğŸ›‘ Watchdog monitoring is disabled for task {self.task_id}")
            return

        if not WATCHDOG_AVAILABLE:
            raise Exception("watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

        self.is_monitoring = True

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
        watch_directory = self.jsonl_file_path.parent

        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print(f"   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {self.project_path}")
        print(f"   - JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {self.jsonl_file_path}")
        print(f"   - ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {watch_directory}")
        print(f"   - ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨: {watch_directory.exists()}")

        # ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not watch_directory.exists():
            print(f"âš ï¸ ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {watch_directory}")
            print(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™...")
            watch_directory.mkdir(parents=True, exist_ok=True)
            print(f"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†: {watch_directory}")

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        event_handler = JSONLWatchdogHandler(self)

        # Observerã‚’ä½œæˆã—ã¦ç›£è¦–é–‹å§‹
        self.observer = Observer()
        self.observer.schedule(event_handler, str(watch_directory), recursive=False)
        self.observer.start()

        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {watch_directory}")
        print(f"ğŸ“„ ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {self.jsonl_file_path}")

    def _is_watchdog_disabled(self):
        """watchdogç›£è¦–ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        import os

        # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
        if os.environ.get('SCRAPY_WATCHDOG_DISABLED') == 'true':
            return True

        if os.environ.get(f'SCRAPY_WATCHDOG_DISABLED_{self.task_id}') == 'true':
            return True

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒã‚§ãƒƒã‚¯
        try:
            project_dir = Path(self.project_path)
            settings_file = project_dir / 'settings.py'

            if settings_file.exists():
                with open(settings_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # è¨­å®šå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                if 'WATCHDOG_MONITORING_ENABLED = False' in content:
                    return True

                if 'SCRAPY_WATCHDOG_DISABLED = True' in content:
                    return True

        except Exception:
            pass

        return False

    def _stop_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False

        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None

        print(f"ğŸ›‘ watchdogç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")

    async def _execute_scrapy_crawl(self, settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """scrapy crawlwithwatchdogã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆDBæŒ¿å…¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆcrawlwithwatchdogã‚’ä½¿ç”¨ï¼‰
            cmd = [
                sys.executable, "-m", "scrapy", "crawlwithwatchdog", self.spider_name,
                "-o", str(self.jsonl_file_path),  # JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                "--task-id", self.task_id,        # ã‚¿ã‚¹ã‚¯IDã‚’æŒ‡å®š
                "-s", "FEED_FORMAT=jsonlines",    # JSONLå½¢å¼æŒ‡å®š
                "-s", "LOG_LEVEL=INFO"
            ]

            # è¿½åŠ è¨­å®šãŒã‚ã‚Œã°é©ç”¨
            if settings:
                for key, value in settings.items():
                    cmd.extend(["-s", f"{key}={value}"])

            print(f"ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            print(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.project_path}")

            # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_path)

            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            self.scrapy_process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹: PID {self.scrapy_process.pid}")

            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = await self.scrapy_process.communicate()

            # çµæœã‚’è§£æï¼ˆæ”¹å–„ç‰ˆï¼šãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ³ã‚‚è€ƒæ…®ï¼‰
            process_success = self.scrapy_process.returncode == 0
            data_success = self.processed_lines > 0

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            success = process_success or data_success

            result = {
                'success': success,
                'process_success': process_success,
                'data_success': data_success,
                'return_code': self.scrapy_process.returncode,
                'processed_lines': self.processed_lines,
                'stdout': stdout.decode('utf-8', errors='ignore'),
                'stderr': stderr.decode('utf-8', errors='ignore')
            }

            if success:
                if process_success and data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ + ãƒ‡ãƒ¼ã‚¿å–å¾—: {self.processed_lines}ä»¶ï¼‰")
                elif data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {self.processed_lines}ä»¶ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚³ãƒ¼ãƒ‰: {self.scrapy_process.returncode}ï¼‰")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stderr: {result['stderr'][:500]}")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stdout: {result['stdout'][-500:]}")
                else:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸã€ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶ï¼‰")
            else:
                print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•— (code: {self.scrapy_process.returncode}, ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶)")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stderr: {result['stderr']}")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stdout: {result['stdout']}")

                # å¤±æ•—åŸå› ã®è©³ç´°åˆ†æ
                self._analyze_failure_cause(result)

            return result

        except Exception as e:
            print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _analyze_failure_cause(self, result: Dict[str, Any]):
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› ã‚’åˆ†æ"""
        try:
            print(f"ğŸ” === Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› åˆ†æé–‹å§‹ ===")
            print(f"ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰: {result['return_code']}")

            stderr = result['stderr']
            stdout = result['stdout']

            # ä¸€èˆ¬çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            failure_patterns = {
                'ImportError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼',
                'ModuleNotFoundError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'AttributeError': 'å±æ€§ã‚¨ãƒ©ãƒ¼',
                'SyntaxError': 'ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼',
                'IndentationError': 'ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼',
                'NameError': 'åå‰ã‚¨ãƒ©ãƒ¼',
                'TypeError': 'å‹ã‚¨ãƒ©ãƒ¼',
                'ValueError': 'å€¤ã‚¨ãƒ©ãƒ¼',
                'ConnectionError': 'æ¥ç¶šã‚¨ãƒ©ãƒ¼',
                'TimeoutError': 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼',
                'PermissionError': 'æ¨©é™ã‚¨ãƒ©ãƒ¼',
                'FileNotFoundError': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'twisted.internet.error': 'Twistedã‚¨ãƒ©ãƒ¼',
                'scrapy.exceptions': 'Scrapyã‚¨ãƒ©ãƒ¼',
                'playwright': 'Playwrightã‚¨ãƒ©ãƒ¼',
                'ERROR': 'ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼',
                'CRITICAL': 'é‡å¤§ãªã‚¨ãƒ©ãƒ¼',
                'Traceback': 'Pythonä¾‹å¤–',
                'Exception': 'ä¾‹å¤–ç™ºç”Ÿ'
            }

            detected_issues = []
            for pattern, description in failure_patterns.items():
                if pattern in stderr or pattern in stdout:
                    detected_issues.append(f"{description} ({pattern})")

            if detected_issues:
                print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
                for issue in detected_issues:
                    print(f"   - {issue}")
            else:
                print(f"ğŸ” æ—¢çŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„å¤±æ•—")

            # ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰åˆ¥ã®åˆ†æ
            return_code_meanings = {
                1: "ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼",
                2: "ã‚·ã‚§ãƒ«ã®èª¤ç”¨",
                126: "å®Ÿè¡Œæ¨©é™ãªã—",
                127: "ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                128: "ç„¡åŠ¹ãªçµ‚äº†å¼•æ•°",
                130: "Ctrl+Cã«ã‚ˆã‚‹ä¸­æ–­",
                137: "SIGKILL (å¼·åˆ¶çµ‚äº†)",
                139: "ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é•å"
            }

            if result['return_code'] in return_code_meanings:
                print(f"ğŸ” ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰ {result['return_code']}: {return_code_meanings[result['return_code']]}")

            print(f"ğŸ” === å¤±æ•—åŸå› åˆ†æå®Œäº† ===")

        except Exception as e:
            print(f"âŒ å¤±æ•—åŸå› åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _process_new_lines_threading(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        print(f"ğŸ§µ _process_new_lines_threadingé–‹å§‹: {threading.current_thread().name}")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                # ç›´æ¥DBæŒ¿å…¥å‡¦ç†ï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰
                successful_inserts = 0
                print(f"ğŸ” ç›´æ¥DBæŒ¿å…¥å‡¦ç†é–‹å§‹: {len(new_lines)}ä»¶ã®æ–°ã—ã„è¡Œ")

                # ãƒãƒ«ã‚¯DBæŒ¿å…¥å‡¦ç†
                print(f"ğŸ” ãƒãƒ«ã‚¯DBæŒ¿å…¥é–‹å§‹: {len(new_lines)}ä»¶")
                successful_inserts = self._bulk_insert_items_threading(new_lines)
                self.processed_lines += successful_inserts
                print(f"âœ… ãƒãƒ«ã‚¯DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                print(f"âœ… ç›´æ¥DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                # WebSocketé€šçŸ¥ï¼ˆthreadingç‰ˆãƒ»åŒæœŸçš„ï¼‰
                print(f"ğŸ” WebSocketé€šçŸ¥é–‹å§‹...")
                try:
                    if self.websocket_callback and successful_inserts > 0:
                        print(f"ğŸ” WebSocketé€šçŸ¥å®Ÿè¡Œä¸­...")
                        # åŒæœŸçš„ã«WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify_threading({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': successful_inserts,
                            'total_items': self.processed_lines
                        })
                        print(f"âœ… WebSocketé€šçŸ¥å®Œäº†")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, inserts={successful_inserts}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lines_threadingå®Œäº†")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰"""
        # asyncãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å‰Šé™¤ã—ã€ç›´æ¥åŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
        print(f"ğŸ” _process_new_linesé–‹å§‹ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                # ç›´æ¥DBæŒ¿å…¥å‡¦ç†ï¼ˆCeleryã‚¿ã‚¹ã‚¯ã‚’ä½¿ã‚ãªã„ï¼‰
                successful_inserts = 0
                print(f"ğŸ” ç›´æ¥DBæŒ¿å…¥å‡¦ç†é–‹å§‹: {len(new_lines)}ä»¶ã®æ–°ã—ã„è¡Œ")

                # ãƒãƒ«ã‚¯DBæŒ¿å…¥å‡¦ç†
                print(f"ğŸ” ãƒãƒ«ã‚¯DBæŒ¿å…¥é–‹å§‹: {len(new_lines)}ä»¶")
                successful_inserts = self._bulk_insert_items(new_lines)
                self.processed_lines += successful_inserts
                print(f"âœ… ãƒãƒ«ã‚¯DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                print(f"âœ… ç›´æ¥DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                # WebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ã«ï¼‰
                print(f"ğŸ” WebSocketé€šçŸ¥é–‹å§‹...")
                try:
                    if self.websocket_callback and successful_inserts > 0:
                        print(f"ğŸ” WebSocketé€šçŸ¥å®Ÿè¡Œä¸­...")
                        # åŒæœŸçš„ã«WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': successful_inserts,
                            'total_items': self.processed_lines
                        })
                        print(f"âœ… WebSocketé€šçŸ¥å®Œäº†")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, inserts={successful_inserts}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lineså®Œäº†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_remaining_lines(self):
        """æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†ï¼ˆæœ€çµ‚å‡¦ç†ï¼‰"""
        try:
            if not self.jsonl_file_path.exists():
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿ç›´ã—ã¦æœªå‡¦ç†ã®è¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len([line for line in all_lines if line.strip()])
            remaining_lines = total_lines - self.processed_lines

            if remaining_lines > 0:
                print(f"ğŸ“ æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†: {remaining_lines}ä»¶")

                # æœªå‡¦ç†ã®è¡Œã‚’ãƒãƒ«ã‚¯å‡¦ç†
                remaining_lines_data = []
                for i in range(self.processed_lines, total_lines):
                    if i < len(all_lines):
                        line = all_lines[i].strip()
                        if line:
                            remaining_lines_data.append(line)

                # ãƒãƒ«ã‚¯DBæŒ¿å…¥
                if remaining_lines_data:
                    successful_inserts = self._bulk_insert_items(remaining_lines_data)
                    self.processed_lines += successful_inserts
                    print(f"âœ… æ®‹ã‚Šè¡Œãƒãƒ«ã‚¯DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(remaining_lines_data)}ä»¶")
                else:
                    successful_inserts = 0

            print(f"âœ… æœ€çµ‚å‡¦ç†å®Œäº†: ç·å‡¦ç†è¡Œæ•° {self.processed_lines}")

        except Exception as e:
            print(f"âŒ æ®‹ã‚Šè¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _sync_insert_item_threading(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        max_retries = 3
        retry_count = 0

        print(f"ğŸ§µ DBæŒ¿å…¥é–‹å§‹: {threading.current_thread().name}")

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                    data_hash = self._generate_data_hash_improved(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()
                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash}")
                            return True  # é‡è¤‡ã¯æˆåŠŸã¨ã¿ãªã™

                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,  # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¿½åŠ 
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1}) - Thread: {threading.current_thread().name}")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆthreadingç‰ˆï¼‰
                    self._update_task_statistics_threading()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _sync_insert_item(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                    data_hash = self._generate_data_hash_improved(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()
                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash}")
                            return True  # é‡è¤‡ã¯æˆåŠŸã¨ã¿ãªã™

                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,  # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¿½åŠ 
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1})")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆåˆ¥ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§ï¼‰
                    self._update_task_statistics_safe()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _realtime_insert_item_threading(self, line: str) -> bool:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ï¼ˆthreadingç‰ˆãƒ»1ä»¶ãšã¤å‡¦ç†ï¼‰"""
        if not line.strip():
            return False

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result
                import json
                import uuid
                import hashlib
                from datetime import datetime

                # JSONè§£æ
                try:
                    item_data = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                    return False

                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                data_hash = self._generate_data_hash_improved(item_data)

                db = SessionLocal()
                try:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()

                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                            return False

                    # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                    result_id = str(uuid.uuid4())
                    new_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(new_result)
                    db.commit()

                    print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æˆåŠŸ: {result_id[:8]}...")
                    return True

                except Exception as e:
                    db.rollback()
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æœ€çµ‚å¤±æ•—: {e}")
                    return False
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return False

    def _realtime_insert_item(self, line: str) -> bool:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ï¼ˆé€šå¸¸ç‰ˆãƒ»1ä»¶ãšã¤å‡¦ç†ï¼‰"""
        if not line.strip():
            return False

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result
                import json
                import uuid
                import hashlib
                from datetime import datetime

                # JSONè§£æ
                try:
                    item_data = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                    return False

                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                data_hash = self._generate_data_hash_improved(item_data)

                db = SessionLocal()
                try:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()

                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                            return False

                    # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                    result_id = str(uuid.uuid4())
                    new_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(new_result)
                    db.commit()

                    print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æˆåŠŸ: {result_id[:8]}...")
                    return True

                except Exception as e:
                    db.rollback()
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æœ€çµ‚å¤±æ•—: {e}")
                    return False
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return False

    def _bulk_insert_items_threading(self, lines: List[str]) -> int:
        """ãƒãƒ«ã‚¯DBæŒ¿å…¥ï¼ˆthreadingç‰ˆï¼‰"""
        if not lines:
            return 0

        max_retries = 3
        retry_count = 0
        batch_size = 100  # ãƒãƒƒãƒã‚µã‚¤ã‚º

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result

                # JSONè§£æã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                items_data = []
                for line in lines:
                    try:
                        item_data = json.loads(line.strip())
                        items_data.append(item_data)
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:50]}...")
                        continue

                if not items_data:
                    print("âŒ æœ‰åŠ¹ãªJSONãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return 0

                successful_inserts = 0

                # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
                for i in range(0, len(items_data), batch_size):
                    batch = items_data[i:i + batch_size]

                    db = SessionLocal()
                    try:
                        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã— - Rich progressã§å¾Œå‡¦ç†ï¼‰
                        bulk_data = []

                        for item_data in batch:
                            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                            data_hash = self._generate_data_hash_improved(item_data)

                            if not data_hash:
                                print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆå¤±æ•—: {item_data}")
                                continue

                            result_id = str(uuid.uuid4())
                            bulk_item = {
                                'id': result_id,
                                'task_id': self.task_id,
                                'data': item_data,
                                'data_hash': data_hash,
                                'item_acquired_datetime': datetime.now(),
                                'created_at': datetime.now()
                            }
                            bulk_data.append(bulk_item)

                        # é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        if bulk_data:
                            print(f"ğŸš€ é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ: {len(bulk_data)}ä»¶")

                            for item in bulk_data:
                                try:
                                    db_result = Result(
                                        id=item['id'],
                                        task_id=item['task_id'],
                                        data=item['data'],
                                        data_hash=item['data_hash'],
                                        item_acquired_datetime=item['item_acquired_datetime'],
                                        created_at=item['created_at']
                                    )
                                    db.add(db_result)
                                except Exception as e:
                                    print(f"âŒ ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                                    continue

                            db.commit()
                            print(f"âœ… é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {len(bulk_data)}ä»¶")
                        else:
                            print("âš ï¸ ãƒãƒ«ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

                        successful_inserts += len(batch)
                        print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {len(batch)}ä»¶ (ç´¯è¨ˆ: {successful_inserts}/{len(items_data)}) - Thread: {threading.current_thread().name}")

                    except Exception as e:
                        db.rollback()
                        print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i//batch_size + 1}): {e}")
                        # ãƒãƒƒãƒãŒå¤±æ•—ã—ãŸå ´åˆã¯å€‹åˆ¥ã«å‡¦ç†ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        for item_data in batch:
                            try:
                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                                data_hash = self._generate_data_hash_improved(item_data)

                                result_id = str(uuid.uuid4())
                                db_result = Result(
                                    id=result_id,
                                    task_id=self.task_id,
                                    data=item_data,
                                    data_hash=data_hash,
                                    item_acquired_datetime=datetime.now(),
                                    created_at=datetime.now()
                                )
                                db.add(db_result)
                                db.commit()
                                successful_inserts += 1
                            except Exception as individual_error:
                                db.rollback()
                                print(f"âŒ å€‹åˆ¥ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {individual_error}")
                    finally:
                        db.close()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                if successful_inserts > 0:
                    self._update_task_statistics_threading()

                print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {successful_inserts}/{len(items_data)}ä»¶ - Thread: {threading.current_thread().name}")
                return successful_inserts

            except Exception as e:
                retry_count += 1
                print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                if retry_count >= max_retries:
                    print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return 0
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return 0

    def _bulk_insert_items(self, lines: List[str]) -> int:
        """ãƒãƒ«ã‚¯DBæŒ¿å…¥ï¼ˆé€šå¸¸ç‰ˆï¼‰"""
        if not lines:
            return 0

        max_retries = 3
        retry_count = 0
        batch_size = 100  # ãƒãƒƒãƒã‚µã‚¤ã‚º

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result

                # JSONè§£æã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                items_data = []
                for line in lines:
                    try:
                        item_data = json.loads(line.strip())
                        items_data.append(item_data)
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:50]}...")
                        continue

                if not items_data:
                    print("âŒ æœ‰åŠ¹ãªJSONãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return 0

                successful_inserts = 0

                # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
                for i in range(0, len(items_data), batch_size):
                    batch = items_data[i:i + batch_size]

                    db = SessionLocal()
                    try:
                        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã— - Rich progressã§å¾Œå‡¦ç†ï¼‰
                        bulk_data = []

                        for item_data in batch:
                            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                            data_hash = self._generate_data_hash_improved(item_data)

                            if not data_hash:
                                print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆå¤±æ•—: {item_data}")
                                continue

                            result_id = str(uuid.uuid4())
                            bulk_data.append({
                                'id': result_id,
                                'task_id': self.task_id,
                                'data': item_data,
                                'data_hash': data_hash,
                                'item_acquired_datetime': datetime.now(),
                                'created_at': datetime.now()
                            })

                        # é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        if bulk_data:
                            print(f"ğŸš€ é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ: {len(bulk_data)}ä»¶")

                            db.bulk_insert_mappings(Result, bulk_data)
                            db.commit()
                        else:
                            print("âš ï¸ ãƒãƒ«ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")

                        successful_inserts += len(batch)
                        print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {len(batch)}ä»¶ (ç´¯è¨ˆ: {successful_inserts}/{len(items_data)})")

                    except Exception as e:
                        db.rollback()
                        print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i//batch_size + 1}): {e}")
                        # ãƒãƒƒãƒãŒå¤±æ•—ã—ãŸå ´åˆã¯å€‹åˆ¥ã«å‡¦ç†ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        for item_data in batch:
                            try:
                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                                data_hash = self._generate_data_hash_improved(item_data)

                                result_id = str(uuid.uuid4())
                                db_result = Result(
                                    id=result_id,
                                    task_id=self.task_id,
                                    data=item_data,
                                    data_hash=data_hash,
                                    item_acquired_datetime=datetime.now(),
                                    created_at=datetime.now()
                                )
                                db.add(db_result)
                                db.commit()
                                successful_inserts += 1
                            except Exception as individual_error:
                                db.rollback()
                                print(f"âŒ å€‹åˆ¥ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {individual_error}")
                    finally:
                        db.close()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                if successful_inserts > 0:
                    self._update_task_statistics_safe()

                print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {successful_inserts}/{len(items_data)}ä»¶")
                return successful_inserts

            except Exception as e:
                retry_count += 1
                print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                if retry_count >= max_retries:
                    print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return 0
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return 0

    def _update_task_statistics(self, db):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°"""
        try:
            from ..database import Task, Result

            # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = db.query(Task).filter(Task.id == self.task_id).first()
            if task:
                # çµæœæ•°ã‚’å–å¾—
                result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®æ­£å¸¸åŒ–ï¼‰
                task.items_count = result_count

                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®æ­£å¸¸åŒ–ï¼ˆç•°å¸¸ã«å¤§ãã„å€¤ã‚’é˜²æ­¢ï¼‰
                estimated_normal_requests = result_count + 20  # ã‚¢ã‚¤ãƒ†ãƒ æ•° + åˆæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
                current_requests = task.requests_count or 0

                if current_requests <= estimated_normal_requests * 2:
                    # ç¾åœ¨å€¤ãŒæ­£å¸¸ç¯„å›²å†…ã®å ´åˆã¯ãã®ã¾ã¾
                    pass
                else:
                    # ç•°å¸¸ã«å¤§ãã„å ´åˆã¯æ¨å®šå€¤ã«ä¿®æ­£
                    task.requests_count = estimated_normal_requests
                    print(f"âš ï¸ Watchdog: Abnormal request count detected for task {self.task_id}, corrected to {estimated_normal_requests}")

                task.updated_at = datetime.now()

                db.commit()
                print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id} - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_threading(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            from ..database import SessionLocal, Task, Result

            print(f"ğŸ§µ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°é–‹å§‹: {threading.current_thread().name}")
            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆé‡è¤‡é˜²æ­¢ï¼šæœ€å¤§å€¤ã®ã¿æ›´æ–°ï¼‰
                    task.items_count = max(result_count, task.items_count or 0)

                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¯æ¨å®šå€¤ã¨ç¾åœ¨å€¤ã®æœ€å¤§å€¤
                    estimated_requests = result_count + 15
                    task.requests_count = max(estimated_requests, task.requests_count or 0)

                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count} - Thread: {threading.current_thread().name}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_safe(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆåˆ¥ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ï¼‰"""
        try:
            from ..database import SessionLocal, Task, Result

            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆé‡è¤‡é˜²æ­¢ï¼šæœ€å¤§å€¤ã®ã¿æ›´æ–°ï¼‰
                    task.items_count = max(result_count, task.items_count or 0)

                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¯æ¨å®šå€¤ã¨ç¾åœ¨å€¤ã®æœ€å¤§å€¤
                    estimated_requests = result_count + 15
                    task.requests_count = max(estimated_requests, task.requests_count or 0)

                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify_threading(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            if not self.websocket_callback:
                return

            print(f"ğŸ§µ WebSocketé€šçŸ¥é–‹å§‹: {threading.current_thread().name}")

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')} - Thread: {threading.current_thread().name}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ï¼‰"""
        try:
            if not self.websocket_callback:
                return

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def test_scrapy_watchdog_monitor():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ¯ scrapy crawl + watchdogç›£è¦–ãƒ†ã‚¹ãƒˆ")

    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ WebSocketé€šçŸ¥: {data}")

    # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    monitor = ScrapyWatchdogMonitor(
        task_id="test_task_123",
        project_path="scrapy_projects/test_project",  # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«å¤‰æ›´
        spider_name="test_spider",  # å®Ÿéš›ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã«å¤‰æ›´
        websocket_callback=websocket_callback
    )

    # å®Ÿè¡Œ
    result = await monitor.execute_spider_with_monitoring(
        settings={
            'LOG_LEVEL': 'INFO',
            'ROBOTSTXT_OBEY': False
        }
    )

    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")


if __name__ == "__main__":
    if WATCHDOG_AVAILABLE:
        asyncio.run(test_scrapy_watchdog_monitor())
    else:
        print("âŒ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install watchdog")
