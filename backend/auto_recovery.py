#!/usr/bin/env python3
"""
ScrapyUI Auto Recovery - è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ 
ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸ã‚’æ¤œçŸ¥ã—ã€è‡ªå‹•çš„ã«å¾©æ—§å‡¦ç†ã‚’å®Ÿè¡Œ
"""

import os
import sys
import time
import subprocess
import psutil
import logging
import json
import requests
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import sqlite3

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/auto_recovery.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class AutoRecovery:
    """è‡ªå‹•å¾©æ—§ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.backend_dir = Path(__file__).parent
        self.project_root = self.backend_dir.parent
        
        # å¾©æ—§ã‚·ãƒŠãƒªã‚ª
        self.recovery_scenarios = {
            'backend_down': {
                'description': 'ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼åœæ­¢',
                'check': self.check_backend_down,
                'recovery': self.recover_backend,
                'priority': 1
            },
            'celery_down': {
                'description': 'Celeryãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢',
                'check': self.check_celery_down,
                'recovery': self.recover_celery,
                'priority': 2
            },
            'database_locked': {
                'description': 'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯',
                'check': self.check_database_locked,
                'recovery': self.recover_database,
                'priority': 3
            },
            'disk_full': {
                'description': 'ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³',
                'check': self.check_disk_full,
                'recovery': self.recover_disk_space,
                'priority': 4
            },
            'memory_leak': {
                'description': 'ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯',
                'check': self.check_memory_leak,
                'recovery': self.recover_memory,
                'priority': 5
            },
            'zombie_processes': {
                'description': 'ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹',
                'check': self.check_zombie_processes,
                'recovery': self.recover_zombies,
                'priority': 6
            }
        }
        
        self.recovery_stats = {
            'total_recoveries': 0,
            'successful_recoveries': 0,
            'failed_recoveries': 0,
            'last_recovery': None,
            'recovery_history': []
        }
    
    def check_backend_down(self) -> bool:
        """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼åœæ­¢ãƒã‚§ãƒƒã‚¯"""
        try:
            response = requests.get('http://localhost:8000/health', timeout=5)
            return response.status_code != 200
        except:
            return True
    
    def check_celery_down(self) -> bool:
        """Celeryãƒ¯ãƒ¼ã‚«ãƒ¼åœæ­¢ãƒã‚§ãƒƒã‚¯"""
        try:
            # Celeryãƒ—ãƒ­ã‚»ã‚¹ã®å­˜åœ¨ç¢ºèª
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                if 'celery' in proc.info['name'] or any('celery' in arg for arg in proc.info['cmdline'] or []):
                    return False
            return True
        except:
            return True
    
    def check_database_locked(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒƒã‚¯ãƒã‚§ãƒƒã‚¯"""
        try:
            db_path = self.backend_dir / 'database' / 'scrapy_ui.db'
            if not db_path.exists():
                return False
            
            conn = sqlite3.connect(str(db_path), timeout=1)
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            conn.close()
            return False
        except sqlite3.OperationalError:
            return True
        except:
            return False
    
    def check_disk_full(self) -> bool:
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ä¸è¶³ãƒã‚§ãƒƒã‚¯"""
        try:
            usage = psutil.disk_usage('/')
            free_percent = (usage.free / usage.total) * 100
            return free_percent < 5  # 5%æœªæº€ã§è­¦å‘Š
        except:
            return False
    
    def check_memory_leak(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãƒã‚§ãƒƒã‚¯"""
        try:
            memory = psutil.virtual_memory()
            return memory.percent > 90  # 90%ä»¥ä¸Šã§è­¦å‘Š
        except:
            return False
    
    def check_zombie_processes(self) -> bool:
        """ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            zombie_count = 0
            for proc in psutil.process_iter(['pid', 'status']):
                if proc.info['status'] == psutil.STATUS_ZOMBIE:
                    zombie_count += 1
            return zombie_count > 5  # 5å€‹ä»¥ä¸Šã§è­¦å‘Š
        except:
            return False
    
    def recover_backend(self) -> bool:
        """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚µãƒ¼ãƒãƒ¼å¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering backend server...")
            
            # æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
            subprocess.run(['pkill', '-f', 'uvicorn.*app.main'], check=False)
            time.sleep(2)
            
            # æ–°ã—ã„ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•
            subprocess.Popen(
                ['python', 'uvicorn_config.py'],
                cwd=str(self.backend_dir),
                stdout=subprocess.DEVNULL,
                stderr=subprocess.DEVNULL
            )
            
            # èµ·å‹•ç¢ºèª
            for i in range(30):
                if not self.check_backend_down():
                    logger.info("âœ… Backend server recovered")
                    return True
                time.sleep(1)
            
            logger.error("âŒ Failed to recover backend server")
            return False
            
        except Exception as e:
            logger.error(f"âŒ Error recovering backend: {str(e)}")
            return False
    
    def recover_celery(self) -> bool:
        """Celeryãƒ¯ãƒ¼ã‚«ãƒ¼å¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering Celery worker...")
            
            # æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢
            subprocess.run(['pkill', '-f', 'celery.*worker'], check=False)
            time.sleep(2)
            
            # æ–°ã—ã„ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•
            subprocess.Popen([
                'python', '-m', 'celery', '-A', 'app.celery_app', 'worker',
                '--loglevel=info', '-Q', 'scrapy,maintenance,monitoring',
                '--concurrency=4', '--pool=prefork'
            ], cwd=str(self.backend_dir),
               stdout=subprocess.DEVNULL,
               stderr=subprocess.DEVNULL)
            
            # èµ·å‹•ç¢ºèª
            time.sleep(5)
            if not self.check_celery_down():
                logger.info("âœ… Celery worker recovered")
                return True
            else:
                logger.error("âŒ Failed to recover Celery worker")
                return False
                
        except Exception as e:
            logger.error(f"âŒ Error recovering Celery: {str(e)}")
            return False
    
    def recover_database(self) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering database...")
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            db_path = self.backend_dir / 'database' / 'scrapy_ui.db'
            backup_path = self.backend_dir / 'database' / f'scrapy_ui_backup_{int(time.time())}.db'
            
            if db_path.exists():
                subprocess.run(['cp', str(db_path), str(backup_path)], check=False)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’å¼·åˆ¶çµ‚äº†
            subprocess.run(['pkill', '-f', 'python.*app.main'], check=False)
            time.sleep(2)
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
            try:
                conn = sqlite3.connect(str(db_path))
                cursor = conn.cursor()
                cursor.execute("PRAGMA integrity_check")
                result = cursor.fetchone()
                conn.close()
                
                if result[0] != 'ok':
                    logger.warning("âš ï¸ Database integrity issues detected")
                    
            except Exception as e:
                logger.error(f"Database integrity check failed: {str(e)}")
            
            logger.info("âœ… Database recovery completed")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error recovering database: {str(e)}")
            return False
    
    def recover_disk_space(self) -> bool:
        """ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡å¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering disk space...")
            
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            log_dir = self.backend_dir / 'logs'
            if log_dir.exists():
                for log_file in log_dir.glob('*.log.*'):
                    if log_file.stat().st_mtime < time.time() - 86400 * 7:  # 7æ—¥ä»¥ä¸Šå¤ã„
                        log_file.unlink()
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            temp_dirs = ['/tmp', '/var/tmp']
            for temp_dir in temp_dirs:
                subprocess.run(['find', temp_dir, '-name', 'scrapy*', '-mtime', '+1', '-delete'], 
                             check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆ30æ—¥ä»¥ä¸Šå¤ã„ï¼‰
            scrapy_projects = self.project_root / 'scrapy_projects'
            if scrapy_projects.exists():
                subprocess.run(['find', str(scrapy_projects), '-name', 'results_*.json', '-mtime', '+30', '-delete'],
                             check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            logger.info("âœ… Disk space recovery completed")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error recovering disk space: {str(e)}")
            return False
    
    def recover_memory(self) -> bool:
        """ãƒ¡ãƒ¢ãƒªå¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering memory...")
            
            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤šã„ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç‰¹å®š
            processes = []
            for proc in psutil.process_iter(['pid', 'name', 'memory_percent', 'cmdline']):
                if proc.info['memory_percent'] > 5:  # 5%ä»¥ä¸Šã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨
                    processes.append(proc.info)
            
            # ScrapyUIãƒ—ãƒ­ã‚»ã‚¹ã‚’å†èµ·å‹•
            subprocess.run(['pkill', '-f', 'python.*app.main'], check=False)
            subprocess.run(['pkill', '-f', 'celery.*worker'], check=False)
            time.sleep(3)
            
            # ã‚·ã‚¹ãƒ†ãƒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢
            subprocess.run(['sync'], check=False)
            subprocess.run(['echo', '3', '>', '/proc/sys/vm/drop_caches'], 
                         shell=True, check=False, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            
            logger.info("âœ… Memory recovery completed")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error recovering memory: {str(e)}")
            return False
    
    def recover_zombies(self) -> bool:
        """ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹å¾©æ—§"""
        try:
            logger.info("ğŸ”§ Recovering zombie processes...")
            
            # ã‚¾ãƒ³ãƒ“ãƒ—ãƒ­ã‚»ã‚¹ã®è¦ªãƒ—ãƒ­ã‚»ã‚¹ã‚’ç‰¹å®šã—ã¦å†èµ·å‹•
            zombie_parents = set()
            for proc in psutil.process_iter(['pid', 'ppid', 'status']):
                if proc.info['status'] == psutil.STATUS_ZOMBIE:
                    zombie_parents.add(proc.info['ppid'])
            
            # è¦ªãƒ—ãƒ­ã‚»ã‚¹ã« SIGCHLD ã‚’é€ä¿¡
            for ppid in zombie_parents:
                try:
                    os.kill(ppid, signal.SIGCHLD)
                except:
                    pass
            
            time.sleep(2)
            
            # ã¾ã ã‚¾ãƒ³ãƒ“ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã¯å¼·åˆ¶çµ‚äº†
            remaining_zombies = 0
            for proc in psutil.process_iter(['pid', 'status']):
                if proc.info['status'] == psutil.STATUS_ZOMBIE:
                    remaining_zombies += 1
                    try:
                        os.kill(proc.info['pid'], signal.SIGKILL)
                    except:
                        pass
            
            logger.info(f"âœ… Zombie process recovery completed (cleaned {len(zombie_parents)} parents)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Error recovering zombies: {str(e)}")
            return False
    
    def run_recovery(self) -> Dict:
        """å¾©æ—§å‡¦ç†ã‚’å®Ÿè¡Œ"""
        logger.info("ğŸ” Starting auto recovery check...")
        
        recovery_results = {
            'timestamp': datetime.now().isoformat(),
            'scenarios_checked': 0,
            'issues_found': 0,
            'recoveries_attempted': 0,
            'recoveries_successful': 0,
            'details': []
        }
        
        # å„ªå…ˆåº¦é †ã«ãƒã‚§ãƒƒã‚¯
        scenarios = sorted(self.recovery_scenarios.items(), key=lambda x: x[1]['priority'])
        
        for name, scenario in scenarios:
            recovery_results['scenarios_checked'] += 1
            
            try:
                if scenario['check']():
                    recovery_results['issues_found'] += 1
                    logger.warning(f"âš ï¸ Issue detected: {scenario['description']}")
                    
                    recovery_results['recoveries_attempted'] += 1
                    self.recovery_stats['total_recoveries'] += 1
                    
                    if scenario['recovery']():
                        recovery_results['recoveries_successful'] += 1
                        self.recovery_stats['successful_recoveries'] += 1
                        recovery_results['details'].append({
                            'scenario': name,
                            'description': scenario['description'],
                            'status': 'success'
                        })
                    else:
                        self.recovery_stats['failed_recoveries'] += 1
                        recovery_results['details'].append({
                            'scenario': name,
                            'description': scenario['description'],
                            'status': 'failed'
                        })
                        
            except Exception as e:
                logger.error(f"âŒ Error checking {name}: {str(e)}")
                recovery_results['details'].append({
                    'scenario': name,
                    'description': scenario['description'],
                    'status': 'error',
                    'error': str(e)
                })
        
        self.recovery_stats['last_recovery'] = datetime.now()
        self.recovery_stats['recovery_history'].append(recovery_results)
        
        # å±¥æ­´ã¯æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
        if len(self.recovery_stats['recovery_history']) > 100:
            self.recovery_stats['recovery_history'] = self.recovery_stats['recovery_history'][-100:]
        
        logger.info(f"âœ… Recovery check completed: {recovery_results['recoveries_successful']}/{recovery_results['recoveries_attempted']} successful")
        
        return recovery_results


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ScrapyUI Auto Recovery")
    parser.add_argument("--interval", type=int, default=60, help="Check interval in seconds")
    parser.add_argument("--once", action="store_true", help="Run once and exit")
    
    args = parser.parse_args()
    
    recovery = AutoRecovery()
    
    if args.once:
        result = recovery.run_recovery()
        print(json.dumps(result, indent=2))
    else:
        logger.info(f"ğŸ”„ Starting auto recovery daemon (interval: {args.interval}s)")
        
        while True:
            try:
                recovery.run_recovery()
                time.sleep(args.interval)
            except KeyboardInterrupt:
                logger.info("ğŸ›‘ Auto recovery stopped by user")
                break
            except Exception as e:
                logger.error(f"âŒ Error in recovery loop: {str(e)}")
                time.sleep(30)


if __name__ == "__main__":
    main()
