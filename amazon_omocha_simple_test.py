import scrapy
from scrapy_playwright.page import PageMethod
import json
import re
from urllib.parse import urljoin


class OmochaSpider(scrapy.Spider):
    name = "omocha"
    allowed_domains = ["amazon.co.jp"]
    start_urls = ["https://www.amazon.co.jp/gp/bestsellers/toys/ref=zg_bs_nav_toys_0"]
    
    # è¶…ã‚·ãƒ³ãƒ—ãƒ«è¨­å®š
    custom_settings = {
        'DOWNLOAD_DELAY': 2,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        
        # Playwrightå¿…é ˆè¨­å®šã®ã¿
        'DOWNLOAD_HANDLERS': {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        
        'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage']
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 30000,
        
        # å…¨ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ç„¡åŠ¹åŒ–
        'DOWNLOADER_MIDDLEWARES': {},
        
        'LOG_LEVEL': 'INFO',
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    def start_requests(self):
        """è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªé–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        self.logger.info('ğŸš€ Starting SIMPLE test spider...')
        
        for url in self.start_urls:
            self.logger.info(f'ğŸ”— Requesting: {url}')
            yield scrapy.Request(
                url=url,
                callback=self.parse,
                meta={
                    'playwright': True,
                    'playwright_include_page': True,
                    'playwright_page_methods': [
                        PageMethod('wait_for_load_state', 'domcontentloaded'),
                        PageMethod('wait_for_timeout', 5000),
                    ]
                }
            )

    async def parse(self, response):
        """è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªè§£æ"""
        page = response.meta['playwright_page']
        
        try:
            self.logger.info(f'âœ… Successfully loaded page: {response.url}')
            self.logger.info(f'ğŸ“„ Status: {response.status}')
            self.logger.info(f'ğŸ“ Content length: {len(response.text)} chars')
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            title = response.css('title::text').get()
            self.logger.info(f'ğŸ“‹ Page title: {title}')
            
            # å•†å“ãƒªãƒ³ã‚¯ã‚’è¤‡æ•°ã®æ–¹æ³•ã§æ¢ã™
            # æ–¹æ³•1: æŒ‡å®šã•ã‚ŒãŸã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            links1 = response.css('a.a-link-normal.aok-block::attr(href)').getall()
            self.logger.info(f'ğŸ”— Method 1 (a.a-link-normal.aok-block): {len(links1)} links')
            
            # æ–¹æ³•2: Amazonå•†å“ãƒšãƒ¼ã‚¸ã®ãƒ‘ã‚¿ãƒ¼ãƒ³
            links2 = response.css('a[href*="/dp/"]::attr(href)').getall()
            self.logger.info(f'ğŸ”— Method 2 (a[href*="/dp/"]): {len(links2)} links')
            
            # æ–¹æ³•3: å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‹ã‚‰Amazonå•†å“ã‚’æŠ½å‡º
            all_links = response.css('a::attr(href)').getall()
            links3 = [link for link in all_links if link and '/dp/' in link]
            self.logger.info(f'ğŸ”— Method 3 (all Amazon product links): {len(links3)} links')
            
            # æ–¹æ³•4: ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ç‰¹æœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            links4 = response.css('div[data-testid="zg-item"] a::attr(href)').getall()
            self.logger.info(f'ğŸ”— Method 4 (zg-item): {len(links4)} links')
            
            # æ–¹æ³•5: ã‚ˆã‚Šåºƒç¯„å›²ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            links5 = response.css('div.zg-item-immersion a::attr(href)').getall()
            self.logger.info(f'ğŸ”— Method 5 (zg-item-immersion): {len(links5)} links')
            
            # ä½¿ç”¨ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ±ºå®š
            product_links = links1 or links2 or links3[:10] or links4 or links5
            self.logger.info(f'ğŸ“¦ Using {len(product_links)} links for processing')
            
            # æœ€åˆã®1ã¤ã ã‘ãƒ†ã‚¹ãƒˆ
            if product_links:
                link = product_links[0]
                full_url = urljoin(response.url, link)
                self.logger.info(f'ğŸ¯ Testing first product: {full_url}')
                
                yield scrapy.Request(
                    url=full_url,
                    callback=self.parse_product,
                    meta={
                        'playwright': True,
                        'playwright_include_page': True,
                        'playwright_page_methods': [
                            PageMethod('wait_for_load_state', 'domcontentloaded'),
                            PageMethod('wait_for_timeout', 3000),
                        ]
                    }
                )
            else:
                self.logger.warning('âš ï¸ No product links found with any method!')
                
                # HTMLã®ä¸€éƒ¨ã‚’ãƒ­ã‚°ã«å‡ºåŠ›ã—ã¦ãƒ‡ãƒãƒƒã‚°
                html_snippet = response.text[:2000]
                self.logger.info(f'ğŸ“„ HTML snippet: {html_snippet}')
            
        except Exception as e:
            self.logger.error(f'âŒ Error in parse: {str(e)}')
            import traceback
            self.logger.error(f'ğŸ“‹ Traceback: {traceback.format_exc()}')
        finally:
            await page.close()

    async def parse_product(self, response):
        """è¶…ã‚·ãƒ³ãƒ—ãƒ«ãªå•†å“è§£æ"""
        page = response.meta['playwright_page']
        
        try:
            self.logger.info(f'ğŸ” Parsing product: {response.url}')
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
            title = None
            title_selectors = [
                '#title span::text',
                '#productTitle::text',
                'h1 span::text'
            ]
            
            for selector in title_selectors:
                elements = response.css(selector).getall()
                if elements:
                    title = ' '.join([t.strip() for t in elements if t.strip()])
                    self.logger.info(f'âœ… Title found: {title[:100]}...')
                    break
            
            if not title:
                title = 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜'
                self.logger.warning('âš ï¸ No title found')
            
            # ç°¡å˜ãªã‚¢ã‚¤ãƒ†ãƒ ã‚’ä½œæˆ
            item = {
                'title': title,
                'product_url': response.url,
                'scraped_at': scrapy.utils.misc.load_object('datetime.datetime').now().isoformat()
            }
            
            self.logger.info(f'âœ… Successfully scraped: {title}')
            yield item
            
        except Exception as e:
            self.logger.error(f'âŒ Error parsing product: {str(e)}')
            import traceback
            self.logger.error(f'ğŸ“‹ Traceback: {traceback.format_exc()}')
        finally:
            await page.close()
