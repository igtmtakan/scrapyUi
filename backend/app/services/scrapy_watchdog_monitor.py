#!/usr/bin/env python3
"""
scrapy crawlã‚³ãƒãƒ³ãƒ‰ + watchdogç›£è¦–ã®å®Ÿè£…
backend/app/services/scrapy_watchdog_monitor.py
"""
import asyncio
import subprocess
import json
import sqlite3
import threading
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
import uuid
import os
import sys

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install watchdog")


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, monitor):
        self.monitor = monitor
        self.last_size = 0

    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return

        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self._handle_file_change,
                daemon=True
            ).start()

    def _handle_file_change(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®å‡¦ç†ï¼ˆé‡è¤‡é˜²æ­¢ã®ãŸã‚DBæŒ¿å…¥ç„¡åŠ¹åŒ–ï¼‰"""
        try:
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
            print(f"â„¹ï¸ DBæŒ¿å…¥ã¯crawlwithwatchdogã‚³ãƒãƒ³ãƒ‰ãŒå‡¦ç†ã™ã‚‹ãŸã‚ã€watchdogç›£è¦–ã§ã¯å®Ÿè¡Œã—ã¾ã›ã‚“")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã®ã¿æ›´æ–°ï¼ˆDBæŒ¿å…¥ã¯è¡Œã‚ãªã„ï¼‰
            if self.monitor.jsonl_file_path.exists():
                current_size = self.monitor.jsonl_file_path.stat().st_size
                print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {self.monitor.last_file_size} â†’ {current_size}")
                self.monitor.last_file_size = current_size

                # è¡Œæ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                with open(self.monitor.jsonl_file_path, 'r', encoding='utf-8') as f:
                    lines = [line.strip() for line in f.readlines() if line.strip()]
                    self.monitor.processed_lines = len(lines)
                    print(f"ğŸ“Š ç¾åœ¨ã®è¡Œæ•°: {len(lines)}è¡Œ")

                # WebSocketé€šçŸ¥ã®ã¿é€ä¿¡ï¼ˆDBæŒ¿å…¥ãªã—ï¼‰
                if self.monitor.websocket_callback:
                    try:
                        import requests
                        response = requests.post(
                            'http://localhost:8000/api/tasks/internal/websocket-notify',
                            json={
                                'type': 'file_update',
                                'task_id': self.monitor.task_id,
                                'file_lines': len(lines),
                                'message': 'ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ¤œå‡ºï¼ˆDBæŒ¿å…¥ã¯crawlwithwatchdogãŒå‡¦ç†ï¼‰'
                            },
                            timeout=5
                        )
                        if response.status_code == 200:
                            print(f"ğŸ“¡ WebSocketé€šçŸ¥é€ä¿¡å®Œäº†")
                    except Exception as ws_error:
                        print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")


class ScrapyWatchdogMonitor:
    """scrapy crawl + watchdogç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self,
                 task_id: str,
                 project_path: str,
                 spider_name: str,
                 db_path: str = "backend/database/scrapy_ui.db",
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.project_path = Path(project_path)
        self.spider_name = spider_name
        self.db_path = db_path
        self.websocket_callback = websocket_callback

        # ç›£è¦–çŠ¶æ…‹
        self.is_monitoring = False
        self.observer = None
        self.loop = None
        self.processed_lines = 0
        self.last_file_size = 0

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.jsonl_file_path = self.project_path / f"results_{task_id}.jsonl"

        # Scrapyãƒ—ãƒ­ã‚»ã‚¹
        self.scrapy_process = None

    async def execute_spider_with_monitoring(self,
                                           settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """watchdogç›£è¦–ä»˜ãã§scrapy crawlã‚’å®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {self.spider_name}")

            # ç¾åœ¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä¿å­˜
            self.loop = asyncio.get_event_loop()

            # 1. watchdogç›£è¦–ã‚’é–‹å§‹
            await self._start_watchdog_monitoring()

            # 2. scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            scrapy_task = asyncio.create_task(
                self._execute_scrapy_crawl(settings)
            )

            # 3. ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã¾ã§å¾…æ©Ÿ
            scrapy_result = await scrapy_task

            # 4. å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç›£è¦–åœæ­¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç¢ºå®Ÿã«å‡¦ç†ï¼‰
            await asyncio.sleep(2)
            self._stop_watchdog_monitoring()

            # 5. æœ€çµ‚çš„ãªçµæœå‡¦ç†
            await self._process_remaining_lines()

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            final_success = scrapy_result['success'] or (self.processed_lines > 0)

            return {
                'success': final_success,
                'process_success': scrapy_result.get('process_success', scrapy_result['success']),
                'data_success': self.processed_lines > 0,
                'task_id': self.task_id,
                'items_processed': self.processed_lines,
                'scrapy_result': scrapy_result,
                'jsonl_file': str(self.jsonl_file_path)
            }

        except Exception as e:
            self._stop_watchdog_monitoring()
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }

    async def _start_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        if not WATCHDOG_AVAILABLE:
            raise Exception("watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

        self.is_monitoring = True

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
        watch_directory = self.jsonl_file_path.parent

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        event_handler = JSONLWatchdogHandler(self)

        # Observerã‚’ä½œæˆã—ã¦ç›£è¦–é–‹å§‹
        self.observer = Observer()
        self.observer.schedule(event_handler, str(watch_directory), recursive=False)
        self.observer.start()

        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {watch_directory}")
        print(f"ğŸ“„ ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {self.jsonl_file_path}")

    def _stop_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False

        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None

        print(f"ğŸ›‘ watchdogç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")

    async def _execute_scrapy_crawl(self, settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """scrapy crawlwithwatchdogã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆDBæŒ¿å…¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆcrawlwithwatchdogã‚’ä½¿ç”¨ï¼‰
            cmd = [
                sys.executable, "-m", "scrapy", "crawlwithwatchdog", self.spider_name,
                "-o", str(self.jsonl_file_path),  # JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                "--task-id", self.task_id,        # ã‚¿ã‚¹ã‚¯IDã‚’æŒ‡å®š
                "-s", "FEED_FORMAT=jsonlines",    # JSONLå½¢å¼æŒ‡å®š
                "-s", "LOG_LEVEL=INFO"
            ]

            # è¿½åŠ è¨­å®šãŒã‚ã‚Œã°é©ç”¨
            if settings:
                for key, value in settings.items():
                    cmd.extend(["-s", f"{key}={value}"])

            print(f"ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            print(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.project_path}")

            # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_path)

            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            self.scrapy_process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹: PID {self.scrapy_process.pid}")

            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = await self.scrapy_process.communicate()

            # çµæœã‚’è§£æï¼ˆæ”¹å–„ç‰ˆï¼šãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ³ã‚‚è€ƒæ…®ï¼‰
            process_success = self.scrapy_process.returncode == 0
            data_success = self.processed_lines > 0

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            success = process_success or data_success

            result = {
                'success': success,
                'process_success': process_success,
                'data_success': data_success,
                'return_code': self.scrapy_process.returncode,
                'processed_lines': self.processed_lines,
                'stdout': stdout.decode('utf-8', errors='ignore'),
                'stderr': stderr.decode('utf-8', errors='ignore')
            }

            if success:
                if process_success and data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ + ãƒ‡ãƒ¼ã‚¿å–å¾—: {self.processed_lines}ä»¶ï¼‰")
                elif data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {self.processed_lines}ä»¶ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚³ãƒ¼ãƒ‰: {self.scrapy_process.returncode}ï¼‰")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stderr: {result['stderr'][:500]}")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stdout: {result['stdout'][-500:]}")
                else:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸã€ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶ï¼‰")
            else:
                print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•— (code: {self.scrapy_process.returncode}, ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶)")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stderr: {result['stderr']}")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stdout: {result['stdout']}")

                # å¤±æ•—åŸå› ã®è©³ç´°åˆ†æ
                self._analyze_failure_cause(result)

            return result

        except Exception as e:
            print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _analyze_failure_cause(self, result: Dict[str, Any]):
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› ã‚’åˆ†æ"""
        try:
            print(f"ğŸ” === Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› åˆ†æé–‹å§‹ ===")
            print(f"ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰: {result['return_code']}")

            stderr = result['stderr']
            stdout = result['stdout']

            # ä¸€èˆ¬çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            failure_patterns = {
                'ImportError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼',
                'ModuleNotFoundError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'AttributeError': 'å±æ€§ã‚¨ãƒ©ãƒ¼',
                'SyntaxError': 'ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼',
                'IndentationError': 'ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼',
                'NameError': 'åå‰ã‚¨ãƒ©ãƒ¼',
                'TypeError': 'å‹ã‚¨ãƒ©ãƒ¼',
                'ValueError': 'å€¤ã‚¨ãƒ©ãƒ¼',
                'ConnectionError': 'æ¥ç¶šã‚¨ãƒ©ãƒ¼',
                'TimeoutError': 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼',
                'PermissionError': 'æ¨©é™ã‚¨ãƒ©ãƒ¼',
                'FileNotFoundError': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'twisted.internet.error': 'Twistedã‚¨ãƒ©ãƒ¼',
                'scrapy.exceptions': 'Scrapyã‚¨ãƒ©ãƒ¼',
                'playwright': 'Playwrightã‚¨ãƒ©ãƒ¼',
                'ERROR': 'ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼',
                'CRITICAL': 'é‡å¤§ãªã‚¨ãƒ©ãƒ¼',
                'Traceback': 'Pythonä¾‹å¤–',
                'Exception': 'ä¾‹å¤–ç™ºç”Ÿ'
            }

            detected_issues = []
            for pattern, description in failure_patterns.items():
                if pattern in stderr or pattern in stdout:
                    detected_issues.append(f"{description} ({pattern})")

            if detected_issues:
                print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
                for issue in detected_issues:
                    print(f"   - {issue}")
            else:
                print(f"ğŸ” æ—¢çŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„å¤±æ•—")

            # ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰åˆ¥ã®åˆ†æ
            return_code_meanings = {
                1: "ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼",
                2: "ã‚·ã‚§ãƒ«ã®èª¤ç”¨",
                126: "å®Ÿè¡Œæ¨©é™ãªã—",
                127: "ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                128: "ç„¡åŠ¹ãªçµ‚äº†å¼•æ•°",
                130: "Ctrl+Cã«ã‚ˆã‚‹ä¸­æ–­",
                137: "SIGKILL (å¼·åˆ¶çµ‚äº†)",
                139: "ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é•å"
            }

            if result['return_code'] in return_code_meanings:
                print(f"ğŸ” ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰ {result['return_code']}: {return_code_meanings[result['return_code']]}")

            print(f"ğŸ” === å¤±æ•—åŸå› åˆ†æå®Œäº† ===")

        except Exception as e:
            print(f"âŒ å¤±æ•—åŸå› åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _process_new_lines_threading(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        print(f"ğŸ§µ _process_new_lines_threadingé–‹å§‹: {threading.current_thread().name}")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                # ç›´æ¥DBæŒ¿å…¥å‡¦ç†ï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰
                successful_inserts = 0
                print(f"ğŸ” ç›´æ¥DBæŒ¿å…¥å‡¦ç†é–‹å§‹: {len(new_lines)}ä»¶ã®æ–°ã—ã„è¡Œ")

                for i, line in enumerate(new_lines):
                    print(f"ğŸ” å‡¦ç†ä¸­ {i+1}/{len(new_lines)}: {line[:50]}...")
                    try:
                        # JSONè§£æ
                        item_data = json.loads(line.strip())
                        print(f"ğŸ” JSONè§£ææˆåŠŸ: {item_data.get('title', 'N/A')[:30]}...")

                        # ç›´æ¥DBæŒ¿å…¥ï¼ˆthreadingç‰ˆï¼‰
                        print(f"ğŸ” DBæŒ¿å…¥é–‹å§‹...")
                        insert_result = self._sync_insert_item_threading(item_data)
                        if insert_result:
                            successful_inserts += 1
                            print(f"âœ… DBæŒ¿å…¥æˆåŠŸ: {successful_inserts}ä»¶ç›®")
                        else:
                            print(f"âŒ DBæŒ¿å…¥å¤±æ•—: {successful_inserts}ä»¶ç›®")

                        self.processed_lines += 1

                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:100]}...")
                    except Exception as e:
                        print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        import traceback
                        print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

                print(f"âœ… ç›´æ¥DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                # WebSocketé€šçŸ¥ï¼ˆthreadingç‰ˆãƒ»åŒæœŸçš„ï¼‰
                print(f"ğŸ” WebSocketé€šçŸ¥é–‹å§‹...")
                try:
                    if self.websocket_callback and successful_inserts > 0:
                        print(f"ğŸ” WebSocketé€šçŸ¥å®Ÿè¡Œä¸­...")
                        # åŒæœŸçš„ã«WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify_threading({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': successful_inserts,
                            'total_items': self.processed_lines
                        })
                        print(f"âœ… WebSocketé€šçŸ¥å®Œäº†")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, inserts={successful_inserts}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lines_threadingå®Œäº†")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰"""
        # asyncãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å‰Šé™¤ã—ã€ç›´æ¥åŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
        print(f"ğŸ” _process_new_linesé–‹å§‹ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                # ç›´æ¥DBæŒ¿å…¥å‡¦ç†ï¼ˆCeleryã‚¿ã‚¹ã‚¯ã‚’ä½¿ã‚ãªã„ï¼‰
                successful_inserts = 0
                print(f"ğŸ” ç›´æ¥DBæŒ¿å…¥å‡¦ç†é–‹å§‹: {len(new_lines)}ä»¶ã®æ–°ã—ã„è¡Œ")

                for i, line in enumerate(new_lines):
                    print(f"ğŸ” å‡¦ç†ä¸­ {i+1}/{len(new_lines)}: {line[:50]}...")
                    try:
                        # JSONè§£æ
                        item_data = json.loads(line.strip())
                        print(f"ğŸ” JSONè§£ææˆåŠŸ: {item_data.get('title', 'N/A')[:30]}...")

                        # ç›´æ¥DBæŒ¿å…¥
                        print(f"ğŸ” DBæŒ¿å…¥é–‹å§‹...")
                        insert_result = self._sync_insert_item(item_data)
                        if insert_result:
                            successful_inserts += 1
                            print(f"âœ… DBæŒ¿å…¥æˆåŠŸ: {successful_inserts}ä»¶ç›®")
                        else:
                            print(f"âŒ DBæŒ¿å…¥å¤±æ•—: {successful_inserts}ä»¶ç›®")

                        self.processed_lines += 1

                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:100]}...")
                    except Exception as e:
                        print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                        import traceback
                        print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

                print(f"âœ… ç›´æ¥DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                # WebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ã«ï¼‰
                print(f"ğŸ” WebSocketé€šçŸ¥é–‹å§‹...")
                try:
                    if self.websocket_callback and successful_inserts > 0:
                        print(f"ğŸ” WebSocketé€šçŸ¥å®Ÿè¡Œä¸­...")
                        # åŒæœŸçš„ã«WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': successful_inserts,
                            'total_items': self.processed_lines
                        })
                        print(f"âœ… WebSocketé€šçŸ¥å®Œäº†")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, inserts={successful_inserts}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lineså®Œäº†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_remaining_lines(self):
        """æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†ï¼ˆæœ€çµ‚å‡¦ç†ï¼‰"""
        try:
            if not self.jsonl_file_path.exists():
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿ç›´ã—ã¦æœªå‡¦ç†ã®è¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len([line for line in all_lines if line.strip()])
            remaining_lines = total_lines - self.processed_lines

            if remaining_lines > 0:
                print(f"ğŸ“ æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†: {remaining_lines}ä»¶")

                # æœªå‡¦ç†ã®è¡Œã‚’åŒæœŸçš„ã«å‡¦ç†
                successful_inserts = 0
                for i in range(self.processed_lines, total_lines):
                    if i < len(all_lines):
                        line = all_lines[i].strip()
                        if line:
                            try:
                                # JSONè§£æ
                                item_data = json.loads(line)

                                # åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                                self._sync_insert_item(item_data)
                                successful_inserts += 1
                                self.processed_lines += 1

                            except json.JSONDecodeError as e:
                                print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:100]}...")
                            except Exception as e:
                                print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

                print(f"âœ… æ®‹ã‚Šè¡ŒDBæŒ¿å…¥å®Œäº†: {successful_inserts}/{remaining_lines}ä»¶")

            print(f"âœ… æœ€çµ‚å‡¦ç†å®Œäº†: ç·å‡¦ç†è¡Œæ•° {self.processed_lines}")

        except Exception as e:
            print(f"âŒ æ®‹ã‚Šè¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _sync_insert_item_threading(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        max_retries = 3
        retry_count = 0

        print(f"ğŸ§µ DBæŒ¿å…¥é–‹å§‹: {threading.current_thread().name}")

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1}) - Thread: {threading.current_thread().name}")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆthreadingç‰ˆï¼‰
                    self._update_task_statistics_threading()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _sync_insert_item(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1})")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆåˆ¥ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§ï¼‰
                    self._update_task_statistics_safe()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _update_task_statistics(self, db):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°"""
        try:
            from ..database import Task, Result

            # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = db.query(Task).filter(Task.id == self.task_id).first()
            if task:
                # çµæœæ•°ã‚’å–å¾—
                result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                task.items_count = result_count
                task.updated_at = datetime.now()

                db.commit()
                print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id} - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_threading(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            from ..database import SessionLocal, Task, Result

            print(f"ğŸ§µ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°é–‹å§‹: {threading.current_thread().name}")
            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                    task.items_count = result_count
                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count} - Thread: {threading.current_thread().name}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_safe(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆåˆ¥ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ï¼‰"""
        try:
            from ..database import SessionLocal, Task, Result

            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                    task.items_count = result_count
                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify_threading(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            if not self.websocket_callback:
                return

            print(f"ğŸ§µ WebSocketé€šçŸ¥é–‹å§‹: {threading.current_thread().name}")

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')} - Thread: {threading.current_thread().name}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ï¼‰"""
        try:
            if not self.websocket_callback:
                return

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def test_scrapy_watchdog_monitor():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ¯ scrapy crawl + watchdogç›£è¦–ãƒ†ã‚¹ãƒˆ")

    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ WebSocketé€šçŸ¥: {data}")

    # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    monitor = ScrapyWatchdogMonitor(
        task_id="test_task_123",
        project_path="scrapy_projects/test_project",  # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«å¤‰æ›´
        spider_name="test_spider",  # å®Ÿéš›ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã«å¤‰æ›´
        websocket_callback=websocket_callback
    )

    # å®Ÿè¡Œ
    result = await monitor.execute_spider_with_monitoring(
        settings={
            'LOG_LEVEL': 'INFO',
            'ROBOTSTXT_OBEY': False
        }
    )

    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")


if __name__ == "__main__":
    if WATCHDOG_AVAILABLE:
        asyncio.run(test_scrapy_watchdog_monitor())
    else:
        print("âŒ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install watchdog")
