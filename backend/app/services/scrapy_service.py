import os
import subprocess
import shutil
import json
import asyncio
import uuid
import threading
import time
import signal
from pathlib import Path
from typing import Dict, List, Any, Optional
import tempfile
import sys
from datetime import datetime, timedelta
import glob

from .scrapy_task_manager import ScrapyTaskManager

# ãƒ­ã‚®ãƒ³ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..utils.logging_config import get_logger, log_with_context, log_exception
from ..utils.error_handler import (
    ScrapyUIException,
    ProjectException,
    SpiderException,
    TaskException,
    ErrorCode
)

# Python 3.13ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
from ..performance.python313_optimizations import (
    FreeThreadedExecutor,
    AsyncOptimizer,
    MemoryOptimizer,
    performance_monitor,
    jit_optimizer
)

# Rich progress imports
try:
    from ..utils.rich_progress import ScrapyProgressTracker, RichSpiderMonitor
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False
    print("âš ï¸ Rich progress not available - falling back to standard progress")

class ScrapyPlaywrightService:
    """Scrapy + Playwrightçµ±åˆã‚’ç®¡ç†ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¹ï¼ˆã‚·ãƒ³ã‚°ãƒ«ãƒˆãƒ³ï¼‰"""

    _instance = None
    _initialized = False

    def __new__(cls, base_projects_dir: str = None):
        if cls._instance is None:
            cls._instance = super(ScrapyPlaywrightService, cls).__new__(cls)
        return cls._instance

    def __init__(self, base_projects_dir: str = None):
        if self._initialized:
            return

        # ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
        self.logger = get_logger(__name__)

        # Python 3.13ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.memory_optimizer = MemoryOptimizer()
        self.async_optimizer = None  # å¿…è¦æ™‚ã«åˆæœŸåŒ–

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’è¨­å®š
        if base_projects_dir is None:
            # ç¾åœ¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½ç½®ã‹ã‚‰ç›¸å¯¾çš„ã«scrapy_projectsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç‰¹å®š
            current_file = Path(__file__)
            # backend/app/services/scrapy_service.py ã‹ã‚‰ ../../scrapy_projects
            base_projects_dir = current_file.parent.parent.parent.parent / "scrapy_projects"

        self.base_projects_dir = Path(base_projects_dir)
        self.base_projects_dir.mkdir(exist_ok=True)
        self.running_processes: Dict[str, subprocess.Popen] = {}
        self.task_progress: Dict[str, Dict[str, Any]] = {}  # ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³ã‚’è¿½è·¡
        self.monitoring_thread = None
        self.stop_monitoring = False

        # Rich progress tracker
        self.rich_tracker = None
        if RICH_AVAILABLE:
            self.rich_tracker = ScrapyProgressTracker()
            print("âœ¨ Rich progress tracking enabled")

        self._initialized = True
        print(f"ğŸ”§ ScrapyPlaywrightService initialized with base_dir: {self.base_projects_dir.absolute()}")

    def create_project(self, project_name: str, project_path: str) -> bool:
        """æ–°ã—ã„Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆscrapy startproject ã¨åŒã˜å‹•ä½œï¼‰"""
        try:
            log_with_context(
                self.logger, "INFO",
                f"Creating Scrapy project: {project_name}",
                extra_data={"project_name": project_name, "project_path": project_path}
            )

            # scrapy_projects ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã«ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            # scrapy startproject project_name ã®å‹•ä½œã‚’å†ç¾

            # scrapy_projects ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            self.base_projects_dir.mkdir(exist_ok=True)

            # scrapy startproject ã‚’ scrapy_projects ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã§å®Ÿè¡Œ
            cmd = [
                sys.executable, "-m", "scrapy", "startproject", project_name
            ]

            self.logger.info(f"Executing command: {' '.join(cmd)} in {self.base_projects_dir}")

            result = subprocess.run(
                cmd,
                cwd=str(self.base_projects_dir),  # scrapy_projects ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§å®Ÿè¡Œ
                capture_output=True,
                text=True,
                check=True
            )

            self.logger.info(f"Scrapy project created successfully: {result.stdout}")

            # ä½œæˆã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
            project_dir = self.base_projects_dir / project_name

            # scrapy-playwrightè¨­å®šã‚’è¿½åŠ 
            self._setup_playwright_config(project_dir / project_name)

            # scrapy.cfgãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ãƒ»ä¿®æ­£ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼‰
            self._validate_and_fix_scrapy_cfg(project_name, project_path)

            log_with_context(
                self.logger, "INFO",
                f"Project creation completed: {project_name}",
                extra_data={"project_name": project_name, "project_dir": str(project_dir)}
            )

            return True

        except subprocess.CalledProcessError as e:
            error_msg = f"Failed to create Scrapy project: {e.stderr}"
            log_exception(
                self.logger, error_msg,
                extra_data={"project_name": project_name, "command": cmd, "stderr": e.stderr}
            )
            raise ProjectException(
                message=error_msg,
                error_code=ErrorCode.PROJECT_CREATION_FAILED,
                details={"stderr": e.stderr, "command": cmd}
            )
        except Exception as e:
            error_msg = f"Error creating project: {str(e)}"
            log_exception(
                self.logger, error_msg,
                extra_data={"project_name": project_name}
            )
            raise ProjectException(
                message=error_msg,
                error_code=ErrorCode.PROJECT_CREATION_FAILED,
                details={"original_error": str(e)}
            )

    def _setup_playwright_config(self, project_dir: Path) -> None:
        """scrapy-playwrightè¨­å®šã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«è¿½åŠ """
        settings_file = project_dir / "settings.py"

        playwright_settings = '''

# Scrapy-Playwright settings
DOWNLOAD_HANDLERS = {
    "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
}

TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"

PLAYWRIGHT_BROWSER_TYPE = "chromium"
PLAYWRIGHT_LAUNCH_OPTIONS = {
    "headless": True,
}

# Default request meta for Playwright
PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT = 30000
PLAYWRIGHT_ABORT_REQUEST = lambda req: req.resource_type == "image"

# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {
    'Accept-Language': 'ja',
}

# Feed export encoding
FEED_EXPORT_ENCODING = 'utf-8'

# HTTP Cache settings (for development efficiency)
HTTPCACHE_ENABLED = True
HTTPCACHE_DIR = 'httpcache'
HTTPCACHE_EXPIRATION_SECS = 86400  # 1 day

# Proxy settings (optional - configure as needed)
# DOWNLOADER_MIDDLEWARES = {
#     'scrapy_proxies.RandomProxy': 350,
# }

# Proxy settings (optional - configure as needed)
# PROXY_LIST = '/path/to/proxy/list.txt'
# PROXY_MODE = 0  # 0: random, 1: round-robin, 2: only once
'''

        if settings_file.exists():
            with open(settings_file, 'a', encoding='utf-8') as f:
                f.write(playwright_settings)

    def _validate_and_fix_scrapy_cfg(self, project_name: str, project_path: str = None) -> None:
        """scrapy.cfgãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œè¨¼ã—ã€å¿…è¦ã«å¿œã˜ã¦ä¿®æ­£ï¼ˆWebUIå¯¾å¿œç‰ˆï¼‰"""
        try:
            # project_pathãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯project_nameã‚’ä½¿ç”¨
            if project_path is None:
                project_path = project_name

            project_dir = self.base_projects_dir / project_name
            scrapy_cfg_path = project_dir / "scrapy.cfg"

            if not scrapy_cfg_path.exists():
                self.logger.warning(f"scrapy.cfg not found: {scrapy_cfg_path}")
                return

            # ç¾åœ¨ã®å†…å®¹ã‚’èª­ã¿è¾¼ã¿
            with open(scrapy_cfg_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # è¨­å®šã‚’ç¢ºèª
            import re
            settings_match = re.search(r'default\s*=\s*(.+?)\.settings', content)
            project_match = re.search(r'project\s*=\s*(.+)', content)

            current_settings_project = settings_match.group(1).strip() if settings_match else None
            current_deploy_project = project_match.group(1).strip() if project_match else None

            # ä¿®æ­£ãŒå¿…è¦ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆproject_pathã‚’ä½¿ç”¨ï¼‰
            needs_fix = (
                current_settings_project != project_path or
                current_deploy_project != project_path
            )

            if needs_fix:
                self.logger.info(f"Fixing scrapy.cfg for project: {project_name} (path: {project_path})")

                # æ­£ã—ã„å†…å®¹ã§ä¿®æ­£ï¼ˆproject_pathã‚’ä½¿ç”¨ï¼‰
                correct_content = f"""# Automatically created by: scrapy startproject
#
# For more information about the [deploy] section see:
# https://scrapyd.readthedocs.io/en/latest/deploy.html

[settings]
default = {project_path}.settings

[deploy]
#url = http://localhost:6800/
project = {project_path}
"""

                with open(scrapy_cfg_path, 'w', encoding='utf-8') as f:
                    f.write(correct_content)

                self.logger.info(f"Fixed scrapy.cfg: {scrapy_cfg_path} (using project_path: {project_path})")
            else:
                self.logger.info(f"scrapy.cfg is correct for project: {project_name} (path: {project_path})")

        except Exception as e:
            self.logger.error(f"Error validating scrapy.cfg for {project_name}: {e}")

    def get_spider_code(self, project_path: str, spider_name: str) -> str:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
        try:
            # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            full_path = self.base_projects_dir / project_path

            possible_spider_paths = [
                # æ¨™æº–Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ : scrapy_projects/project_name/project_name/spiders/spider_name.py
                full_path / project_path / "spiders" / f"{spider_name}.py",
                # ç°¡ç•¥åŒ–æ§‹é€ : scrapy_projects/project_name/spiders/spider_name.py
                full_path / "spiders" / f"{spider_name}.py",
                # ç›´æ¥é…ç½®: scrapy_projects/project_name/spider_name.py
                full_path / f"{spider_name}.py"
            ]

            spider_file = None
            for path in possible_spider_paths:
                if path.exists():
                    spider_file = path
                    print(f"âœ… Spider file found: {spider_file}")
                    break

            if not spider_file:
                # å†å¸°æ¤œç´¢ã§æœ€å¾Œã®æ‰‹æ®µ
                import glob
                pattern = str(full_path / "**" / f"{spider_name}.py")
                matches = glob.glob(pattern, recursive=True)
                if matches:
                    spider_file = Path(matches[0])
                    print(f"âœ… Spider file found via recursive search: {spider_file}")
                else:
                    print(f"âŒ Spider file not found in any location:")
                    for path in possible_spider_paths:
                        print(f"   - {path}")
                    raise Exception(f"Spider file not found: {spider_name}.py in {full_path}")

            with open(spider_file, 'r', encoding='utf-8') as f:
                return f.read()

        except Exception as e:
            print(f"Error reading spider code: {str(e)}")
            raise Exception(f"Error reading spider code: {str(e)}")

    def _get_spider_custom_settings(self, project_path: str, spider_name: str) -> dict:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®custom_settingsã‚’å–å¾—ï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            spider_code = self.get_spider_code(project_path, spider_name)

            # ASTã‚’ä½¿ç”¨ã—ã¦ã‚ˆã‚Šå®‰å…¨ã«custom_settingsã‚’æŠ½å‡º
            import ast

            try:
                # Pythonã‚³ãƒ¼ãƒ‰ã‚’è§£æ
                tree = ast.parse(spider_code)

                # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¢ã™
                for node in ast.walk(tree):
                    if isinstance(node, ast.ClassDef):
                        # ã‚¯ãƒ©ã‚¹å†…ã®custom_settingså±æ€§ã‚’æ¢ã™
                        for item in node.body:
                            if (isinstance(item, ast.Assign) and
                                len(item.targets) == 1 and
                                isinstance(item.targets[0], ast.Name) and
                                item.targets[0].id == 'custom_settings'):

                                # è¾æ›¸ãƒªãƒ†ãƒ©ãƒ«ã‚’è©•ä¾¡
                                if isinstance(item.value, ast.Dict):
                                    custom_settings = ast.literal_eval(item.value)
                                    print(f"âœ… Extracted custom_settings from {spider_name} using AST: {custom_settings}")
                                    return custom_settings

                print(f"â„¹ï¸ No custom_settings found in {spider_name} using AST")

                # ASTè§£æã«å¤±æ•—ã—ãŸå ´åˆã¯ã€ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ã‚’ä½¿ç”¨
                return self._fallback_extract_custom_settings(spider_code, spider_name)

            except Exception as ast_error:
                print(f"âš ï¸ AST parsing failed for {spider_name}: {ast_error}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼ã‚’ä½¿ç”¨
                return self._fallback_extract_custom_settings(spider_code, spider_name)

        except Exception as e:
            print(f"âš ï¸ Error getting custom_settings for {spider_name}: {e}")
            return {}

    def _fallback_extract_custom_settings(self, spider_code: str, spider_name: str) -> dict:
        """custom_settingsæŠ½å‡ºã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹å¼"""
        try:
            import re

            # ã‚ˆã‚Šå …ç‰¢ãªæ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆãƒã‚¹ãƒˆã—ãŸè¾æ›¸ã«å¯¾å¿œï¼‰
            # custom_settings = { ... } ã®éƒ¨åˆ†ã‚’æŠ½å‡º
            pattern = r'custom_settings\s*=\s*\{'
            match = re.search(pattern, spider_code)

            if match:
                start_pos = match.end() - 1  # '{' ã®ä½ç½®
                brace_count = 0
                end_pos = start_pos

                # å¯¾å¿œã™ã‚‹ '}' ã‚’è¦‹ã¤ã‘ã‚‹
                for i, char in enumerate(spider_code[start_pos:], start_pos):
                    if char == '{':
                        brace_count += 1
                    elif char == '}':
                        brace_count -= 1
                        if brace_count == 0:
                            end_pos = i + 1
                            break

                if brace_count == 0:
                    settings_content = spider_code[start_pos:end_pos]

                    try:
                        # å®‰å…¨ã«evalã‚’ä½¿ç”¨
                        safe_dict = {"__builtins__": {}, "True": True, "False": False, "None": None}
                        custom_settings = eval(settings_content, safe_dict)
                        print(f"âœ… Extracted custom_settings from {spider_name} using fallback: {custom_settings}")
                        return custom_settings
                    except Exception as e:
                        print(f"âš ï¸ Error evaluating custom_settings for {spider_name}: {e}")
                        # åŸºæœ¬çš„ãªè¨­å®šã®ã¿è¿”ã™
                        return {
                            'DOWNLOAD_DELAY': 3,
                            'CONCURRENT_REQUESTS': 1,
                            'CONCURRENT_REQUESTS_PER_DOMAIN': 1
                        }
                else:
                    print(f"âš ï¸ Unmatched braces in custom_settings for {spider_name}")
            else:
                print(f"â„¹ï¸ No custom_settings pattern found in {spider_name}")

            return {}

        except Exception as e:
            print(f"âš ï¸ Fallback extraction failed for {spider_name}: {e}")
            return {}

    def save_spider_code(self, project_path: str, spider_name: str, code: str) -> bool:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜"""
        try:
            # æ—¢å­˜ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            full_path = self.base_projects_dir / project_path

            possible_spider_paths = [
                # æ¨™æº–Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ : scrapy_projects/project_name/project_name/spiders/spider_name.py
                full_path / project_path / "spiders" / f"{spider_name}.py",
                # ç°¡ç•¥åŒ–æ§‹é€ : scrapy_projects/project_name/spiders/spider_name.py
                full_path / "spiders" / f"{spider_name}.py",
                # ç›´æ¥é…ç½®: scrapy_projects/project_name/spider_name.py
                full_path / f"{spider_name}.py"
            ]

            spider_file = None
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            for path in possible_spider_paths:
                if path.exists():
                    spider_file = path
                    print(f"âœ… Updating existing spider file: {spider_file}")
                    break

            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã¯æ¨™æº–æ§‹é€ ã§ä½œæˆ
            if not spider_file:
                spider_file = possible_spider_paths[0]  # æ¨™æº–æ§‹é€ ã‚’ä½¿ç”¨
                print(f"âœ… Creating new spider file: {spider_file}")

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            spider_file.parent.mkdir(parents=True, exist_ok=True)

            with open(spider_file, 'w', encoding='utf-8') as f:
                f.write(code)

            print(f"Spider code saved: {spider_file}")
            return True

        except Exception as e:
            print(f"Error saving spider code: {str(e)}")
            raise Exception(f"Error saving spider code: {str(e)}")

    def save_project_file(self, project_path: str, file_path: str, content: str) -> bool:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜æ–¹æ³•ï¼‰"""
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            full_path = self.base_projects_dir / project_path

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ­£è¦åŒ–ï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ï¼‰
            file_path = file_path.replace("../", "").replace("..\\", "")

            # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
            full_file_path = full_path / file_path

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            full_file_path.parent.mkdir(parents=True, exist_ok=True)

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã¨åŒã˜æ–¹æ³•ï¼‰
            with open(full_file_path, 'w', encoding='utf-8') as f:
                f.write(content)

            print(f"Project file saved: {full_file_path}")
            return True

        except Exception as e:
            print(f"Error saving project file: {str(e)}")
            raise Exception(f"Error saving project file: {str(e)}")

    async def run_spider_with_manager(self, project_path: str, spider_name: str, task_id: str,
                                     settings: Optional[Dict[str, Any]] = None,
                                     progress_callback: Optional[callable] = None,
                                     websocket_callback: Optional[callable] = None) -> Dict[str, Any]:
        """ScrapyTaskManagerã‚’ä½¿ç”¨ã—ã¦ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œï¼ˆçµ±ä¸€ç®¡ç†ç‰ˆï¼‰"""
        try:
            log_with_context(
                self.logger, "INFO",
                f"Starting spider execution with TaskManager: {spider_name}",
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name,
                extra_data={"settings": settings}
            )

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            full_project_path = self.base_projects_dir / project_path

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼è¨­å®šã‚’æ§‹ç¯‰
            spider_config = {
                'project_path': str(full_project_path),
                'spider_name': spider_name,
                'settings': settings or {}
            }

            # ScrapyTaskManagerã‚’ä½œæˆ
            task_manager = ScrapyTaskManager(
                task_id=task_id,
                spider_config=spider_config,
                progress_callback=progress_callback,
                websocket_callback=websocket_callback
            )

            # ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
            result = await task_manager.execute()

            log_with_context(
                self.logger, "INFO",
                f"Spider execution completed: {spider_name}",
                task_id=task_id,
                extra_data={"result": result}
            )

            return result

        except Exception as e:
            log_exception(
                self.logger,
                f"Error in spider execution with TaskManager: {str(e)}",
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name
            )
            return {
                'success': False,
                'task_id': task_id,
                'error': str(e)
            }

    def run_spider(self, project_path: str, spider_name: str, task_id: str, settings: Optional[Dict[str, Any]] = None) -> str:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œï¼ˆéåŒæœŸï¼‰"""
        try:
            log_with_context(
                self.logger, "INFO",
                f"Starting spider execution: {spider_name}",
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name,
                extra_data={"settings": settings}
            )

            # scrapy_projects/project_name ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§scrapy crawlã‚’å®Ÿè¡Œ
            full_path = self.base_projects_dir / project_path

            if not full_path.exists():
                raise SpiderException(
                    message=f"Project directory not found: {full_path}",
                    error_code=ErrorCode.PROJECT_NOT_FOUND,
                    project_id=project_path
                )

            cmd = [sys.executable, "-m", "scrapy", "crawl", spider_name]

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å›ºæœ‰è¨­å®šã‚’ç¢ºèª
            spider_custom_settings = self._get_spider_custom_settings(project_path, spider_name)

            # æœ€å°é™ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆCLIã¨åŒã˜å‹•ä½œã‚’ç›®æŒ‡ã™ï¼‰
            default_settings = {
                'LOG_LEVEL': 'INFO',  # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®ã¿è¨­å®š
                'ROBOTSTXT_OBEY': False  # robots.txtã‚’ç„¡è¦–
            }

            # æœ€å°é™ã®è¨­å®šã®ã¿é©ç”¨ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®custom_settingsã‚’å„ªå…ˆï¼‰
            final_settings = default_settings.copy()

            # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã§ä¸Šæ›¸ãï¼ˆå¿…è¦æœ€å°é™ï¼‰
            if settings:
                # é‡è¦ãªè¨­å®šã®ã¿é©ç”¨
                important_settings = ['LOG_LEVEL', 'ROBOTSTXT_OBEY']
                for key in important_settings:
                    if key in settings:
                        final_settings[key] = settings[key]

            print(f"ğŸ¯ Using minimal settings for {spider_name}: {final_settings}")
            print(f"ğŸ“‹ Spider has custom_settings: {bool(spider_custom_settings)}")

            # æœ€å°é™ã®è¨­å®šã®ã¿ã‚³ãƒãƒ³ãƒ‰ã«è¿½åŠ 
            for key, value in final_settings.items():
                cmd.extend(["-s", f"{key}={value}"])

            # çµæœã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
            output_file = full_path / f"results_{task_id}.json"
            cmd.extend(["-o", str(output_file)])

            self.logger.info(f"Executing spider command: {' '.join(cmd)} in {full_path}")

            # æ‰‹å‹•å®Ÿè¡Œã¨åŒã˜ç’°å¢ƒã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            env = os.environ.copy()
            env['PYTHONPATH'] = str(full_path)
            project_name = full_path.name  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å–å¾—
            env['SCRAPY_SETTINGS_MODULE'] = f'{project_name}.settings'

            try:
                # æ‰‹å‹•å®Ÿè¡Œã¨åŒã˜è¨­å®šã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
                process = subprocess.Popen(
                    cmd,
                    cwd=str(full_path),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.STDOUT,  # stderrã‚’stdoutã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆ
                    text=True,
                    env=env,  # ç’°å¢ƒå¤‰æ•°ã‚’æ˜ç¤ºçš„ã«è¨­å®š
                    bufsize=1,  # è¡Œãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°
                    universal_newlines=True
                )
                self.logger.info(f"âœ… Spider process started successfully: PID {process.pid}")
            except Exception as e:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæœ€å°é™ã®è¨­å®šã§å†è©¦è¡Œ
                self.logger.warning(f"Failed to start process with advanced settings, using fallback: {e}")
                process = subprocess.Popen(
                    cmd,
                    cwd=str(full_path),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True
                )

            self.running_processes[task_id] = process

            # é€²è¡ŒçŠ¶æ³ã®åˆæœŸåŒ–ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°å¯¾å¿œï¼‰
            self.task_progress[task_id] = {
                'started_at': datetime.now(),
                'items_scraped': 0,
                'requests_made': 0,
                'errors_count': 0,
                'progress_percentage': 5,  # é–‹å§‹æ™‚ã¯5%
                'estimated_total': 60,  # åˆæœŸæ¨å®šå€¤
                'current_url': None,
                'last_update': datetime.now(),
                'last_notification': datetime.now(),
                'process': process,  # ãƒ—ãƒ­ã‚»ã‚¹å‚ç…§ã‚’ä¿å­˜
                'output_file': str(output_file)  # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ä¿å­˜
            }

            # Rich progress tracking ã‚’é–‹å§‹
            if self.rich_tracker:
                self.rich_tracker.add_spider_task(task_id, spider_name, total_pages=100)
                print(f"âœ¨ Rich progress tracking started for {spider_name}")

            # åˆæœŸãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ã‚’é€ä¿¡
            self._send_initial_progress_notification(task_id)

            # ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚’é–‹å§‹ï¼ˆprogress_callbackã‚’å‘¼ã³å‡ºã™ãŸã‚ï¼‰
            self._start_process_monitoring(task_id, process, str(output_file))

            log_with_context(
                self.logger, "INFO",
                f"Spider process started successfully: {spider_name}",
                task_id=task_id,
                extra_data={"pid": process.pid, "output_file": str(output_file)}
            )

            return task_id

        except SpiderException:
            # æ—¢ã«SpiderExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
            raise
        except Exception as e:
            error_msg = f"Error running spider: {str(e)}"
            log_exception(
                self.logger, error_msg,
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name,
                extra_data={"settings": settings}
            )
            raise TaskException(
                message=error_msg,
                error_code=ErrorCode.SPIDER_EXECUTION_FAILED,
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name,
                details={"original_error": str(e)}
            )

    def _send_initial_progress_notification(self, task_id: str):
        """åˆæœŸãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ã‚’é€ä¿¡"""
        try:
            import requests

            notification_url = "http://localhost:8000/api/tasks/internal/websocket-notify"
            payload = {
                "task_id": task_id,
                "data": {
                    "id": task_id,
                    "status": "RUNNING",
                    "items_count": 0,
                    "requests_count": 0,
                    "progress": 5  # é–‹å§‹æ™‚ã¯5%
                }
            }

            response = requests.post(
                notification_url,
                json=payload,
                timeout=0.5,
                headers={"Content-Type": "application/json"}
            )

            if response.status_code == 200:
                print(f"ğŸ“Š Initial progress notification sent: Task {task_id} - 5%")

        except Exception as e:
            print(f"ğŸ“¡ Initial progress notification error: {str(e)}")

    def _start_process_monitoring(self, task_id: str, process: subprocess.Popen, output_file: str):
        """ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã‚’é–‹å§‹ã—ã¦progress_callbackã‚’å‘¼ã³å‡ºã™"""
        def monitor_process():
            try:
                print(f"ğŸ” Starting process monitoring for task {task_id}")

                # ç›£è¦–é–“éš”ï¼ˆç§’ï¼‰
                monitor_interval = 2
                last_items_count = 0
                last_requests_count = 0
                error_count = 0

                while process.poll() is None:  # ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Ÿè¡Œä¸­ã®é–“
                    try:
                        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                        current_items, current_requests = self._get_real_time_statistics(task_id, output_file)

                        # å¤‰åŒ–ãŒã‚ã£ãŸå ´åˆã®ã¿progress_callbackã‚’å‘¼ã³å‡ºã—
                        if (current_items != last_items_count or
                            current_requests != last_requests_count):

                            print(f"ğŸ“Š Progress detected: Task {task_id} - Items: {current_items}, Requests: {current_requests}")

                            # progress_callbackã‚’å‘¼ã³å‡ºã—ï¼ˆDBã«ä¿å­˜ï¼‰
                            self._call_progress_callback(task_id, current_items, current_requests, error_count)

                            last_items_count = current_items
                            last_requests_count = current_requests

                        # ç›£è¦–é–“éš”ã§å¾…æ©Ÿ
                        time.sleep(monitor_interval)

                    except Exception as monitor_error:
                        print(f"âš ï¸ Monitor error for task {task_id}: {monitor_error}")
                        error_count += 1
                        time.sleep(monitor_interval)

                # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†å¾Œã®æœ€çµ‚çµ±è¨ˆå–å¾—
                print(f"ğŸ Process completed for task {task_id}, getting final statistics")
                final_items, final_requests = self._get_real_time_statistics(task_id, output_file)

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥çµ±è¨ˆã‚’å–å¾—ï¼ˆã‚ˆã‚Šæ­£ç¢ºï¼‰
                try:
                    import json
                    from pathlib import Path
                    result_path = Path(output_file)
                    if result_path.exists() and result_path.stat().st_size > 100:
                        with open(result_path, 'r', encoding='utf-8') as f:
                            file_data = json.load(f)
                        if isinstance(file_data, list):
                            actual_items = len(file_data)
                            if actual_items > final_items:
                                print(f"ğŸ“Š File-based count ({actual_items}) > real-time count ({final_items}), using file count")
                                final_items = actual_items
                                final_requests = max(final_requests, actual_items + 10)
                except Exception as e:
                    print(f"âš ï¸ Error reading final statistics from file: {e}")

                # æœ€çµ‚progress_callbackã‚’å‘¼ã³å‡ºã—
                self._call_progress_callback(task_id, final_items, final_requests, error_count)

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’DBã«æ ¼ç´
                self._store_results_to_db(task_id, output_file)

                # ã‚¿ã‚¹ã‚¯å®Œäº†å‡¦ç†
                success = process.returncode == 0
                self._update_task_completion(task_id, success)

                # Rich progress tracking ã‚’å®Œäº†çŠ¶æ…‹ã«
                if self.rich_tracker:
                    status = "COMPLETED" if success else "FAILED"
                    self.rich_tracker.complete_task(task_id, status)
                    print(f"âœ¨ Rich progress tracking completed for task {task_id}: {status}")

                print(f"âœ… Process monitoring completed for task {task_id}: success={success}, items={final_items}, requests={final_requests}")

            except Exception as e:
                print(f"âŒ Process monitoring failed for task {task_id}: {str(e)}")

        # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§ç›£è¦–ã‚’é–‹å§‹
        monitor_thread = threading.Thread(target=monitor_process, daemon=True)
        monitor_thread.start()
        print(f"ğŸš€ Process monitoring thread started for task {task_id}")

    def _get_real_time_statistics(self, task_id: str, output_file: str) -> tuple:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            import json
            from pathlib import Path

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
            result_path = Path(output_file)
            if result_path.exists() and result_path.stat().st_size > 0:
                try:
                    with open(result_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆï¼ˆ1è¡Œ1ã‚¢ã‚¤ãƒ†ãƒ ï¼‰
                            if content.count('\n') > 0:
                                items = content.strip().split('\n')
                                items_count = len([line for line in items if line.strip()])
                            else:
                                # å˜ä¸€JSONã®å ´åˆ
                                data = json.loads(content)
                                items_count = len(data) if isinstance(data, list) else 1

                            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¯æ¨å®šï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•° + Î±ï¼‰
                            requests_count = max(items_count + 2, 1)

                            return items_count, requests_count
                except (json.JSONDecodeError, Exception) as e:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸å®Œå…¨ãªå ´åˆã¯0ã‚’è¿”ã™
                    pass

            return 0, 0

        except Exception as e:
            print(f"âš ï¸ Error getting real-time statistics for {task_id}: {e}")
            return 0, 0

    def _call_progress_callback(self, task_id: str, items_count: int, requests_count: int, error_count: int):
        """progress_callbackã‚’å‘¼ã³å‡ºã—ã¦DBã‚’æ›´æ–°"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus

            db = SessionLocal()
            try:
                task = db.query(DBTask).filter(DBTask.id == task_id).first()
                if task:
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ›´æ–°ï¼ˆã‚ˆã‚Šè©³ç´°ãªçŠ¶æ…‹ç®¡ç†ï¼‰
                    task.items_count = items_count
                    task.requests_count = requests_count
                    task.error_count = error_count

                    # å®Ÿè¡ŒçŠ¶æ…‹ã®ç¢ºå®Ÿãªè¨˜éŒ²
                    if items_count > 0 or requests_count > 0:
                        task.status = TaskStatus.RUNNING
                        if not task.started_at:
                            task.started_at = datetime.now()

                    # å³åº§ã«ã‚³ãƒŸãƒƒãƒˆï¼ˆWebUIã¨ã®åŒæœŸã‚’ç¢ºå®Ÿã«ï¼‰
                    db.commit()

                    print(f"ğŸ“Š Progress callback executed: Task {task_id} - Items: {items_count}, Requests: {requests_count}, Errors: {error_count}")

                    # Rich progress tracking ã‚’æ›´æ–°
                    if self.rich_tracker:
                        self.rich_tracker.update_progress(
                            task_id,
                            items_scraped=items_count,
                            requests_made=requests_count,
                            errors=error_count,
                            pages_visited=min(requests_count // 10, 100)  # æ¨å®šãƒšãƒ¼ã‚¸æ•°
                        )

                    # WebSocketé€šçŸ¥ã‚’é€ä¿¡
                    self._send_progress_notification(task_id, items_count, requests_count, error_count)

            finally:
                db.close()

        except Exception as e:
            print(f"âŒ Progress callback error for task {task_id}: {str(e)}")

    def _send_progress_notification(self, task_id: str, items_count: int, requests_count: int, error_count: int):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ã‚’é€ä¿¡"""
        try:
            import requests

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¨ˆç®—
            elapsed_seconds = 0
            if task_id in self.task_progress:
                elapsed_seconds = (datetime.now() - self.task_progress[task_id]['started_at']).total_seconds()

            if items_count > 0:
                # ã‚¢ã‚¤ãƒ†ãƒ ãƒ™ãƒ¼ã‚¹ã®é€²è¡Œè¨ˆç®—
                pending_items = max(0, min(60 - items_count, max(requests_count - items_count, 10)))
                total_estimated = items_count + pending_items
                item_progress = (items_count / total_estimated) * 100 if total_estimated > 0 else 10

                # æ™‚é–“ãƒ™ãƒ¼ã‚¹ã®é€²è¡Œæ¨å®š
                time_progress = min(80, elapsed_seconds * 1.5)

                # è¤‡åˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹
                progress_percentage = min(95, max(item_progress, time_progress))
            else:
                # åˆæœŸæ®µéšã®é€²è¡Œ
                progress_percentage = min(15, elapsed_seconds * 2) if elapsed_seconds > 0 else 5

            notification_url = "http://localhost:8000/api/tasks/internal/websocket-notify"
            payload = {
                "task_id": task_id,
                "data": {
                    "id": task_id,
                    "status": "RUNNING",
                    "items_count": items_count,
                    "requests_count": requests_count,
                    "error_count": error_count,
                    "progress": progress_percentage,
                    "elapsed_seconds": elapsed_seconds
                }
            }

            response = requests.post(
                notification_url,
                json=payload,
                timeout=0.5,
                headers={"Content-Type": "application/json"}
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ Progress notification sent: Task {task_id} - {progress_percentage:.1f}%")

        except Exception as e:
            print(f"ğŸ“¡ Progress notification error: {str(e)}")

    def _store_results_to_db(self, task_id: str, output_file: str):
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’DBã«æ ¼ç´"""
        try:
            import json
            from pathlib import Path
            from ..database import SessionLocal, Result as DBResult

            # è¤‡æ•°ã®å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç¢ºèª
            possible_paths = []

            # 1. æŒ‡å®šã•ã‚ŒãŸãƒ‘ã‚¹
            if output_file:
                possible_paths.append(Path(output_file))

            # 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            from ..database import SessionLocal as TempDB, Task as TempTask
            temp_db = TempDB()
            try:
                task = temp_db.query(TempTask).filter(TempTask.id == task_id).first()
                if task and task.project:
                    project_path = self.base_projects_dir / task.project.path
                    # è¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§æ¤œç´¢
                    patterns = [
                        f"results_{task_id}.json",
                        f"results_{task_id}*.json",
                        f"*{task_id}*.json",
                        "results_*.json"
                    ]

                    for pattern in patterns:
                        files = list(project_path.glob(pattern))
                        if files:
                            # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
                            latest_file = max(files, key=lambda f: f.stat().st_mtime)
                            possible_paths.append(latest_file)
                            break
            finally:
                temp_db.close()

            # å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
            result_path = None
            for path in possible_paths:
                if path and path.exists() and path.stat().st_size > 0:
                    result_path = path
                    break

            if not result_path:
                print(f"ğŸ“ No valid result file found for task {task_id}")
                print(f"   Searched paths: {[str(p) for p in possible_paths if p]}")
                return

            print(f"ğŸ“ Storing results to DB for task {task_id}: {result_path}")

            db = SessionLocal()
            try:
                with open(result_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()

                if not content:
                    print(f"ğŸ“ Empty result file for task {task_id}")
                    return

                # ã¾ãšJSONé…åˆ—ã¨ã—ã¦è§£æã‚’è©¦è¡Œ
                try:
                    data = json.loads(content)

                    if isinstance(data, list):
                        # JSONé…åˆ—å½¢å¼ã®å ´åˆï¼ˆæœ€ã‚‚ä¸€èˆ¬çš„ï¼‰
                        stored_count = 0
                        for item in data:
                            import uuid
                            db_result = DBResult(
                                id=str(uuid.uuid4()),  # IDã‚’æ‰‹å‹•ã§ç”Ÿæˆ
                                task_id=task_id,
                                data=item,
                                created_at=datetime.now()
                            )
                            db.add(db_result)
                            stored_count += 1

                        db.commit()
                        print(f"âœ… Stored {stored_count} items (JSON array) to DB for task {task_id}")

                    else:
                        # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                        import uuid
                        db_result = DBResult(
                            id=str(uuid.uuid4()),  # IDã‚’æ‰‹å‹•ã§ç”Ÿæˆ
                            task_id=task_id,
                            data=data,
                            created_at=datetime.now()
                        )
                        db.add(db_result)
                        db.commit()
                        print(f"âœ… Stored 1 item (single object) to DB for task {task_id}")

                except json.JSONDecodeError:
                    # JSONé…åˆ—ã¨ã—ã¦è§£æã§ããªã„å ´åˆã€JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†
                    print(f"ğŸ“ Trying JSONL format for task {task_id}")

                    if content.count('\n') > 0:
                        items = content.strip().split('\n')
                        stored_count = 0

                        for line in items:
                            line = line.strip()
                            if line:
                                try:
                                    item_data = json.loads(line)

                                    # DBã«çµæœã‚’ä¿å­˜
                                    import uuid
                                    db_result = DBResult(
                                        id=str(uuid.uuid4()),  # IDã‚’æ‰‹å‹•ã§ç”Ÿæˆ
                                        task_id=task_id,
                                        data=item_data,
                                        created_at=datetime.now()
                                    )
                                    db.add(db_result)
                                    stored_count += 1

                                except json.JSONDecodeError as e:
                                    print(f"âš ï¸ Invalid JSON in result line: {line[:100]}... Error: {e}")
                                    continue

                        db.commit()
                        print(f"âœ… Stored {stored_count} items (JSONL format) to DB for task {task_id}")
                    else:
                        print(f"âŒ Unable to parse result file for task {task_id}: Not valid JSON or JSONL")

            finally:
                db.close()

        except Exception as e:
            print(f"âŒ Error storing results to DB for task {task_id}: {str(e)}")

    @performance_monitor
    @jit_optimizer.hot_function
    def run_spider_optimized(self, project_path: str, spider_name: str, task_id: str, settings: Optional[Dict[str, Any]] = None) -> str:
        """
        Python 3.13æœ€é©åŒ–ç‰ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
        Free-threadedä¸¦åˆ—å‡¦ç†ã¨JITæœ€é©åŒ–ã‚’æ´»ç”¨
        """
        try:
            log_with_context(
                self.logger, "INFO",
                f"Starting optimized spider execution: {spider_name}",
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name,
                extra_data={"optimization": "python313", "settings": settings}
            )

            # Free-threadedä¸¦åˆ—å®Ÿè¡Œã‚’ä½¿ç”¨
            with FreeThreadedExecutor(max_workers=4) as executor:
                # CPUé›†ç´„çš„ãªå‰å‡¦ç†ã‚’ä¸¦åˆ—å®Ÿè¡Œ
                preprocessing_future = executor.submit_cpu_intensive(
                    self._preprocess_spider_execution,
                    project_path, spider_name, task_id, settings
                )

                # ä¸¦åˆ—ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œè¨¼
                validation_future = executor.submit_cpu_intensive(
                    self._validate_project_structure,
                    project_path
                )

                # çµæœã‚’å–å¾—
                preprocessing_result = preprocessing_future.result()
                validation_result = validation_future.result()

                if not validation_result:
                    raise SpiderException(
                        message=f"Project validation failed: {project_path}",
                        error_code=ErrorCode.PROJECT_NOT_FOUND,
                        project_id=project_path
                    )

            # é€šå¸¸ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
            return self.run_spider(project_path, spider_name, task_id, settings)

        except Exception as e:
            error_msg = f"Error in optimized spider execution: {str(e)}"
            log_exception(
                self.logger, error_msg,
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name
            )
            raise TaskException(
                message=error_msg,
                error_code=ErrorCode.SPIDER_EXECUTION_FAILED,
                task_id=task_id,
                project_id=project_path,
                spider_id=spider_name
            )

    def _preprocess_spider_execution(self, project_path: str, spider_name: str, task_id: str, settings: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œã®å‰å‡¦ç†ï¼ˆCPUé›†ç´„çš„ï¼‰"""
        full_path = self.base_projects_dir / project_path

        # è¨­å®šã®æœ€é©åŒ–
        optimized_settings = settings.copy() if settings else {}

        # Python 3.13ã®æœ€é©åŒ–è¨­å®šã‚’è¿½åŠ 
        optimized_settings.update({
            'CONCURRENT_REQUESTS': 32,  # Free-threadedç’°å¢ƒã§ã¯é«˜ã„ä¸¦è¡Œæ€§
            'CONCURRENT_REQUESTS_PER_DOMAIN': 16,
            'DOWNLOAD_DELAY': 0.1,
            'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
            'AUTOTHROTTLE_ENABLED': True,
            'AUTOTHROTTLE_START_DELAY': 0.1,
            'AUTOTHROTTLE_MAX_DELAY': 3.0,
            'AUTOTHROTTLE_TARGET_CONCURRENCY': 8.0,
        })

        return {
            'project_path': str(full_path),
            'optimized_settings': optimized_settings,
            'task_id': task_id
        }

    def _validate_project_structure(self, project_path: str) -> bool:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã®æ¤œè¨¼ï¼ˆCPUé›†ç´„çš„ï¼‰"""
        full_path = self.base_projects_dir / project_path

        if not full_path.exists():
            return False

        # å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
        required_files = [
            full_path / 'scrapy.cfg',
            full_path / project_path / '__init__.py',
            full_path / project_path / 'settings.py',
        ]

        return all(file.exists() for file in required_files)

    def stop_spider(self, task_id: str) -> bool:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®å®Ÿè¡Œã‚’åœæ­¢ï¼ˆCeleryç’°å¢ƒå¯¾å¿œï¼‰"""
        try:
            if task_id in self.running_processes:
                process = self.running_processes[task_id]

                # ã¾ãšå„ªé›…ã«çµ‚äº†ã‚’è©¦è¡Œ
                process.terminate()
                try:
                    process.wait(timeout=5)
                except subprocess.TimeoutExpired:
                    # å¼·åˆ¶çµ‚äº†ï¼ˆCeleryç’°å¢ƒã§ã¯å®‰å…¨ãªæ–¹æ³•ã‚’ä½¿ç”¨ï¼‰
                    try:
                        process.kill()
                        process.wait()
                    except Exception as kill_error:
                        self.logger.warning(f"Failed to kill process {process.pid}: {kill_error}")

                del self.running_processes[task_id]
                if task_id in self.task_progress:
                    del self.task_progress[task_id]
                return True
            return False
        except Exception as e:
            self.logger.error(f"Error stopping spider {task_id}: {str(e)}")
            return False

    def get_task_status(self, task_id: str) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡ŒçŠ¶æ³ã‚’å–å¾—"""
        try:
            if task_id not in self.running_processes:
                return {"status": "not_found"}

            process = self.running_processes[task_id]

            if process.poll() is None:
                return {"status": "running", "pid": process.pid}
            else:
                # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†
                stdout, stderr = process.communicate()
                del self.running_processes[task_id]

                # ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
                self._update_task_completion(task_id, process.returncode == 0)

                return {
                    "status": "completed" if process.returncode == 0 else "failed",
                    "return_code": process.returncode,
                    "stdout": stdout,
                    "stderr": stderr
                }

        except Exception as e:
            return {"status": "error", "error": str(e)}

    def _update_task_completion(self, task_id: str, success: bool, items_count: int = 0, requests_count: int = 0):
        """ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°ï¼ˆæ ¹æœ¬å¯¾å¿œç‰ˆï¼‰"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus, Spider as DBSpider
            import json
            import asyncio

            db = SessionLocal()
            try:
                task = db.query(DBTask).filter(DBTask.id == task_id).first()
                if task:
                    print(f"ğŸ”§ Updating task completion for {task_id}: success={success}")

                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆæœ€å„ªå…ˆï¼‰
                    actual_items, actual_requests = self._get_accurate_task_statistics(task_id, task.project_id)
                    print(f"ğŸ“Š Task {task_id}: File-based stats - items={actual_items}, requests={actual_requests}")

                    # ç¾åœ¨ã®é€²è¡ŒçŠ¶æ³ã‚’ä¿æŒ
                    current_items = task.items_count or 0
                    current_requests = task.requests_count or 0
                    current_errors = task.error_count or 0

                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯æˆåŠŸã¨ã¿ãªã™
                    has_results = self._verify_task_results(task_id)

                    # çµ±è¨ˆæƒ…å ±ã®æ±ºå®šï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã‚’æœ€å„ªå…ˆï¼‰
                    final_items = actual_items if actual_items > 0 else current_items
                    final_requests = actual_requests if actual_requests > 0 else current_requests

                    # ã‚ˆã‚Šè©³ç´°ãªæˆåŠŸåˆ¤å®š
                    # 1. ãƒ—ãƒ­ã‚»ã‚¹ãŒæ­£å¸¸çµ‚äº† (success=True)
                    # 2. ã‚¢ã‚¤ãƒ†ãƒ ãŒå–å¾—ã•ã‚Œã¦ã„ã‚‹ (final_items > 0)
                    # 3. çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ (has_results=True)
                    task_success = success and (final_items > 0 or has_results)

                    print(f"ğŸ“Š Final statistics for task {task_id}:")
                    print(f"   Items: {final_items} (file: {actual_items}, current: {current_items})")
                    print(f"   Requests: {final_requests} (file: {actual_requests}, current: {current_requests})")
                    print(f"   Success: {task_success} (process: {success}, has_results: {has_results})")

                    # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
                    if task_success:
                        task.status = TaskStatus.FINISHED
                        task.items_count = final_items
                        task.requests_count = final_requests
                        task.error_count = current_errors
                        task.finished_at = datetime.now()
                        print(f"âœ… Task {task_id} marked as FINISHED with {final_items} items")
                    else:
                        task.status = TaskStatus.FAILED
                        task.error_count = max(current_errors, 1)
                        task.finished_at = datetime.now()
                        print(f"âŒ Task {task_id} marked as FAILED")

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆ
                    db.commit()
                    print(f"ğŸ’¾ Task {task_id} completion saved to database")

                    print(f"âœ… Task {task_id} completion updated: status={task.status}, items={task.items_count}, requests={task.requests_count}, errors={task.error_count}")

                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚’DBã«æ ¼ç´ï¼ˆå®Œäº†æ™‚ã«ç¢ºå®Ÿã«å®Ÿè¡Œï¼‰
                    if task_success and task.items_count > 0:
                        print(f"ğŸ“ Attempting to store results to DB for completed task {task_id}")
                        try:
                            self._store_results_to_db(task_id, None)  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã¯è‡ªå‹•æ¤œç´¢
                        except Exception as store_error:
                            print(f"âš ï¸ Failed to store results to DB: {store_error}")

                    # å®‰å…¨ãªWebSocketé€šçŸ¥
                    self._safe_websocket_notify_completion(task_id, {
                        "status": task.status.value,
                        "finished_at": task.finished_at.isoformat(),
                        "items_count": task.items_count,
                        "requests_count": task.requests_count,
                        "error_count": task.error_count,
                        "progress": 100 if task_success else 0
                    })
                else:
                    print(f"âš ï¸ Task {task_id} not found in database")
                    print(f"  Items: {task.items_count}, Requests: {task.requests_count}")

                    # WebSocketé€šçŸ¥ã‚’é€ä¿¡ï¼ˆéåŒæœŸï¼‰
                    try:
                        spider = db.query(DBSpider).filter(DBSpider.id == task.spider_id).first()
                        spider_name = spider.name if spider else "unknown"

                        # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆå®Œäº†æ™‚ã¯100%ï¼‰
                        notification_data = {
                            "id": task_id,
                            "name": spider_name,
                            "status": task.status.value,
                            "startedAt": task.started_at.isoformat() if task.started_at else None,
                            "finishedAt": task.finished_at.isoformat() if task.finished_at else None,
                            "itemsCount": task.items_count or 0,
                            "requestsCount": task.requests_count or 0,
                            "errorCount": task.error_count or 0,
                            "progress": 100 if task.status.value in ["FINISHED", "FAILED"] else 0
                        }

                        # WebSocketé€šçŸ¥ã‚’é€ä¿¡ï¼ˆåˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰
                        self._send_websocket_notification_async(task_id, notification_data)

                    except Exception as e:
                        print(f"Error sending WebSocket notification: {str(e)}")

            finally:
                db.close()
        except Exception as e:
            print(f"Error updating task completion: {str(e)}")

    def _send_websocket_notification_async(self, task_id: str, data: dict):
        """WebSocketé€šçŸ¥ã‚’éåŒæœŸã§é€ä¿¡"""
        try:
            import threading

            def send_notification():
                try:
                    # WebSocketãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
                    from ..websocket.manager import manager

                    # æ–°ã—ã„ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
                    import asyncio
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)

                    try:
                        loop.run_until_complete(manager.send_task_update(task_id, data))
                        print(f"WebSocket notification sent for task {task_id}")
                    finally:
                        loop.close()

                except Exception as e:
                    print(f"Error in WebSocket notification thread: {str(e)}")

            # åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
            notification_thread = threading.Thread(target=send_notification, daemon=True)
            notification_thread.start()

        except Exception as e:
            print(f"Error creating WebSocket notification thread: {str(e)}")

    def _safe_websocket_notify_completion(self, task_id: str, data: dict):
        """ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã®å®‰å…¨ãªWebSocketé€šçŸ¥"""
        try:
            # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å†…ã§ã¯éåŒæœŸå‡¦ç†ã‚’é¿ã‘ã‚‹
            print(f"ğŸ“¡ Task completion notification: {task_id} - {data.get('status', 'unknown')}")
            # å®Ÿéš›ã®WebSocketé€šçŸ¥ã¯åˆ¥ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å‡¦ç†ã•ã‚Œã‚‹
        except Exception as e:
            print(f"ğŸ“¡ WebSocket notification error: {str(e)}")

    def _get_task_statistics(self, task_id: str, project_id: str) -> tuple[int, int]:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å®Ÿéš›ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            from ..database import SessionLocal, Project as DBProject
            import glob

            db = SessionLocal()
            try:
                project = db.query(DBProject).filter(DBProject.id == project_id).first()
                if not project:
                    return 0, 0

                # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®ã«åŸºã¥ãé †åºï¼‰
                possible_paths = [
                    # å®Ÿéš›ã®ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / f"results_{task_id}.json",
                    # äºŒé‡ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / project.path / f"results_{task_id}.json",
                ]

                result_file = None
                for path in possible_paths:
                    if path.exists():
                        result_file = path
                        break

                # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å†å¸°æ¤œç´¢
                if not result_file:
                    pattern = str(self.base_projects_dir / project.path / "**" / f"results_{task_id}.json")
                    matches = glob.glob(pattern, recursive=True)
                    if matches:
                        result_file = Path(matches[0])

                if result_file and result_file.exists():
                    try:
                        with open(result_file, 'r', encoding='utf-8') as f:
                            content = f.read().strip()

                        # JSONãƒ•ã‚¡ã‚¤ãƒ«ãŒä¸å®Œå…¨ãªå ´åˆã®å¯¾å‡¦
                        if content.startswith('[') and not content.endswith(']'):
                            # æœ€å¾Œã« ] ã‚’è¿½åŠ ã—ã¦ä¿®æ­£
                            content = content.rstrip(',') + '\n]'

                        data = json.loads(content)
                        items_count = len(data) if isinstance(data, list) else 1

                        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æ¨å®šï¼ˆç°¡æ˜“ç‰ˆï¼‰
                        requests_count = max(items_count + 1, 7)  # æœ€ä½7ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆrobots.txtå«ã‚€ï¼‰

                        print(f"Statistics from result file: items={items_count}, requests={requests_count} at {result_file}")
                        return items_count, requests_count

                    except json.JSONDecodeError as e:
                        print(f"JSON decode error in {result_file}: {str(e)}")
                        # JSONã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰æ¨å®š
                        file_size = result_file.stat().st_size
                        estimated_items = max(1, file_size // 2000)  # 2KB per item estimate
                        estimated_requests = max(estimated_items + 1, 7)
                        print(f"Estimated from file size: items={estimated_items}, requests={estimated_requests}")
                        return estimated_items, estimated_requests

            finally:
                db.close()

        except Exception as e:
            print(f"Error getting task statistics: {str(e)}")

        return 0, 0

    def start_monitoring(self):
        """ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§ã‚¿ã‚¹ã‚¯ã®ç›£è¦–ã‚’é–‹å§‹ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        if self.monitoring_thread is None or not self.monitoring_thread.is_alive():
            self.stop_monitoring = False

            # ç›£è¦–çµ±è¨ˆã®åˆæœŸåŒ–
            self.monitoring_stats = {
                'started_at': datetime.now(),
                'tasks_monitored': 0,
                'tasks_completed': 0,
                'tasks_failed': 0,
                'average_execution_time': 0,
                'last_activity': datetime.now(),
                'health_checks': 0,
                'performance_metrics': {
                    'cpu_usage': [],
                    'memory_usage': [],
                    'disk_usage': []
                }
            }

            self.monitoring_thread = threading.Thread(target=self._monitor_tasks, daemon=True)
            self.monitoring_thread.start()
            print("ğŸ” Enhanced task monitoring started with performance tracking")
            print(f"ğŸ“Š Monitoring statistics initialized at {self.monitoring_stats['started_at']}")

    def stop_monitoring_tasks(self):
        """ã‚¿ã‚¹ã‚¯ç›£è¦–ã‚’åœæ­¢"""
        self.stop_monitoring = True
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=5)
            print("Task monitoring stopped")

    def _monitor_tasks(self):
        """å®šæœŸçš„ã«ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼ç›£è¦–ï¼‰"""
        print(f"Task monitoring thread started. PID: {os.getpid()}")

        while not self.stop_monitoring:
            try:
                # ç›£è¦–çŠ¶æ³ã‚’ãƒ­ã‚°å‡ºåŠ›
                if self.running_processes:
                    print(f"Monitoring {len(self.running_processes)} running processes: {list(self.running_processes.keys())}")

                # å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯
                completed_tasks = []
                for task_id, process in list(self.running_processes.items()):
                    # ãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼ç›£è¦–
                    completion_status = self._check_task_completion_multilayer(task_id, process)

                    if completion_status['completed']:
                        completed_tasks.append(task_id)
                        print(f"Task {task_id} detected as completed via {completion_status['method']}")

                        # å®Œäº†å‡¦ç†
                        try:
                            success = completion_status['success']
                            print(f"Task {task_id}: Completion status: {success}")

                            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€çµ‚ç¢ºèªï¼ˆé…å»¶å¯¾å¿œï¼‰
                            if success:
                                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚’æœ€å¤§60ç§’å¾…æ©Ÿï¼ˆScrapyéåŒæœŸæ›¸ãè¾¼ã¿å¯¾å¿œï¼‰
                                success = self._wait_for_results_file(task_id, timeout=60)
                                print(f"Task {task_id}: After file verification with wait: {success}")
                            else:
                                # ãƒ—ãƒ­ã‚»ã‚¹ãŒå¤±æ•—ã—ãŸå ´åˆã§ã‚‚çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆéƒ¨åˆ†çš„æˆåŠŸã®å¯èƒ½æ€§ï¼‰
                                print(f"Task {task_id}: Process failed, but checking for partial results...")
                                if self._wait_for_results_file(task_id, timeout=30):
                                    success = True
                                    print(f"Task {task_id}: Found partial results, marking as success")

                            self._update_task_completion(task_id, success)
                            print(f"Task {task_id} completed successfully: {success}")

                        except Exception as e:
                            print(f"Error processing completed task {task_id}: {str(e)}")
                            import traceback
                            traceback.print_exc()
                            self._update_task_completion(task_id, False)

                # å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å‰Šé™¤
                for task_id in completed_tasks:
                    if task_id in self.running_processes:
                        del self.running_processes[task_id]
                        print(f"Removed completed task {task_id} from running processes")
                    # é€²è¡ŒçŠ¶æ³ã‚‚å‰Šé™¤
                    if task_id in self.task_progress:
                        del self.task_progress[task_id]
                        print(f"Removed progress tracking for task {task_id}")

                # å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³ã‚’æ›´æ–°
                self._update_running_tasks_progress()

                # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆ30åˆ†ä»¥ä¸Šå®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’å¼·åˆ¶çµ‚äº†ï¼‰
                self._check_task_timeouts()

                # ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆ1åˆ†é–“éš”ã§å®Ÿè¡Œï¼‰
                if not hasattr(self, '_last_health_check'):
                    self._last_health_check = datetime.now()

                if (datetime.now() - self._last_health_check).total_seconds() > 60:
                    self._perform_health_check()
                    self._last_health_check = datetime.now()

                # è‡ªå‹•ä¿®å¾©æ©Ÿèƒ½ï¼ˆ2åˆ†é–“éš”ã§å®Ÿè¡Œ - ã‚ˆã‚Šç©æ¥µçš„ã«ï¼‰
                if not hasattr(self, '_last_auto_fix'):
                    self._last_auto_fix = datetime.now()

                if (datetime.now() - self._last_auto_fix).total_seconds() > 120:  # 2åˆ† = 120ç§’
                    self._auto_fix_failed_tasks()
                    self._last_auto_fix = datetime.now()

                # çµ±è¨ˆæƒ…å ±ã®æ›´æ–°
                self._update_monitoring_stats()

                # 1ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æ›´æ–°ã®ãŸã‚ï¼‰
                time.sleep(1)

            except Exception as e:
                print(f"Error in task monitoring: {str(e)}")
                import traceback
                traceback.print_exc()
                time.sleep(5)

        print("Task monitoring thread stopped")

    def _perform_health_check(self):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
        try:
            import psutil

            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)

            # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
            memory = psutil.virtual_memory()
            memory_percent = memory.percent

            # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
            disk = psutil.disk_usage('/')
            disk_percent = (disk.used / disk.total) * 100

            # çµ±è¨ˆã«è¿½åŠ 
            if hasattr(self, 'monitoring_stats'):
                self.monitoring_stats['performance_metrics']['cpu_usage'].append(cpu_percent)
                self.monitoring_stats['performance_metrics']['memory_usage'].append(memory_percent)
                self.monitoring_stats['performance_metrics']['disk_usage'].append(disk_percent)

                # æœ€æ–°10ä»¶ã®ã¿ä¿æŒ
                for metric in self.monitoring_stats['performance_metrics'].values():
                    if len(metric) > 10:
                        metric.pop(0)

                self.monitoring_stats['health_checks'] += 1

                # è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯
                warnings = []
                if cpu_percent > 80:
                    warnings.append(f"High CPU usage: {cpu_percent:.1f}%")
                if memory_percent > 80:
                    warnings.append(f"High memory usage: {memory_percent:.1f}%")
                if disk_percent > 80:
                    warnings.append(f"High disk usage: {disk_percent:.1f}%")

                if warnings:
                    print(f"âš ï¸ System warnings: {', '.join(warnings)}")
                else:
                    print(f"âœ… System health OK - CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%, Disk: {disk_percent:.1f}%")

        except ImportError:
            print("psutil not available for health check")
        except Exception as e:
            print(f"Error in health check: {str(e)}")

    def _auto_fix_failed_tasks(self):
        """å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•ä¿®å¾©"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus
            import json
            from pathlib import Path

            db = SessionLocal()
            try:
                # æœ€è¿‘ã®å¤±æ•—ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ï¼ˆéå»1æ™‚é–“ä»¥å†…ï¼‰
                one_hour_ago = datetime.now() - timedelta(hours=1)
                failed_tasks = db.query(DBTask).filter(
                    DBTask.status == TaskStatus.FAILED,
                    DBTask.started_at >= one_hour_ago
                ).all()

                if not failed_tasks:
                    return

                print(f"ğŸ”§ Auto-fixing {len(failed_tasks)} failed tasks from the last hour")
                fixed_count = 0

                for task in failed_tasks:
                    try:
                        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                        has_results = self._verify_task_results(task.id)
                        if has_results:
                            # å®Ÿéš›ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                            actual_items, actual_requests = self._get_task_statistics(task.id, task.project_id)

                            if actual_items > 0:
                                # ã‚¿ã‚¹ã‚¯ã‚’æˆåŠŸã«ä¿®æ­£
                                task.status = TaskStatus.FINISHED
                                task.items_count = actual_items
                                task.requests_count = actual_requests
                                task.error_count = 0
                                task.finished_at = datetime.now()

                                fixed_count += 1
                                print(f"âœ… Auto-fixed task {task.id[:8]}... - {actual_items} items found")

                    except Exception as e:
                        print(f"Error auto-fixing task {task.id}: {str(e)}")

                if fixed_count > 0:
                    db.commit()
                    print(f"ğŸ‰ Auto-fixed {fixed_count} tasks successfully")

            finally:
                db.close()

        except Exception as e:
            print(f"Error in auto-fix: {str(e)}")

    def _update_monitoring_stats(self):
        """ç›£è¦–çµ±è¨ˆã‚’æ›´æ–°"""
        if hasattr(self, 'monitoring_stats'):
            self.monitoring_stats['last_activity'] = datetime.now()
            self.monitoring_stats['tasks_monitored'] = len(self.running_processes)

    def _check_task_timeouts(self):
        """ã‚¿ã‚¹ã‚¯ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’ãƒã‚§ãƒƒã‚¯"""
        timeout_minutes = 45  # 45åˆ†ã«å»¶é•·ï¼ˆCeleryã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚ˆã‚Šé•·ãè¨­å®šï¼‰
        current_time = datetime.now()

        for task_id, process in list(self.running_processes.items()):
            if task_id in self.task_progress:
                start_time = self.task_progress[task_id].get('started_at')
                if start_time:
                    elapsed = (current_time - start_time).total_seconds() / 60
                    if elapsed > timeout_minutes:
                        print(f"â° Task {task_id} timeout after {elapsed:.1f} minutes, terminating...")
                        try:
                            # å„ªé›…ãªçµ‚äº†ã‚’è©¦è¡Œ
                            process.terminate()

                            # 10ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰å¼·åˆ¶çµ‚äº†
                            import threading
                            def force_kill():
                                time.sleep(10)
                                try:
                                    if process.poll() is None:  # ã¾ã å®Ÿè¡Œä¸­ã®å ´åˆ
                                        process.kill()
                                        print(f"ğŸ”ª Force killed task {task_id}")
                                except:
                                    pass
                            threading.Thread(target=force_kill, daemon=True).start()

                            # ã‚¿ã‚¹ã‚¯ã‚’å®Œäº†ã¨ã—ã¦è¨˜éŒ²ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã•ã‚Œã¦ã„ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ï¼‰
                            self._update_task_completion(task_id, True)
                            print(f"ğŸ“Š Task {task_id} marked as completed due to timeout (data may have been collected)")

                        except Exception as e:
                            print(f"Error terminating timeout task {task_id}: {str(e)}")

    def get_monitoring_stats(self) -> Dict[str, Any]:
        """ç›£è¦–çµ±è¨ˆã‚’å–å¾—"""
        if hasattr(self, 'monitoring_stats'):
            stats = self.monitoring_stats.copy()

            # å®Ÿè¡Œæ™‚é–“ã®è¨ˆç®—
            if stats['started_at']:
                uptime = (datetime.now() - stats['started_at']).total_seconds()
                stats['uptime_seconds'] = uptime
                stats['uptime_formatted'] = f"{uptime // 3600:.0f}h {(uptime % 3600) // 60:.0f}m"

            # å¹³å‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
            perf = stats['performance_metrics']
            if perf['cpu_usage']:
                stats['avg_cpu'] = sum(perf['cpu_usage']) / len(perf['cpu_usage'])
            if perf['memory_usage']:
                stats['avg_memory'] = sum(perf['memory_usage']) / len(perf['memory_usage'])
            if perf['disk_usage']:
                stats['avg_disk'] = sum(perf['disk_usage']) / len(perf['disk_usage'])

            return stats
        return {}

    def _check_task_completion_multilayer(self, task_id: str, process) -> Dict[str, Any]:
        """ãƒãƒ«ãƒãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã‚¿ã‚¹ã‚¯å®Œäº†ã‚’æ¤œå‡º"""
        try:
            # ãƒ¬ã‚¤ãƒ¤ãƒ¼1: ãƒ—ãƒ­ã‚»ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
            poll_result = process.poll()
            print(f"Task {task_id}: Process poll result: {poll_result}")

            if poll_result is not None:
                return {
                    'completed': True,
                    'success': poll_result == 0,
                    'method': 'process_poll',
                    'return_code': poll_result
                }

            # ãƒ¬ã‚¤ãƒ¤ãƒ¼2: PIDãƒ™ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
            try:
                import psutil
                if psutil.pid_exists(process.pid):
                    proc = psutil.Process(process.pid)
                    if proc.status() in [psutil.STATUS_ZOMBIE, psutil.STATUS_DEAD]:
                        print(f"Task {task_id}: Process {process.pid} is zombie/dead")
                        return {
                            'completed': True,
                            'success': True,  # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã§æœ€çµ‚åˆ¤å®š
                            'method': 'psutil_status'
                        }
                else:
                    print(f"Task {task_id}: PID {process.pid} no longer exists")
                    return {
                        'completed': True,
                        'success': True,  # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã§æœ€çµ‚åˆ¤å®š
                        'method': 'pid_not_exists'
                    }
            except ImportError:
                print("psutil not available, skipping PID-based check")
            except Exception as e:
                print(f"Error in PID-based check: {str(e)}")

            # ãƒ¬ã‚¤ãƒ¤ãƒ¼3: çµæœãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ãƒã‚§ãƒƒã‚¯ï¼ˆå®Ÿè¡Œæ™‚é–“ãŒé•·ã„å ´åˆï¼‰
            if task_id in self.task_progress:
                start_time = self.task_progress[task_id].get('started_at')
                if start_time:
                    elapsed = (datetime.now() - start_time).total_seconds()
                    # 5åˆ†ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹å ´åˆã€çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
                    if elapsed > 300:
                        if self._verify_task_results(task_id):
                            print(f"Task {task_id}: Detected completion via result file after {elapsed}s")
                            return {
                                'completed': True,
                                'success': True,
                                'method': 'result_file_timeout'
                            }

            # ãƒ¬ã‚¤ãƒ¤ãƒ¼4: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ï¼ˆä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒæ›´æ–°ã—ãŸå ´åˆï¼‰
            try:
                from ..database import SessionLocal, Task as DBTask, TaskStatus
                db = SessionLocal()
                try:
                    task = db.query(DBTask).filter(DBTask.id == task_id).first()
                    if task and task.status in [TaskStatus.FINISHED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                        print(f"Task {task_id}: Database shows task as {task.status}")
                        return {
                            'completed': True,
                            'success': task.status == TaskStatus.FINISHED,
                            'method': 'database_status'
                        }
                finally:
                    db.close()
            except Exception as e:
                print(f"Error checking database status: {str(e)}")

            return {'completed': False}

        except Exception as e:
            print(f"Error in multilayer completion check: {str(e)}")
            return {'completed': False}

    def _wait_for_results_file(self, task_id: str, timeout: int = 30) -> bool:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã‚’å¾…æ©Ÿï¼ˆæ”¹å–„ç‰ˆï¼‰"""
        print(f"ğŸ” Task {task_id}: Waiting for results file (timeout: {timeout}s)")

        start_time = time.time()
        last_log_time = 0

        while time.time() - start_time < timeout:
            elapsed = time.time() - start_time

            # 5ç§’é–“éš”ã§ãƒ­ã‚°å‡ºåŠ›
            if elapsed - last_log_time >= 5:
                print(f"â³ Task {task_id}: Still waiting for results... ({elapsed:.1f}s/{timeout}s)")
                last_log_time = elapsed

            if self._verify_task_results(task_id):
                print(f"âœ… Task {task_id}: Results file found after {elapsed:.1f}s")
                return True

            time.sleep(0.5)  # ã‚ˆã‚Šç´°ã‹ã„é–“éš”ã§ãƒã‚§ãƒƒã‚¯

        print(f"â° Task {task_id}: Timeout waiting for results file ({timeout}s)")
        return False

    def _update_running_tasks_progress(self):
        """å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³ã‚’æ›´æ–°"""
        for task_id in list(self.task_progress.keys()):
            if task_id in self.running_processes:
                try:
                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç¾åœ¨ã®é€²è¡ŒçŠ¶æ³ã‚’æ¨å®š
                    progress_info = self._estimate_task_progress(task_id)
                    if progress_info:
                        self.task_progress[task_id].update(progress_info)
                        self.task_progress[task_id]['last_update'] = datetime.now()

                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®é€²è¡ŒçŠ¶æ³ã‚‚æ›´æ–°
                        self._update_task_progress_in_db(task_id, progress_info)

                except Exception as e:
                    print(f"Error updating progress for task {task_id}: {str(e)}")

    def _estimate_task_progress(self, task_id: str) -> Dict[str, Any]:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é€²è¡ŒçŠ¶æ³ã‚’æ¨å®š"""
        try:
            from ..database import SessionLocal, Task as DBTask, Project as DBProject

            db = SessionLocal()
            try:
                task = db.query(DBTask).filter(DBTask.id == task_id).first()
                if not task:
                    return {}

                project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
                if not project:
                    return {}

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®ã«åŸºã¥ãé †åºï¼‰
                possible_paths = [
                    # å®Ÿéš›ã®ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / f"results_{task_id}.json",
                    # äºŒé‡ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / project.path / f"results_{task_id}.json",
                ]

                result_file = None
                for path in possible_paths:
                    if path.exists():
                        result_file = path
                        break

                if not result_file:
                    pattern = str(self.base_projects_dir / project.path / "**" / f"results_{task_id}.json")
                    matches = glob.glob(pattern, recursive=True)
                    if matches:
                        result_file = Path(matches[0])

                if result_file and result_file.exists():
                    with open(result_file, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‹ã‚‰é€²è¡ŒçŠ¶æ³ã‚’æ¨å®š
                            lines = content.split('\n')
                            items_count = len([line for line in lines if line.strip() and line.strip() != '[' and line.strip() != ']'])

                            # é€²è¡ŒçŠ¶æ³ã‚’è¨ˆç®—ï¼ˆçµŒé(%) = ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°/ã‚¢ã‚¤ãƒ†ãƒ æ•°ï¼‰
                            if task_id in self.task_progress:
                                start_time = self.task_progress[task_id]['started_at']
                                elapsed = (datetime.now() - start_time).total_seconds()

                                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æ¨å®šï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•° + åˆæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
                                requests_made = max(items_count + 1, 1)

                                # é€²è¡ŒçŠ¶æ³ã‚’è¨ˆç®—: æ–°æ–¹å¼ = ç¾åœ¨ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°/(ç¾åœ¨ã®ã‚¢ã‚¤ãƒ†ãƒ æ•° + pendingã‚¢ã‚¤ãƒ†ãƒ æ•°)
                                pending_items = self._estimate_pending_items(task_id, items_count, requests_made, elapsed)
                                total_estimated = items_count + pending_items

                                if total_estimated > 0:
                                    # pendingã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ã®é€²è¡ŒçŠ¶æ³è¨ˆç®—
                                    progress_percentage = min((items_count / total_estimated) * 100, 95)
                                else:
                                    # ã¾ã ã‚¢ã‚¤ãƒ†ãƒ ãŒå–å¾—ã§ãã¦ã„ãªã„å ´åˆã¯åˆæœŸå€¤
                                    progress_percentage = 10  # é–‹å§‹æ™‚ã¯10%

                                return {
                                    'items_scraped': items_count,
                                    'requests_made': requests_made,
                                    'pending_items': pending_items,
                                    'progress_percentage': progress_percentage,
                                    'estimated_total': total_estimated
                                }

            finally:
                db.close()

        except Exception as e:
            print(f"Error estimating progress for task {task_id}: {str(e)}")

        return {}

    def _update_task_progress_in_db(self, task_id: str, progress_info: Dict[str, Any]):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯é€²è¡ŒçŠ¶æ³ã‚’æ›´æ–°"""
        try:
            from ..database import SessionLocal, Task as DBTask
            import requests

            db = SessionLocal()
            try:
                task = db.query(DBTask).filter(DBTask.id == task_id).first()
                if task:
                    old_items = task.items_count or 0
                    old_requests = task.requests_count or 0

                    new_items = progress_info.get('items_scraped', 0)
                    new_requests = progress_info.get('requests_made', 0)

                    # é€²è¡ŒçŠ¶æ³ãŒå¤‰åŒ–ã—ãŸå ´åˆã®ã¿æ›´æ–°
                    if new_items != old_items or new_requests != old_requests:
                        task.items_count = new_items
                        task.requests_count = new_requests
                        db.commit()

                        # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¨ˆç®—
                        if new_items > 0:
                            pending_items = max(0, min(60 - new_items, max(new_requests - new_items, 10)))
                            total_estimated = new_items + pending_items
                            progress_percentage = min(95, (new_items / total_estimated) * 100) if total_estimated > 0 else 10
                        else:
                            progress_percentage = 5

                        # WebSocketé€šçŸ¥ã‚’é€ä¿¡ï¼ˆHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆçµŒç”±ï¼‰
                        try:
                            notification_url = "http://localhost:8000/api/tasks/internal/websocket-notify"
                            payload = {
                                "task_id": task_id,
                                "data": {
                                    "id": task_id,
                                    "status": "RUNNING",
                                    "items_count": new_items,
                                    "requests_count": new_requests,
                                    "progress": progress_percentage
                                }
                            }

                            response = requests.post(
                                notification_url,
                                json=payload,
                                timeout=0.5,
                                headers={"Content-Type": "application/json"}
                            )

                            if response.status_code == 200:
                                print(f"ğŸ“Š Progress notification sent: Task {task_id} - Items: {new_items}, Progress: {progress_percentage:.1f}%")

                        except Exception as notify_error:
                            print(f"ğŸ“¡ Progress notification error: {str(notify_error)}")

            finally:
                db.close()

        except Exception as e:
            print(f"Error updating task progress in DB: {str(e)}")

    def get_task_progress(self, task_id: str) -> Dict[str, Any]:
        """ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³ã‚’å–å¾—"""
        if task_id in self.task_progress:
            return self.task_progress[task_id].copy()
        return {}

    def _estimate_pending_items(self, task_id: str, current_items: int, requests_made: int, elapsed_seconds: float) -> int:
        """pendingã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ¨å®š"""
        try:
            # æ–¹æ³•1: çµŒéæ™‚é–“ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
            if elapsed_seconds > 30:  # 30ç§’ä»¥ä¸ŠçµŒéã—ã¦ã„ã‚‹å ´åˆ
                # ã‚¢ã‚¤ãƒ†ãƒ å–å¾—ç‡ã‚’è¨ˆç®—ï¼ˆã‚¢ã‚¤ãƒ†ãƒ /ç§’ï¼‰
                items_per_second = current_items / elapsed_seconds if elapsed_seconds > 0 else 0

                # é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã§ã¯60ã‚¢ã‚¤ãƒ†ãƒ ç¨‹åº¦ã‚’æƒ³å®š
                estimated_total = 60

                # ç¾åœ¨ã®å–å¾—ç‡ã‹ã‚‰æ®‹ã‚Šæ™‚é–“ã‚’æ¨å®š
                if items_per_second > 0:
                    remaining_items = max(0, estimated_total - current_items)
                    return remaining_items

            # æ–¹æ³•2: ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãƒ™ãƒ¼ã‚¹ã®æ¨å®š
            if requests_made > current_items:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚ˆã‚Šå¤šã„å ´åˆã€å‡¦ç†ä¸­ã®ã‚¢ã‚¤ãƒ†ãƒ ãŒã‚ã‚‹
                processing_items = requests_made - current_items
                return min(processing_items, 20)  # æœ€å¤§20ã‚¢ã‚¤ãƒ†ãƒ 

            # æ–¹æ³•3: åˆæœŸæ®µéšã®æ¨å®š
            if current_items < 10:
                # é–‹å§‹ç›´å¾Œã¯å¤šã‚ã«æ¨å®š
                return max(50 - current_items, 0)
            elif current_items < 30:
                # ä¸­é–“æ®µéš
                return max(60 - current_items, 0)
            else:
                # å¾ŒåŠæ®µéš
                return max(10, int(current_items * 0.1))  # ç¾åœ¨ã®10%ç¨‹åº¦

        except Exception as e:
            print(f"Error estimating pending items for task {task_id}: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return max(20 - current_items, 0)

    def _verify_task_results(self, task_id: str) -> bool:
        """ã‚¿ã‚¹ã‚¯ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£å¸¸ã«ç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            from ..database import SessionLocal, Task as DBTask, Project as DBProject
            import glob
            import json

            db = SessionLocal()
            try:
                task = db.query(DBTask).filter(DBTask.id == task_id).first()
                if not task:
                    return False

                project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
                if not project:
                    return False

                # è¤‡æ•°ã®ãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã§çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«é…ç½®ã«åŸºã¥ãé †åºï¼‰
                possible_paths = [
                    # å®Ÿéš›ã®ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / f"results_{task_id}.json",
                    # äºŒé‡ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
                    self.base_projects_dir / project.path / project.path / f"results_{task_id}.json",
                ]

                result_file = None
                for path in possible_paths:
                    if path.exists():
                        result_file = path
                        break

                # è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å†å¸°æ¤œç´¢
                if not result_file:
                    pattern = str(self.base_projects_dir / project.path / "**" / f"results_{task_id}.json")
                    matches = glob.glob(pattern, recursive=True)
                    if matches:
                        result_file = Path(matches[0])

                # ã•ã‚‰ã«è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€æœ€æ–°ã®results_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                if not result_file:
                    pattern = str(self.base_projects_dir / project.path / "**" / "results_*.json")
                    matches = glob.glob(pattern, recursive=True)
                    if matches:
                        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—ï¼ˆä½œæˆæ™‚é–“é †ï¼‰
                        latest_file = max(matches, key=lambda x: Path(x).stat().st_mtime)
                        # 5åˆ†ä»¥å†…ã«ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å¯¾è±¡
                        if time.time() - Path(latest_file).stat().st_mtime < 300:
                            result_file = Path(latest_file)
                            print(f"Task {task_id}: Using latest result file: {result_file}")

                if result_file and result_file.exists():
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆç©ºã§ãªã„ã‹ï¼‰
                    file_size = result_file.stat().st_size
                    if file_size > 50:  # æœ€ä½50ãƒã‚¤ãƒˆï¼ˆã‚ˆã‚Šå¯›å®¹ã«ï¼‰
                        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚‚æ¤œè¨¼
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                content = f.read().strip()
                                if content:
                                    # JSONã¨ã—ã¦è§£æå¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                                    data = json.loads(content)
                                    item_count = len(data) if isinstance(data, list) else 1

                                    print(f"Task {task_id}: Result file verified - {item_count} items, {file_size} bytes at {result_file}")

                                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµæœã‚’åæ˜ 
                                    task.items_count = item_count
                                    task.requests_count = max(item_count + 5, 10)  # æ¨å®šãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
                                    db.commit()

                                    return True
                        except json.JSONDecodeError:
                            print(f"Task {task_id}: Result file is not valid JSON, attempting repair at {result_file}")
                            # ä¸å®Œå…¨ãªJSONã®ä¿®å¾©ã‚’è©¦è¡Œ
                            return self._repair_and_verify_json(task_id, content, result_file, task, db)
                        except Exception as e:
                            print(f"Task {task_id}: Error reading result file: {e}")
                            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒååˆ†å¤§ãã‘ã‚Œã°æˆåŠŸã¨ã¿ãªã™
                            if file_size > 1000:  # 1KBä»¥ä¸Š
                                print(f"Task {task_id}: Large file size, assuming success")
                                return True
                            return False
                    else:
                        print(f"Task {task_id}: Result file is too small ({file_size} bytes) at {result_file}")
                        return False
                else:
                    print(f"Task {task_id}: Result file not found in any expected location")
                    # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šåˆ©ç”¨å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤º
                    debug_pattern = str(self.base_projects_dir / project.path / "**" / "*.json")
                    debug_matches = glob.glob(debug_pattern, recursive=True)
                    if debug_matches:
                        print(f"Task {task_id}: Available JSON files: {debug_matches[:5]}")  # æœ€åˆã®5ä»¶ã®ã¿
                    return False

            finally:
                db.close()

        except Exception as e:
            print(f"Error verifying task results: {str(e)}")
            return False

    def _repair_and_verify_json(self, task_id: str, content: str, result_file, task, db) -> bool:
        """ä¸å®Œå…¨ãªJSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿®å¾©ã—ã¦æ¤œè¨¼"""
        try:
            import json

            print(f"ğŸ”§ Task {task_id}: Attempting to repair incomplete JSON")

            # æœ€å¾Œã®ã‚«ãƒ³ãƒã‚’é™¤å»ã—ã¦é–‰ã˜æ‹¬å¼§ã‚’è¿½åŠ 
            fixed_content = content.rstrip().rstrip(',') + ']'

            try:
                data = json.loads(fixed_content)
                if isinstance(data, list) and len(data) > 0:
                    print(f"âœ… Task {task_id}: Successfully repaired JSON with {len(data)} items")

                    # ä¿®å¾©ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
                    backup_file = str(result_file) + '.backup'
                    with open(backup_file, 'w', encoding='utf-8') as f:
                        f.write(content)  # å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—

                    with open(result_file, 'w', encoding='utf-8') as f:
                        json.dump(data, f, ensure_ascii=False, indent=2)

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«çµæœã‚’åæ˜ 
                    task.items_count = len(data)
                    task.requests_count = max(len(data) + 5, 10)
                    db.commit()

                    print(f"ğŸ’¾ Task {task_id}: Repaired file saved, {len(data)} items")
                    return True

            except json.JSONDecodeError as e:
                print(f"âŒ Task {task_id}: Failed to repair JSON: {e}")

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã‘ã‚Œã°éƒ¨åˆ†çš„æˆåŠŸã¨ã¿ãªã™
                file_size = len(content)
                if file_size > 5000:  # 5KBä»¥ä¸Š
                    estimated_items = max(file_size // 200, 5)  # æ¨å®šã‚¢ã‚¤ãƒ†ãƒ æ•°
                    print(f"ğŸ“Š Task {task_id}: Large file ({file_size} bytes), estimating {estimated_items} items")

                    task.items_count = estimated_items
                    task.requests_count = estimated_items + 10
                    db.commit()

                    return True

                return False

        except Exception as e:
            print(f"âŒ Task {task_id}: Error during JSON repair: {e}")
            return False

    def _check_task_timeouts(self):
        """é•·æ™‚é–“å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå‡¦ç†"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus

            db = SessionLocal()
            try:
                # 30åˆ†ä»¥ä¸Šå®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                timeout_threshold = datetime.now() - timedelta(minutes=30)

                timeout_tasks = db.query(DBTask).filter(
                    DBTask.status == TaskStatus.RUNNING,
                    DBTask.started_at < timeout_threshold
                ).all()

                for task in timeout_tasks:
                    print(f"ğŸ” Task {task.id} timed out (started: {task.started_at}), checking for results...")

                    # ã¾ãšçµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§ã‚‚æˆåŠŸã®å¯èƒ½æ€§ï¼‰
                    if self._verify_task_results(task.id):
                        print(f"âœ… Task {task.id}: Found results despite timeout, marking as completed")
                        task.status = TaskStatus.FINISHED
                        task.finished_at = datetime.now()
                        continue

                    # ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†
                    if task.id in self.running_processes:
                        process = self.running_processes[task.id]
                        try:
                            # ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ãƒã‚§ãƒƒã‚¯
                            try:
                                import psutil
                                ps_process = psutil.Process(process.pid)
                                memory_mb = ps_process.memory_info().rss / 1024 / 1024
                                print(f"ğŸ“Š Task {task.id}: Memory usage before termination: {memory_mb:.1f}MB")
                            except (ImportError, psutil.NoSuchProcess):
                                pass

                            process.terminate()
                            time.sleep(5)
                            if process.poll() is None:
                                process.kill()
                            del self.running_processes[task.id]
                        except Exception as e:
                            print(f"âŒ Error terminating process for task {task.id}: {str(e)}")

                    # ã‚¿ã‚¹ã‚¯ã‚’å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯ï¼ˆé€²è¡ŒçŠ¶æ³ã¯ä¿æŒï¼‰
                    current_items = task.items_count or 0
                    current_requests = task.requests_count or 0
                    current_errors = task.error_count or 0

                    task.status = TaskStatus.FAILED
                    task.finished_at = datetime.now()
                    # é€²è¡ŒçŠ¶æ³ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒ
                    task.items_count = current_items
                    task.requests_count = current_requests
                    task.error_count = current_errors + 1  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ã‚’è¿½åŠ 

                    print(f"âŒ Task {task.id} timed out - preserved progress: {current_items} items, {current_requests} requests")

                if timeout_tasks:
                    db.commit()
                    print(f"Marked {len(timeout_tasks)} tasks as timed out")

            finally:
                db.close()

        except Exception as e:
            print(f"Error checking task timeouts: {str(e)}")

    def _perform_health_check(self):
        """ã‚¿ã‚¹ã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus

            db = SessionLocal()
            try:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ—ãƒ­ã‚»ã‚¹ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
                running_tasks_db = db.query(DBTask).filter(DBTask.status == TaskStatus.RUNNING).all()

                for task in running_tasks_db:
                    if task.id not in self.running_processes:
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§ã¯å®Ÿè¡Œä¸­ã ãŒã€ãƒ—ãƒ­ã‚»ã‚¹ãŒå­˜åœ¨ã—ãªã„
                        print(f"Health check: Task {task.id} marked as running but no process found")

                        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦å®Œäº†åˆ¤å®š
                        if self._verify_task_results(task.id):
                            print(f"Health check: Task {task.id} has results, marking as completed")

                            # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
                            actual_items, actual_requests = self._get_task_statistics(task.id, task.project_id)

                            # ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã•ã‚Œã¦ã„ã‚Œã°æˆåŠŸã¨ã¿ãªã™
                            if actual_items > 0:
                                task.status = TaskStatus.FINISHED
                                task.items_count = actual_items
                                task.requests_count = actual_requests
                                task.error_count = 0
                                print(f"Health check: Task {task.id} completed successfully with {actual_items} items")
                            else:
                                # ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚ã‚‹ãŒãƒ‡ãƒ¼ã‚¿ãŒãªã„å ´åˆ
                                task.status = TaskStatus.FAILED
                                task.error_count = 1
                                print(f"Health check: Task {task.id} has empty results, marking as failed")

                            task.finished_at = datetime.now()
                        else:
                            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ãªã„å ´åˆã¯å¤±æ•—ã¨ã™ã‚‹
                            print(f"Health check: Task {task.id} has no results, marking as failed")
                            task.status = TaskStatus.FAILED
                            task.finished_at = datetime.now()
                            task.error_count = 1

                # ãƒ—ãƒ­ã‚»ã‚¹ã¯å­˜åœ¨ã™ã‚‹ãŒãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã§å®Œäº†ã—ã¦ã„ã‚‹å ´åˆ
                for task_id, process in list(self.running_processes.items()):
                    task = db.query(DBTask).filter(DBTask.id == task_id).first()
                    if task and task.status in [TaskStatus.FINISHED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                        print(f"Health check: Process {task_id} still running but DB shows {task.status}")
                        try:
                            process.terminate()
                            time.sleep(2)
                            if process.poll() is None:
                                process.kill()
                            del self.running_processes[task_id]
                            print(f"Health check: Cleaned up orphaned process {task_id}")
                        except Exception as e:
                            print(f"Health check: Error cleaning up process {task_id}: {str(e)}")

                db.commit()

            finally:
                db.close()

        except Exception as e:
            print(f"Error in health check: {str(e)}")

    def fix_failed_tasks_with_results(self):
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«FAILEDã«ãªã£ã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus

            db = SessionLocal()
            try:
                # FAILEDã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                failed_tasks = db.query(DBTask).filter(DBTask.status == TaskStatus.FAILED).all()

                fixed_count = 0
                for task in failed_tasks:
                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯
                    if self._verify_task_results(task.id):
                        # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                        actual_items, actual_requests = self._get_task_statistics(task.id, task.project_id)

                        if actual_items > 0:
                            # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹ã®ã§æˆåŠŸã«å¤‰æ›´
                            task.status = TaskStatus.FINISHED
                            task.items_count = actual_items
                            task.requests_count = actual_requests
                            task.error_count = 0
                            fixed_count += 1
                            print(f"Fixed task {task.id}: {actual_items} items found, marked as FINISHED")

                if fixed_count > 0:
                    db.commit()
                    print(f"Fixed {fixed_count} failed tasks that actually had results")
                else:
                    print("No failed tasks with results found to fix")

            finally:
                db.close()

        except Exception as e:
            print(f"Error fixing failed tasks: {str(e)}")

    def delete_project(self, project_path: str) -> bool:
        """Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤"""
        try:
            # scrapy_projects/project_name ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤
            full_path = self.base_projects_dir / project_path
            if full_path.exists():
                shutil.rmtree(full_path)
                print(f"Deleted project directory: {full_path}")
            return True
        except Exception as e:
            print(f"Error deleting project: {str(e)}")
            raise Exception(f"Error deleting project: {str(e)}")

    def get_project_spiders(self, project_path: str) -> List[str]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä¸€è¦§ã‚’å–å¾—"""
        try:
            # scrapy_projects/project_name/project_name/spiders ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
            full_path = self.base_projects_dir / project_path
            spiders_dir = full_path / project_path / "spiders"

            if not spiders_dir.exists():
                print(f"Spiders directory not found: {spiders_dir}")
                return []

            spider_files = []
            for file in spiders_dir.glob("*.py"):
                if file.name != "__init__.py":
                    spider_files.append(file.stem)

            print(f"Found spiders: {spider_files}")
            return spider_files

        except Exception as e:
            print(f"Error getting spiders: {str(e)}")
            raise Exception(f"Error getting spiders: {str(e)}")

    def _get_accurate_task_statistics(self, task_id: str, project_id: str) -> tuple[int, int]:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ­£ç¢ºãªçµ±è¨ˆæƒ…å ±ã‚’å–å¾—ï¼ˆæ ¹æœ¬å¯¾å¿œç‰ˆï¼‰"""
        try:
            from ..database import SessionLocal, Project as DBProject
            import json
            from pathlib import Path

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
            db = SessionLocal()
            try:
                project = db.query(DBProject).filter(DBProject.id == project_id).first()
                if not project:
                    print(f"âš ï¸ Project not found for task {task_id}")
                    return 0, 0

                project_path = project.path
            finally:
                db.close()

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            base_dir = Path("/home/igtmtakan/workplace/python/scrapyUI/scrapy_projects")
            result_file = base_dir / project_path / f"results_{task_id}.json"

            print(f"ğŸ“ Checking result file: {result_file}")

            if not result_file.exists():
                print(f"âŒ Result file not found: {result_file}")
                return 0, 0

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯
            file_size = result_file.stat().st_size
            print(f"ğŸ“Š File size: {file_size} bytes")

            if file_size < 50:  # 50ãƒã‚¤ãƒˆæœªæº€ã¯ç©ºãƒ•ã‚¡ã‚¤ãƒ«ã¨ã¿ãªã™
                print(f"âš ï¸ File too small: {file_size} bytes")
                return 0, 0

            # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                if isinstance(data, list):
                    items_count = len(data)
                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¯æ¨å®šï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•° + 10ã€œ20ã®ç¯„å›²ï¼‰
                    requests_count = max(items_count + 10, 20)

                    print(f"âœ… Accurate stats from file: items={items_count}, requests={requests_count}")
                    return items_count, requests_count
                else:
                    # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    print(f"âœ… Single item found in file")
                    return 1, 10

            except json.JSONDecodeError as e:
                print(f"âŒ JSON decode error: {e}")
                # JSONã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã‘ã‚Œã°æ¨å®šå€¤ã‚’è¿”ã™
                if file_size > 5000:  # 5KBä»¥ä¸Š
                    estimated_items = max(file_size // 100, 10)
                    estimated_requests = estimated_items + 10
                    print(f"ğŸ“Š Estimated from file size: items={estimated_items}, requests={estimated_requests}")
                    return estimated_items, estimated_requests
                return 0, 0

        except Exception as e:
            print(f"âŒ Error in _get_accurate_task_statistics: {str(e)}")
            import traceback
            traceback.print_exc()
            return 0, 0

    def create_spider(self, project_path: str, spider_name: str, template: str = "basic") -> bool:
        """æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""
        try:
            # scrapy_projects/project_name ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§scrapy genspiderã‚’å®Ÿè¡Œ
            full_path = self.base_projects_dir / project_path

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¦ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
            cmd = [
                sys.executable, "-m", "scrapy", "genspider",
                spider_name, "example.com"
            ]

            print(f"Creating spider: {spider_name} in {full_path}")
            print(f"Command: {' '.join(cmd)}")

            result = subprocess.run(
                cmd,
                cwd=str(full_path),
                capture_output=True,
                text=True,
                check=True
            )

            print(f"Spider created successfully: {result.stdout}")
            return True

        except subprocess.CalledProcessError as e:
            print(f"Failed to create spider: {e.stderr}")
            raise Exception(f"Failed to create spider: {e.stderr}")
        except Exception as e:
            print(f"Error creating spider: {str(e)}")
            raise Exception(f"Error creating spider: {str(e)}")





    def get_project_settings(self, project_path: str) -> Dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¨­å®šã‚’å–å¾—"""
        try:
            # scrapy_projects/project_name/project_name/settings.py
            full_path = self.base_projects_dir / project_path
            settings_file = full_path / project_path / "settings.py"

            if not settings_file.exists():
                print(f"Settings file not found: {settings_file}")
                return {}

            # settings.pyã‚’èª­ã¿è¾¼ã‚“ã§è¨­å®šã‚’æŠ½å‡º
            # ç°¡å˜ãªå®Ÿè£…ã¨ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è¿”ã™
            with open(settings_file, 'r', encoding='utf-8') as f:
                content = f.read()

            return {"content": content}

        except Exception as e:
            print(f"Error reading project settings: {str(e)}")
            raise Exception(f"Error reading project settings: {str(e)}")

    def validate_spider_code(self, code: str) -> Dict[str, Any]:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
        try:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã‚“ã§æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
                f.write(code)
                temp_file = f.name

            try:
                # Pythonã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯
                with open(temp_file, 'r') as f:
                    compile(f.read(), temp_file, 'exec')

                return {"valid": True, "errors": []}

            except SyntaxError as e:
                return {
                    "valid": False,
                    "errors": [f"Syntax error at line {e.lineno}: {e.msg}"]
                }
            finally:
                os.unlink(temp_file)

        except Exception as e:
            return {
                "valid": False,
                "errors": [f"Validation error: {str(e)}"]
            }
