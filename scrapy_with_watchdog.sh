#!/bin/bash
"""
scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’watchdogç›£è¦–ä»˜ãã§å®Ÿè¡Œã™ã‚‹ã‚·ã‚§ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä½¿ç”¨ä¾‹: ./scrapy_with_watchdog.sh spider_name
"""

# è¨­å®š
SPIDER_NAME=$1
TASK_ID=${2:-"shell_$(date +%s)"}
PROJECT_PATH=$(pwd)
OUTPUT_FILE="results_${TASK_ID}.jsonl"
DB_PATH="backend/database/scrapy_ui.db"

if [ -z "$SPIDER_NAME" ]; then
    echo "âŒ Usage: $0 <spider_name> [task_id]"
    exit 1
fi

echo "ğŸ¯ Starting Scrapy with watchdog monitoring"
echo "   Spider: $SPIDER_NAME"
echo "   Task ID: $TASK_ID"
echo "   Output: $OUTPUT_FILE"
echo "   Project: $PROJECT_PATH"

# Python watchdogç›£è¦–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
cat > /tmp/watchdog_monitor_${TASK_ID}.py << EOF
#!/usr/bin/env python3
import asyncio
import sys
import os
sys.path.append('${PROJECT_PATH}/backend/app/services')

from scrapy_watchdog_monitor import ScrapyWatchdogMonitor

async def main():
    monitor = ScrapyWatchdogMonitor(
        task_id='${TASK_ID}',
        project_path='${PROJECT_PATH}',
        spider_name='${SPIDER_NAME}',
        db_path='${DB_PATH}'
    )
    
    await monitor._start_jsonl_monitoring('${OUTPUT_FILE}')

if __name__ == "__main__":
    asyncio.run(main())
EOF

# watchdogç›£è¦–ã‚’ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§é–‹å§‹
echo "ğŸ” Starting watchdog monitoring..."
python /tmp/watchdog_monitor_${TASK_ID}.py &
WATCHDOG_PID=$!

# å°‘ã—å¾…ã£ã¦ã‹ã‚‰Scrapyã‚’é–‹å§‹
sleep 2

echo "ğŸš€ Starting Scrapy crawler..."
scrapy crawl $SPIDER_NAME -o $OUTPUT_FILE -s FEED_FORMAT=jsonlines -s LOG_LEVEL=INFO

# ScrapyãŒå®Œäº†ã—ãŸã‚‰watchdogç›£è¦–ã‚’åœæ­¢
echo "ğŸ›‘ Stopping watchdog monitoring..."
kill $WATCHDOG_PID 2>/dev/null

# ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
rm -f /tmp/watchdog_monitor_${TASK_ID}.py

echo "âœ… Scrapy with watchdog monitoring completed"
echo "   Results saved to: $OUTPUT_FILE"
echo "   Database updated with real-time inserts"
