"""
ScrapyUI Rich Progress Extension

Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«richãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸç¾ã—ã„é€²æ—ãƒãƒ¼ã‚’è¿½åŠ ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Optional, Dict, Any
from scrapy import signals
from scrapy.crawler import Crawler
from scrapy.spiders import Spider
from scrapy.http import Request, Response
from scrapy.exceptions import NotConfigured

try:
    from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn, SpinnerColumn
    from rich.console import Console
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class RichProgressExtension:
    """
    Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ç”¨Riché€²æ—ãƒãƒ¼æ‹¡å¼µæ©Ÿèƒ½
    
    Features:
    - ç¾ã—ã„é€²æ—ãƒãƒ¼è¡¨ç¤º
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæƒ…å ±
    - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè¡¨ç¤ºå½¢å¼
    - WebSocketçµŒç”±ã§ã®é€²æ—é€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    
    def __init__(self, crawler: Crawler):
        if not RICH_AVAILABLE:
            raise NotConfigured("Rich library is not installed. Run: pip install rich")

        self.crawler = crawler
        self.settings = crawler.settings

        # Riché€²æ—ãƒãƒ¼è¨­å®š
        self.console = Console()
        self.progress = None
        self.live = None
        self.task_id: Optional[TaskID] = None

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'requests_count': 0,
            'responses_count': 0,
            'items_count': 0,
            'errors_count': 0,
            'start_time': None,
            'finish_time': None,
            'total_urls': 0
        }

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.stats_file = None
        self.task_id_str = None

        # è¨­å®š
        self.enabled = self.settings.getbool('RICH_PROGRESS_ENABLED', True)
        self.show_stats = self.settings.getbool('RICH_PROGRESS_SHOW_STATS', True)
        self.update_interval = self.settings.getfloat('RICH_PROGRESS_UPDATE_INTERVAL', 0.1)
        self.websocket_enabled = self.settings.getbool('RICH_PROGRESS_WEBSOCKET', False)

        if not self.enabled:
            raise NotConfigured("Rich progress bar is disabled")
    
    @classmethod
    def from_crawler(cls, crawler: Crawler):
        """Crawlerã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        extension = cls(crawler)
        
        # ã‚·ã‚°ãƒŠãƒ«ã‚’æ¥ç¶š
        crawler.signals.connect(extension.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(extension.request_scheduled, signal=signals.request_scheduled)
        crawler.signals.connect(extension.response_received, signal=signals.response_received)
        crawler.signals.connect(extension.item_scraped, signal=signals.item_scraped)
        crawler.signals.connect(extension.spider_error, signal=signals.spider_error)
        
        return extension
    
    def spider_opened(self, spider: Spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†"""
        self.stats['start_time'] = time.time()

        # ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯crawlerã‹ã‚‰ï¼‰
        self.task_id_str = (
            os.environ.get('SCRAPY_TASK_ID') or
            getattr(self.crawler, 'task_id', None) or
            f"task_{int(time.time())}"
        )

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
        project_dir = Path.cwd()
        self.stats_file = project_dir / f"stats_{self.task_id_str}.json"

        # start_urlsã®æ•°ã‚’å–å¾—
        if hasattr(spider, 'start_urls'):
            self.stats['total_urls'] = len(spider.start_urls)

        # åˆæœŸçµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        self._save_stats()

        # Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–
        self._initialize_progress(spider)

        spider.logger.info(f"ğŸ¨ Riché€²æ—ãƒãƒ¼é–‹å§‹: {spider.name}")
        spider.logger.info(f"ğŸ“Š çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«: {self.stats_file}")
    
    def spider_closed(self, spider: Spider, reason: str):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†"""
        # çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        self.stats['finish_time'] = time.time()

        # Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸ
        self._sync_with_scrapy_stats()

        # æœ€çµ‚çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        self._save_stats()

        # å®Œäº†é€šçŸ¥ã¨ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç™ºå‹•
        if reason == 'finished' and hasattr(spider, 'task_id'):
            spider.logger.info(f"ğŸ¯ Spider completed successfully with Rich progress tracking for task {spider.task_id}")

            # Rich progresså®Œäº†é€šçŸ¥ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’ç™ºå‹•
            self._trigger_bulk_insert_on_completion(spider)

        if self.live:
            self.live.stop()

        if self.progress:
            self.progress.stop()

        # æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º
        self._show_final_stats(spider, reason)

    def _trigger_bulk_insert_on_completion(self, spider):
        """Rich progresså®Œäº†é€šçŸ¥ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’ç™ºå‹•"""
        try:
            task_id = getattr(spider, 'task_id', None)
            if not task_id:
                spider.logger.warning("ğŸ” Task ID not found - skipping bulk insert")
                return

            spider.logger.info(f"ğŸš€ Rich progress completion triggered - starting bulk insert for task {task_id}")

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
            project_path = getattr(spider, 'project_path', None)
            if not project_path:
                # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¨æ¸¬
                import os
                project_path = os.getcwd()

            spider.logger.info(f"ğŸ“ Project path: {project_path}")

            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
            from pathlib import Path
            jsonl_file_path = Path(project_path) / f"results_{task_id}.jsonl"

            if not jsonl_file_path.exists():
                spider.logger.warning(f"ğŸ“„ JSONL file not found: {jsonl_file_path}")
                return

            spider.logger.info(f"ğŸ“„ Found JSONL file: {jsonl_file_path}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°ã‚’ç¢ºèª
            file_size = jsonl_file_path.stat().st_size
            with open(jsonl_file_path, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]

            spider.logger.info(f"ğŸ“Š File size: {file_size} bytes, Lines: {len(lines)}")

            if len(lines) == 0:
                spider.logger.warning("ğŸ“„ No data lines found in JSONL file")
                return

            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
            self._execute_bulk_insert(task_id, lines, spider)

        except Exception as e:
            spider.logger.error(f"âŒ Bulk insert trigger error: {e}")
            import traceback
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")

    def _execute_bulk_insert(self, task_id: str, lines: list, spider):
        """ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        try:
            spider.logger.info(f"ğŸ”„ Starting bulk insert for {len(lines)} lines")

            # ScrapyWatchdogMonitorã®ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆæ©Ÿèƒ½ã‚’ä½¿ç”¨
            from ..services.scrapy_watchdog_monitor import ScrapyWatchdogMonitor

            # ä¸€æ™‚çš„ãªãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå°‚ç”¨ï¼‰
            monitor = ScrapyWatchdogMonitor(
                task_id=task_id,
                project_path=getattr(spider, 'project_path', os.getcwd()),
                spider_name=spider.name
            )

            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
            successful_inserts = monitor._bulk_insert_items_threading(lines)

            spider.logger.info(f"âœ… Bulk insert completed: {successful_inserts}/{len(lines)} items inserted")

            # é‡è¤‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
            cleanup_result = self._cleanup_duplicate_records(task_id, spider)

            # WebSocketé€šçŸ¥ã‚’é€ä¿¡
            self._send_completion_websocket_notification(task_id, successful_inserts, spider, cleanup_result)

        except Exception as e:
            spider.logger.error(f"âŒ Bulk insert execution error: {e}")
            import traceback
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")

    def _cleanup_duplicate_records(self, task_id: str, spider):
        """é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        try:
            spider.logger.info(f"ğŸ§¹ Starting duplicate cleanup for task {task_id}")

            from ..database import get_db, Result as DBResult
            from sqlalchemy import func

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            db_gen = get_db()
            db = next(db_gen)

            try:
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‰ã®ä»¶æ•°ã‚’ç¢ºèª
                before_count = db.query(DBResult).filter(DBResult.task_id == task_id).count()
                spider.logger.info(f"ğŸ“Š Before cleanup: {before_count} records")

                # é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®šï¼ˆdata_hashãŒåŒã˜ã‚‚ã®ã‚’æ¤œç´¢ï¼‰
                duplicate_subquery = (
                    db.query(DBResult.data_hash)
                    .filter(DBResult.task_id == task_id)
                    .group_by(DBResult.data_hash)
                    .having(func.count(DBResult.data_hash) > 1)
                    .subquery()
                )

                # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æœ€æ–°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å‰Šé™¤
                duplicates_to_delete = []
                duplicate_hashes = db.query(duplicate_subquery.c.data_hash).all()

                spider.logger.info(f"ğŸ” Found {len(duplicate_hashes)} duplicate hash groups")

                for (hash_value,) in duplicate_hashes:
                    # åŒã˜ãƒãƒƒã‚·ãƒ¥ã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆä½œæˆæ—¥æ™‚é †ï¼‰
                    duplicate_records = (
                        db.query(DBResult)
                        .filter(DBResult.task_id == task_id)
                        .filter(DBResult.data_hash == hash_value)
                        .order_by(DBResult.created_at.desc())
                        .all()
                    )

                    # æœ€æ–°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
                    if len(duplicate_records) > 1:
                        records_to_delete = duplicate_records[1:]  # æœ€æ–°ä»¥å¤–
                        duplicates_to_delete.extend(records_to_delete)

                        spider.logger.info(f"ğŸ—‘ï¸ Hash {hash_value[:8]}...: keeping 1, deleting {len(records_to_delete)} duplicates")

                # é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
                deleted_count = 0
                if duplicates_to_delete:
                    for record in duplicates_to_delete:
                        db.delete(record)
                        deleted_count += 1

                    db.commit()
                    spider.logger.info(f"âœ… Deleted {deleted_count} duplicate records")
                else:
                    spider.logger.info(f"âœ… No duplicate records found to delete")

                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ä»¶æ•°ã‚’ç¢ºèª
                after_count = db.query(DBResult).filter(DBResult.task_id == task_id).count()
                spider.logger.info(f"ğŸ“Š After cleanup: {after_count} records")

                # çµæœã‚’ã¾ã¨ã‚ã‚‹
                cleanup_result = {
                    'before_count': before_count,
                    'after_count': after_count,
                    'deleted_count': deleted_count,
                    'duplicate_groups': len(duplicate_hashes)
                }

                spider.logger.info(f"ğŸ§¹ Cleanup completed: {before_count} â†’ {after_count} (-{deleted_count})")

                return cleanup_result

            finally:
                db.close()

        except Exception as e:
            spider.logger.error(f"âŒ Duplicate cleanup error: {e}")
            import traceback
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            return {
                'before_count': 0,
                'after_count': 0,
                'deleted_count': 0,
                'duplicate_groups': 0,
                'error': str(e)
            }

    def _send_completion_websocket_notification(self, task_id: str, items_inserted: int, spider, cleanup_result=None):
        """å®Œäº†é€šçŸ¥ã®WebSocketé€ä¿¡"""
        try:
            # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            completion_data = {
                'taskId': task_id,
                'status': 'completed',
                'itemsScraped': items_inserted,
                'requestsCount': self.stats['requests_count'],
                'errorCount': self.stats['errors_count'],
                'elapsedTime': int(self.stats.get('finish_time', time.time()) - self.stats.get('start_time', time.time())),
                'progressPercentage': 100.0,
                'message': f'Rich progress completed - {items_inserted} items bulk inserted',
                'bulkInsertCompleted': True
            }

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚’è¿½åŠ 
            if cleanup_result:
                completion_data.update({
                    'cleanupCompleted': True,
                    'cleanupResult': cleanup_result,
                    'finalItemCount': cleanup_result.get('after_count', items_inserted),
                    'duplicatesRemoved': cleanup_result.get('deleted_count', 0)
                })

                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
                deleted_count = cleanup_result.get('deleted_count', 0)
                if deleted_count > 0:
                    completion_data['message'] = f'Rich progress completed - {items_inserted} items inserted, {deleted_count} duplicates removed'
                else:
                    completion_data['message'] = f'Rich progress completed - {items_inserted} items inserted, no duplicates found'

            spider.logger.info(f"ğŸ“¡ Sending completion WebSocket notification: {completion_data}")

            # WebSocketé€ä¿¡ï¼ˆéåŒæœŸï¼‰
            import asyncio
            from ..api.websocket_progress import broadcast_rich_progress_update

            try:
                # éåŒæœŸã§WebSocketé€ä¿¡
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(
                        broadcast_rich_progress_update(task_id, completion_data)
                    )
                else:
                    loop.run_until_complete(
                        broadcast_rich_progress_update(task_id, completion_data)
                    )
                spider.logger.info("ğŸ“¡ Completion WebSocket notification sent successfully")
            except Exception as ws_error:
                spider.logger.warning(f"ğŸ“¡ WebSocket notification failed: {ws_error}")

        except Exception as e:
            spider.logger.error(f"âŒ Completion notification error: {e}")
    
    def request_scheduled(self, request: Request, spider: Spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚ã®å‡¦ç†"""
        self.stats['requests_count'] += 1
        self._update_progress()
        self._save_stats()

    def response_received(self, response: Response, request: Request, spider: Spider):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡æ™‚ã®å‡¦ç†"""
        self.stats['responses_count'] += 1
        self._update_progress()
        self._save_stats()

    def item_scraped(self, item: Dict[str, Any], response: Response, spider: Spider):
        """ã‚¢ã‚¤ãƒ†ãƒ å–å¾—æ™‚ã®å‡¦ç†"""
        self.stats['items_count'] += 1
        self._update_progress()
        self._save_stats()

    def spider_error(self, failure, response: Response, spider: Spider):
        """ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†"""
        self.stats['errors_count'] += 1
        self._update_progress()
        self._save_stats()
    
    def _initialize_progress(self, spider: Spider):
        """Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–"""
        # ã‚«ã‚¹ã‚¿ãƒ é€²æ—ãƒãƒ¼ã‚«ãƒ©ãƒ 
        columns = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=40),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("â€¢"),
            TextColumn("[bold green]{task.completed}/{task.total}"),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
        ]
        
        self.progress = Progress(*columns, console=self.console)
        
        # ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
        total = max(self.stats['total_urls'], 1)  # æœ€ä½1ã«è¨­å®š
        self.task_id = self.progress.add_task(
            f"ğŸ•·ï¸ {spider.name}",
            total=total
        )
        
        if self.show_stats:
            # ãƒ©ã‚¤ãƒ–è¡¨ç¤ºã§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨é€²æ—ãƒãƒ¼ã‚’çµ„ã¿åˆã‚ã›
            self.live = Live(self._create_layout(), console=self.console, refresh_per_second=10)
            self.live.start()
        else:
            self.progress.start()
    
    def _create_layout(self):
        """è¡¨ç¤ºãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ"""
        # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        stats_table = Table(title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ", show_header=True, header_style="bold magenta")
        stats_table.add_column("é …ç›®", style="cyan", width=15)
        stats_table.add_column("å€¤", style="green", width=10)
        
        # çµŒéæ™‚é–“ã‚’è¨ˆç®—
        elapsed_time = 0
        if self.stats['start_time']:
            import time
            elapsed_time = time.time() - self.stats['start_time']
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        stats_table.add_row("ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", str(self.stats['requests_count']))
        stats_table.add_row("ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹", str(self.stats['responses_count']))
        stats_table.add_row("ğŸ“¦ ã‚¢ã‚¤ãƒ†ãƒ ", str(self.stats['items_count']))
        stats_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼", str(self.stats['errors_count']))
        stats_table.add_row("â±ï¸ çµŒéæ™‚é–“", f"{elapsed_time:.1f}ç§’")
        
        # é€Ÿåº¦è¨ˆç®—
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            stats_table.add_row("ğŸš€ å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’çµ„ã¿åˆã‚ã›
        layout = Table.grid()
        layout.add_row(Panel(self.progress, title="ğŸ¯ é€²æ—çŠ¶æ³", border_style="blue"))
        layout.add_row(Panel(stats_table, title="ğŸ“ˆ è©³ç´°çµ±è¨ˆ", border_style="green"))
        
        return layout
    
    def _update_progress(self):
        """é€²æ—ãƒãƒ¼ã‚’æ›´æ–°"""
        if not self.progress or not self.task_id:
            return
        
        # é€²æ—ã‚’æ›´æ–°ï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        completed = self.stats['items_count']
        
        # å‹•çš„ã«ç·æ•°ã‚’èª¿æ•´ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒåˆæœŸäºˆæƒ³ã‚’è¶…ãˆãŸå ´åˆï¼‰
        if self.stats['requests_count'] > self.stats['total_urls']:
            total = max(self.stats['requests_count'], self.stats['total_urls'])
            self.progress.update(self.task_id, total=total)
        
        self.progress.update(self.task_id, completed=completed)
        
        # WebSocketé€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.websocket_enabled:
            self._send_websocket_update()
    
    def _send_websocket_update(self):
        """WebSocketçµŒç”±ã§é€²æ—ã‚’é€šçŸ¥"""
        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                import time
                elapsed_time = time.time() - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time

            # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            progress_data = {
                'taskId': getattr(self.crawler, 'task_id', 'unknown'),
                'status': 'running',
                'itemsScraped': self.stats['items_count'],
                'requestsCount': self.stats['requests_count'],
                'errorCount': self.stats['errors_count'],
                'elapsedTime': int(elapsed_time),
                'progressPercentage': self._calculate_progress_percentage(),
                'itemsPerSecond': round(items_per_second, 2),
                'requestsPerSecond': round(requests_per_second, 2),
                'totalPages': self.stats['total_urls'],
                'currentPage': self.stats['responses_count']
            }

            # ScrapyUIã®WebSocketãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«é€ä¿¡
            try:
                import asyncio
                from ..api.websocket_progress import broadcast_rich_progress_update

                # éåŒæœŸã§WebSocketé€ä¿¡
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
                else:
                    loop.run_until_complete(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
            except Exception as ws_error:
                # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
                pass

        except Exception as e:
            # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass
    
    def _calculate_progress_percentage(self) -> float:
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        if self.stats['total_urls'] == 0:
            return 0.0
        
        # ã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—
        return min(100.0, (self.stats['items_count'] / self.stats['total_urls']) * 100)
    
    def _show_final_stats(self, spider: Spider, reason: str):
        """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
        import time
        
        elapsed_time = 0
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_table = Table(title=f"ğŸ {spider.name} å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ", show_header=True, header_style="bold yellow")
        final_table.add_column("é …ç›®", style="cyan", width=20)
        final_table.add_column("å€¤", style="green", width=15)
        
        final_table.add_row("ğŸ“¤ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", str(self.stats['requests_count']))
        final_table.add_row("ğŸ“¥ ç·ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°", str(self.stats['responses_count']))
        final_table.add_row("ğŸ“¦ ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°", str(self.stats['items_count']))
        final_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼æ•°", str(self.stats['errors_count']))
        final_table.add_row("â±ï¸ ç·å®Ÿè¡Œæ™‚é–“", f"{elapsed_time:.2f}ç§’")
        final_table.add_row("ğŸ çµ‚äº†ç†ç”±", reason)
        
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            final_table.add_row("ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        self.console.print(Panel(final_table, title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†", border_style="yellow"))

    def _save_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.stats_file:
            return

        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                current_time = self.stats['finish_time'] or time.time()
                elapsed_time = current_time - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            items_per_minute = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time
                items_per_minute = items_per_second * 60

            # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡è¨ˆç®—
            total_responses = self.stats['responses_count']
            success_rate = 0
            error_rate = 0
            if total_responses > 0:
                success_rate = ((total_responses - self.stats['errors_count']) / total_responses) * 100
                error_rate = (self.stats['errors_count'] / total_responses) * 100

            # Scrapyæ¨™æº–å½¢å¼ã®çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
            scrapy_stats = {
                # åŸºæœ¬çµ±è¨ˆ
                'item_scraped_count': self.stats['items_count'],
                'downloader/request_count': self.stats['requests_count'],
                'response_received_count': self.stats['responses_count'],
                'spider_exceptions': self.stats['errors_count'],

                # æ™‚é–“æƒ…å ±
                'elapsed_time_seconds': elapsed_time,
                'start_time': self.stats['start_time'],
                'finish_time': self.stats['finish_time'],

                # é€Ÿåº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                'items_per_second': items_per_second,
                'requests_per_second': requests_per_second,
                'items_per_minute': items_per_minute,

                # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡
                'success_rate': success_rate,
                'error_rate': error_rate,

                # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'downloader/response_status_count/200': max(0, self.stats['responses_count'] - self.stats['errors_count']),
                'downloader/response_status_count/404': 0,
                'downloader/response_status_count/500': self.stats['errors_count'],

                # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'log_count/DEBUG': 0,
                'log_count/INFO': self.stats['items_count'],
                'log_count/WARNING': 0,
                'log_count/ERROR': self.stats['errors_count'],
                'log_count/CRITICAL': 0,

                # Rich progressæ‹¡å¼µçµ±è¨ˆ
                'rich_progress_enabled': True,
                'rich_progress_version': '1.0.0'
            }

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(scrapy_stats, f, indent=2, ensure_ascii=False)

        except Exception as e:
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass

    def _sync_with_scrapy_stats(self):
        """Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸ"""
        try:
            if hasattr(self.crawler, 'stats'):
                scrapy_stats = self.crawler.stats

                # Scrapyã®çµ±è¨ˆæƒ…å ±ã‹ã‚‰å€¤ã‚’å–å¾—
                self.stats['items_count'] = scrapy_stats.get_value('item_scraped_count', self.stats['items_count'])
                self.stats['requests_count'] = scrapy_stats.get_value('downloader/request_count', self.stats['requests_count'])
                self.stats['responses_count'] = scrapy_stats.get_value('response_received_count', self.stats['responses_count'])
                self.stats['errors_count'] = scrapy_stats.get_value('spider_exceptions', self.stats['errors_count'])

        except Exception as e:
            # åŒæœŸã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            pass




# è¨­å®šä¾‹ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã§è¨˜è¼‰
"""
# settings.pyã«è¿½åŠ ã™ã‚‹è¨­å®šä¾‹

# Riché€²æ—ãƒãƒ¼ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_ENABLED = True

# è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
RICH_PROGRESS_SHOW_STATS = True

# æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰
RICH_PROGRESS_UPDATE_INTERVAL = 0.1

# WebSocketé€šçŸ¥ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_WEBSOCKET = True

# æ‹¡å¼µæ©Ÿèƒ½ã‚’ç™»éŒ²
EXTENSIONS = {
    'app.scrapy_extensions.rich_progress_extension.RichProgressExtension': 500,
}
"""
