#!/usr/bin/env python3
"""
Scrapyã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒãƒ³ãƒ‰: crawlwithwatchdog
watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ

ä½¿ç”¨ä¾‹:
scrapy crawlwithwatchdog spider_name -o results.jsonl --task-id=test_123
"""
import asyncio
import threading
import time
import uuid
import json
import sqlite3
import os
import sys
from pathlib import Path
from datetime import datetime
from scrapy.commands import ScrapyCommand
from scrapy.utils.conf import arglist_to_dict
from scrapy.exceptions import UsageError

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, monitor):
        self.monitor = monitor
        
    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return
            
        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self.monitor.process_new_lines,
                daemon=True
            ).start()


class JSONLMonitor:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, task_id, spider_name, jsonl_file_path, db_path):
        self.task_id = task_id
        self.spider_name = spider_name
        self.jsonl_file_path = Path(jsonl_file_path)
        self.db_path = db_path
        self.processed_lines = 0
        self.last_file_size = 0
        self.is_monitoring = False
        self.observer = None
        
    def start_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        if not WATCHDOG_AVAILABLE:
            print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._start_polling_monitoring()
            return
        
        self.is_monitoring = True
        
        # watchdogç›£è¦–ã‚’é–‹å§‹
        event_handler = JSONLWatchdogHandler(self)
        self.observer = Observer()
        self.observer.schedule(event_handler, str(self.jsonl_file_path.parent), recursive=False)
        self.observer.start()
        
        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {self.jsonl_file_path}")
        
    def _start_polling_monitoring(self):
        """ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        self.is_monitoring = True
        
        def polling_loop():
            while self.is_monitoring:
                self.process_new_lines()
                time.sleep(1)
        
        polling_thread = threading.Thread(target=polling_loop, daemon=True)
        polling_thread.start()
        
        print(f"ğŸ”„ ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–é–‹å§‹: {self.jsonl_file_path}")
    
    def stop_monitoring(self):
        """ç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False
        
        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None
        
        print(f"ğŸ›‘ ç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")
    
    def process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†"""
        try:
            if not self.jsonl_file_path.exists():
                return
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            if current_size <= self.last_file_size:
                return
            
            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()
            
            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            
            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")
                
                for line in new_lines:
                    self._process_single_line(line)
                    self.processed_lines += 1
                
                print(f"ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.processed_lines}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            self.last_file_size = current_size
            
        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _process_single_line(self, json_line):
        """å˜ä¸€ã®è¡Œã‚’å‡¦ç†ã—ã¦DBã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            # JSONè§£æ
            item_data = json.loads(json_line)
            
            # DBã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            self._insert_item_to_db(item_data)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {json_line[:100]}...")
        except Exception as e:
            print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _insert_item_to_db(self, item_data):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # scraped_itemsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            item_id = str(uuid.uuid4())
            cursor.execute("""
                INSERT INTO scraped_items 
                (id, task_id, project_id, spider_name, data, scraped_at, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                item_id,
                self.task_id,
                "command_project",  # ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œæ™‚ã¯ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’å›ºå®š
                self.spider_name,
                json.dumps(item_data, ensure_ascii=False),
                datetime.now().isoformat(),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
            print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {item_id}")
            
        except Exception as e:
            print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")


class Command(ScrapyCommand):
    """watchdogç›£è¦–ä»˜ãcrawlã‚³ãƒãƒ³ãƒ‰"""
    
    requires_project = True
    default_settings = {'LOG_LEVEL': 'INFO'}

    def syntax(self):
        return "<spider> [options]"

    def short_desc(self):
        return "Run a spider with watchdog monitoring for real-time DB insertion"

    def long_desc(self):
        return """
Run a spider with watchdog monitoring that automatically inserts
scraped items into the database in real-time as they are written
to the JSONL output file.

Examples:
  scrapy crawlwithwatchdog myspider -o results.jsonl --task-id=test_123
  scrapy crawlwithwatchdog myspider -o results.jsonl --db-path=/path/to/db.sqlite
        """

    def add_options(self, parser):
        ScrapyCommand.add_options(self, parser)
        parser.add_option("-o", "--output", dest="output",
                         help="dump scraped items to JSONL file (required for watchdog monitoring)")
        parser.add_option("-t", "--output-format", dest="output_format", default="jsonlines",
                         help="format to use for dumping items (default: jsonlines)")
        parser.add_option("--task-id", dest="task_id",
                         help="task ID for monitoring (auto-generated if not provided)")
        parser.add_option("--db-path", dest="db_path", 
                         default="backend/database/scrapy_ui.db",
                         help="database path for storing results")

    def process_options(self, args, opts):
        ScrapyCommand.process_options(self, args, opts)
        try:
            opts.spargs, opts.spkwargs = arglist_to_dict(args[1:])
        except ValueError:
            raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)

    def run(self, args, opts):
        if len(args) < 1:
            raise UsageError("Spider name is required")

        spider_name = args[0]
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not opts.output:
            raise UsageError("Output file (-o) is required for watchdog monitoring")
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if not opts.output.endswith('.jsonl'):
            print("âš ï¸ Warning: Output file should be .jsonl for optimal monitoring")
        
        # ã‚¿ã‚¹ã‚¯IDã‚’ç”Ÿæˆã¾ãŸã¯å–å¾—
        task_id = opts.task_id or f"cmd_{spider_name}_{int(time.time())}"
        
        print(f"ğŸš€ Starting spider with watchdog monitoring")
        print(f"   Spider: {spider_name}")
        print(f"   Task ID: {task_id}")
        print(f"   Output: {opts.output}")
        print(f"   DB Path: {opts.db_path}")
        print(f"   Watchdog Available: {'Yes' if WATCHDOG_AVAILABLE else 'No (using polling)'}")
        
        # watchdogç›£è¦–ã‚’é–‹å§‹
        monitor = JSONLMonitor(
            task_id=task_id,
            spider_name=spider_name,
            jsonl_file_path=opts.output,
            db_path=opts.db_path
        )
        
        monitor.start_monitoring()
        
        try:
            # Scrapyã®è¨­å®šã‚’æ›´æ–°
            self.settings.set('FEED_URI', opts.output)
            self.settings.set('FEED_FORMAT', opts.output_format or 'jsonlines')
            
            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
            print(f"ğŸ•·ï¸ Starting Scrapy crawler...")
            self.crawler_process.crawl(spider_name, **opts.spkwargs)
            self.crawler_process.start()
            
        except KeyboardInterrupt:
            print(f"\nâš ï¸ Interrupted by user")
        except Exception as e:
            print(f"âŒ Crawler error: {e}")
        finally:
            # ç›£è¦–ã‚’åœæ­¢
            monitor.stop_monitoring()
            
            # æœ€çµ‚çš„ãªçµ±è¨ˆã‚’è¡¨ç¤º
            print(f"\nğŸ“Š Final Statistics:")
            print(f"   Total items processed: {monitor.processed_lines}")
            print(f"   Output file: {opts.output}")
            print(f"   Database: {opts.db_path}")
            print(f"âœ… crawlwithwatchdog completed")
