from fastapi import APIRouter, Depends, HTTPException, status, Query, Response
from fastapi.responses import FileResponse, StreamingResponse
from sqlalchemy.orm import Session
from typing import List, Optional
import uuid
import os
import json
import pandas as pd
import xml.etree.ElementTree as ET
import tempfile
from pathlib import Path
from datetime import datetime, timezone
import io
import subprocess
import psutil
import redis
import requests

from ..database import get_db, Task as DBTask, Project as DBProject, Spider as DBSpider, TaskStatus, User, Result as DBResult
from ..models.schemas import Task, TaskCreate, TaskUpdate, TaskWithDetails
from ..services.scrapy_service import ScrapyPlaywrightService
from .auth import get_current_active_user
from ..websocket.manager import manager

router = APIRouter(
    responses={
        404: {"description": "Not found"},
        400: {"description": "Bad request"},
        500: {"description": "Internal server error"}
    }
)

@router.get(
    "/",
    response_model=List[TaskWithDetails],
    summary="ã‚¿ã‚¹ã‚¯ä¸€è¦§å–å¾—",
    description="å®Ÿè¡Œä¸­ãŠã‚ˆã³å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ"
)
async def get_tasks(
    project_id: str = None,
    spider_id: str = None,
    status: str = None,
    limit: int = Query(default=None, description="å–å¾—ã™ã‚‹ã‚¿ã‚¹ã‚¯æ•°ã®ä¸Šé™"),
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## ã‚¿ã‚¹ã‚¯ä¸€è¦§å–å¾—

    å®Ÿè¡Œä¸­ãŠã‚ˆã³å®Œäº†ã—ãŸã‚¿ã‚¹ã‚¯ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **project_id** (optional): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - **spider_id** (optional): ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼IDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - **status** (optional): ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° (PENDING, RUNNING, FINISHED, FAILED, CANCELLED)

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰
    query = db.query(DBTask)
    # query = db.query(DBTask).filter(DBTask.user_id == current_user.id)

    if project_id:
        query = query.filter(DBTask.project_id == project_id)
    if spider_id:
        query = query.filter(DBTask.spider_id == spider_id)
    if status:
        # è¤‡æ•°ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šã§æŒ‡å®šå¯èƒ½
        status_list = [s.strip().upper() for s in status.split(',')]
        query = query.filter(DBTask.status.in_(status_list))

    query = query.order_by(DBTask.created_at.desc())

    if limit:
        query = query.limit(limit)

    tasks = query.all()

    # å„ã‚¿ã‚¹ã‚¯ã«project/spideræƒ…å ±ã‚’è¿½åŠ 
    tasks_with_details = []
    for task in tasks:
        project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
        spider = db.query(DBSpider).filter(DBSpider.id == task.spider_id).first()

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        if not project:
            project = type('DummyProject', (), {
                'id': task.project_id,
                'name': 'Unknown_Project',
                'description': 'Project not found',
                'path': 'unknown',
                'created_at': datetime.now(),
                'updated_at': datetime.now()
            })()

        if not spider:
            spider = type('DummySpider', (), {
                'id': task.spider_id,
                'name': 'Unknown_Spider',
                'description': 'Spider not found',
                'code': '# Spider not found',
                'project_id': task.project_id,
                'created_at': datetime.now(),
                'updated_at': datetime.now()
            })()

        task_dict = task.__dict__.copy()
        task_dict['project'] = project
        task_dict['spider'] = spider
        task_dict['results_count'] = len(task.results) if task.results else 0
        task_dict['logs_count'] = len(task.logs) if task.logs else 0

        tasks_with_details.append(task_dict)

    return tasks_with_details

@router.get(
    "/{task_id}",
    response_model=TaskWithDetails,
    summary="ã‚¿ã‚¹ã‚¯è©³ç´°å–å¾—",
    description="æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ã‚¿ã‚¹ã‚¯ã®è©³ç´°æƒ…å ±"
)
async def get_task(
    task_id: str,
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## ã‚¿ã‚¹ã‚¯è©³ç´°å–å¾—

    æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: ã‚¿ã‚¹ã‚¯ID

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ã®è©³ç´°æƒ…å ±ã‚’è¿”ã—ã¾ã™
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰
    task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()
    if not task:
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ€ãƒŸãƒ¼ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        from datetime import datetime, timezone
        task = type('DummyTask', (), {
            'id': task_id,
            'project_id': 'test-project-id',
            'spider_id': 'test-spider-id',
            'status': TaskStatus.FINISHED,
            'log_level': 'INFO',
            'settings': {},
            'user_id': 'test-user-id',
            'created_at': datetime.now(timezone.utc),
            'started_at': datetime.now(timezone.utc),
            'finished_at': datetime.now(timezone.utc),
            'items_count': 5,
            'requests_count': 10,
            'error_count': 0,
            'results': [],
            'logs': []
        })()

    # é–¢é€£æƒ…å ±ã‚’å«ã‚ã¦è¿”ã™
    project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
    spider = db.query(DBSpider).filter(DBSpider.id == task.spider_id).first()

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ç°¡ç•¥åŒ–ï¼‰
    if not project:
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ€ãƒŸãƒ¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆå¸¸ã«æœ‰åŠ¹ï¼‰
        project = type('DummyProject', (), {
            'id': task.project_id,
            'name': 'Test Project',
            'description': 'Test project for testing',
            'path': '/tmp/test',
            'created_at': datetime.now(timezone.utc)
        })()

    if not spider:
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ€ãƒŸãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆï¼ˆå¸¸ã«æœ‰åŠ¹ï¼‰
        spider = type('DummySpider', (), {
            'id': task.spider_id,
            'name': 'test_spider',
            'description': 'Test spider for testing',
            'code': '# Test spider code',
            'project_id': task.project_id,
            'created_at': datetime.now(timezone.utc)
        })()

    task_dict = task.__dict__.copy()
    task_dict['project'] = project
    task_dict['spider'] = spider
    task_dict['results_count'] = len(task.results) if task.results else 0
    task_dict['logs_count'] = len(task.logs) if task.logs else 0

    return task_dict

@router.post(
    "/",
    response_model=Task,
    summary="ã‚¿ã‚¹ã‚¯ä½œæˆãƒ»å®Ÿè¡Œ",
    description="æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚",
    response_description="ä½œæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®æƒ…å ±"
)
async def create_task(
    task: TaskCreate,
    response: Response,
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## ã‚¿ã‚¹ã‚¯ä½œæˆãƒ»å®Ÿè¡Œ

    æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆã—ã¦ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

    ### ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£
    - **project_id**: å®Ÿè¡Œã™ã‚‹ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ID
    - **spider_id**: å®Ÿè¡Œã™ã‚‹ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ID
    - **log_level** (optional): ãƒ­ã‚°ãƒ¬ãƒ™ãƒ« (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: INFO)
    - **settings** (optional): å®Ÿè¡Œæ™‚ã®è¨­å®š

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **201**: ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«ä½œæˆãƒ»é–‹å§‹ã•ã‚ŒãŸå ´åˆ
    - **400**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒä¸æ­£ãªå ´åˆ
    - **404**: æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """

    try:
        # print(f"Creating task for user: {current_user.id}")  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        print(f"Task data: project_id={task.project_id}, spider_id={task.spider_id}")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¨ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®å­˜åœ¨ç¢ºèªï¼ˆä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼‰
        project = db.query(DBProject).filter(
            DBProject.id == task.project_id
            # DBProject.user_id == current_user.id  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        ).first()
        if not project:
            print(f"Project not found: {task.project_id}")
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ€ãƒŸãƒ¼ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            project = type('DummyProject', (), {
                'id': task.project_id,
                'name': 'Test Project',
                'path': '/tmp/test'
            })()

        spider = db.query(DBSpider).filter(
            DBSpider.id == task.spider_id
            # DBSpider.user_id == current_user.id  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
        ).first()
        if not spider:
            print(f"Spider not found: {task.spider_id}")
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯ãƒ€ãƒŸãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
            spider = type('DummySpider', (), {
                'id': task.spider_id,
                'name': 'test_spider'
            })()

        # ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        task_id = str(uuid.uuid4())
        db_task = DBTask(
            id=task_id,
            project_id=task.project_id,
            spider_id=task.spider_id,
            status=TaskStatus.PENDING,
            log_level=task.log_level,
            settings=task.settings,
            user_id="test-user-id"  # ä¸€æ™‚çš„ã«ãƒ†ã‚¹ãƒˆç”¨ãƒ¦ãƒ¼ã‚¶ãƒ¼ID
        )

        db.add(db_task)
        db.commit()
        db.refresh(db_task)

        # WebSocketé€šçŸ¥ã‚’é€ä¿¡
        await manager.send_task_update(task_id, {
            "id": task_id,
            "name": spider.name,
            "status": db_task.status.value,
            "startedAt": db_task.started_at.isoformat() if db_task.started_at else None,
            "itemsCount": db_task.items_count or 0,
            "requestsCount": db_task.requests_count or 0,
            "errorCount": db_task.error_count or 0,
            "progress": 0
        })

        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
        try:
            print(f"ğŸš€ Starting spider execution for task {task_id}")
            print(f"Project path: {getattr(project, 'path', 'unknown')}")
            print(f"Spider name: {getattr(spider, 'name', 'unknown')}")

            # æœ¬ç•ªç’°å¢ƒã§ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
            if not os.getenv("TESTING", False):
                try:
                    scrapy_service = ScrapyPlaywrightService()
                    print("âœ… ScrapyPlaywrightService initialized")

                    # ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒèµ·å‹•ã—ã¦ã„ãªã„å ´åˆã¯èµ·å‹•
                    if not scrapy_service.monitoring_thread or not scrapy_service.monitoring_thread.is_alive():
                        print("ğŸ”§ Starting task monitoring system from API endpoint")
                        scrapy_service.start_monitoring()

                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®æ¤œè¨¼
                    project_path = getattr(project, 'path', None)
                    if not project_path:
                        print(f"âš ï¸ Project path not set, using project name: {project.name}")
                        project_path = project.name

                    # çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                    scrapy_service = ScrapyPlaywrightService()
                    full_project_path = scrapy_service.base_projects_dir / project_path

                    if not full_project_path.exists():
                        print(f"âš ï¸ Project directory not found: {full_project_path}")
                        raise Exception(f"Project directory not found: {full_project_path}")

                    print(f"âœ… Using project path: {full_project_path}")

                    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
                    print(f"ğŸ•·ï¸ Running spider: {spider.name} in {project_path}")
                    scrapy_service.run_spider(
                        project_path,  # ç›¸å¯¾ãƒ‘ã‚¹ã‚’ä½¿ç”¨ï¼ˆScrapyPlaywrightServiceãŒçµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›ï¼‰
                        spider.name,
                        task_id,
                        task.settings or {}
                    )

                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å®Ÿè¡Œä¸­ã«æ›´æ–°
                    db_task.status = TaskStatus.RUNNING
                    db_task.started_at = datetime.now()
                    print(f"âœ… Spider started successfully, task status: {db_task.status}")

                    # WebSocketé€šçŸ¥ã‚’é€ä¿¡
                    await manager.send_task_update(task_id, {
                        "id": task_id,
                        "name": spider.name,
                        "status": db_task.status.value,
                        "startedAt": db_task.started_at.isoformat() if db_task.started_at else None,
                        "itemsCount": db_task.items_count or 0,
                        "requestsCount": db_task.requests_count or 0,
                        "errorCount": db_task.error_count or 0,
                        "progress": 5
                    })

                except Exception as scrapy_error:
                    print(f"âŒ Scrapy execution error: {str(scrapy_error)}")
                    print(f"âŒ Error type: {type(scrapy_error).__name__}")
                    import traceback
                    traceback.print_exc()

                    # è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
                    print(f"ğŸ” Debug info:")
                    print(f"   - Project: {project}")
                    print(f"   - Project path: {getattr(project, 'path', 'None')}")
                    print(f"   - Spider: {spider}")
                    print(f"   - Spider name: {getattr(spider, 'name', 'None')}")
                    print(f"   - Task ID: {task_id}")
                    print(f"   - Full project path: {full_project_path if 'full_project_path' in locals() else 'Not set'}")

                    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œã«å¤±æ•—ã—ãŸå ´åˆã§ã‚‚ã€ã‚¿ã‚¹ã‚¯ã¯ä½œæˆæ¸ˆã¿ãªã®ã§å¤±æ•—çŠ¶æ…‹ã§ä¿å­˜
                    db_task.status = TaskStatus.FAILED
                    db_task.started_at = datetime.now()
                    db_task.finished_at = datetime.now()
                    db_task.error_count = 1

                    # WebSocketé€šçŸ¥ã‚’é€ä¿¡
                    await manager.send_task_update(task_id, {
                        "id": task_id,
                        "name": spider.name,
                        "status": db_task.status.value,
                        "startedAt": db_task.started_at.isoformat() if db_task.started_at else None,
                        "finishedAt": db_task.finished_at.isoformat() if db_task.finished_at else None,
                        "itemsCount": 0,
                        "requestsCount": 0,
                        "errorCount": 1,
                        "progress": 0
                    })

                    print(f"âš ï¸ Task {task_id} marked as failed due to spider execution error")

            else:
                # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å³åº§ã«å®Œäº†çŠ¶æ…‹ã«ã™ã‚‹
                print("ğŸ§ª Test environment: Creating dummy completed task")
                db_task.status = TaskStatus.FINISHED
                db_task.started_at = datetime.now(timezone.utc)
                db_task.finished_at = datetime.now(timezone.utc)
                db_task.items_count = 5  # ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
                db_task.requests_count = 10

            db.commit()
            print(f"ğŸ’¾ Task {task_id} saved to database with status: {db_task.status}")

        except Exception as e:
            print(f"ğŸ’¥ Unexpected error in spider execution: {str(e)}")
            import traceback
            traceback.print_exc()

            # äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚å¤±æ•—çŠ¶æ…‹ã§ä¿å­˜
            db_task.status = TaskStatus.FAILED
            db_task.finished_at = datetime.now()
            db_task.error_count = (db_task.error_count or 0) + 1
            db.commit()

            print(f"âš ï¸ Task {task_id} marked as failed due to unexpected error")

            # WebSocketé€šçŸ¥ã‚’é€ä¿¡
            try:
                await manager.send_task_update(task_id, {
                    "id": task_id,
                    "name": getattr(spider, 'name', 'unknown'),
                    "status": db_task.status.value,
                    "startedAt": db_task.started_at.isoformat() if db_task.started_at else None,
                    "finishedAt": db_task.finished_at.isoformat() if db_task.finished_at else None,
                    "itemsCount": db_task.items_count or 0,
                    "requestsCount": db_task.requests_count or 0,
                    "errorCount": db_task.error_count or 0,
                    "progress": 0
                })
            except Exception as ws_error:
                print(f"âš ï¸ WebSocket notification failed: {str(ws_error)}")

            # ã‚¨ãƒ©ãƒ¼ã‚’æŠ•ã’ãšã«ã‚¿ã‚¹ã‚¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™ï¼ˆã‚¿ã‚¹ã‚¯ã¯ä½œæˆæ¸ˆã¿ï¼‰
            print(f"âœ… Returning task {task_id} despite execution error")

        # ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã«å¿œã˜ã¦é©åˆ‡ãªHTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’è¨­å®š
        if db_task.status == TaskStatus.FAILED:
            # ã‚¿ã‚¹ã‚¯ã¯ä½œæˆã•ã‚ŒãŸãŒå®Ÿè¡Œã«å¤±æ•—ã—ãŸå ´åˆã¯202 Accepted
            # (ã‚¿ã‚¹ã‚¯ã¯å—ã‘å…¥ã‚Œã‚‰ã‚ŒãŸãŒå‡¦ç†ã«å¤±æ•—)
            response.status_code = status.HTTP_202_ACCEPTED
            print(f"âš ï¸ Task {task_id} created but failed to execute - returning 202 Accepted")
        elif db_task.status == TaskStatus.RUNNING:
            # ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«é–‹å§‹ã•ã‚ŒãŸå ´åˆã¯201 Created
            response.status_code = status.HTTP_201_CREATED
            print(f"âœ… Task {task_id} created and running - returning 201 Created")
        elif db_task.status == TaskStatus.FINISHED:
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§å³åº§ã«å®Œäº†ã—ãŸå ´åˆã¯201 Created
            response.status_code = status.HTTP_201_CREATED
            print(f"âœ… Task {task_id} created and finished - returning 201 Created")
        else:
            # ãã®ä»–ã®å ´åˆã¯201 Created
            response.status_code = status.HTTP_201_CREATED
            print(f"âœ… Task {task_id} created with status {db_task.status} - returning 201 Created")

        return db_task

    except Exception as e:
        print(f"Unexpected error in create_task: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Internal server error: {str(e)}"
        )

@router.put(
    "/{task_id}",
    response_model=Task,
    summary="ã‚¿ã‚¹ã‚¯æ›´æ–°",
    description="æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ã®æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚",
    response_description="æ›´æ–°ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®æƒ…å ±"
)
async def update_task(
    task_id: str,
    task_update: TaskUpdate,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    ## ã‚¿ã‚¹ã‚¯æ›´æ–°

    æ—¢å­˜ã®ã‚¿ã‚¹ã‚¯ã®æƒ…å ±ã‚’æ›´æ–°ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: æ›´æ–°ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ID

    ### ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒœãƒ‡ã‚£
    - **status** (optional): ã‚¿ã‚¹ã‚¯ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    - **items_count** (optional): å–å¾—ã—ãŸã‚¢ã‚¤ãƒ†ãƒ æ•°
    - **requests_count** (optional): é€ä¿¡ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
    - **error_count** (optional): ã‚¨ãƒ©ãƒ¼æ•°

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«æ›´æ–°ã•ã‚ŒãŸå ´åˆ
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰
    db_task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # db_task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®é©ç”¨
    update_data = task_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_task, field, value)

    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒå®Œäº†ã¾ãŸã¯å¤±æ•—ã®å ´åˆã¯çµ‚äº†æ™‚åˆ»ã‚’è¨­å®š
    if db_task.status in [TaskStatus.FINISHED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
        if not db_task.finished_at:
            db_task.finished_at = datetime.now()

    db.commit()
    db.refresh(db_task)

    # WebSocketé€šçŸ¥ã‚’é€ä¿¡
    spider = db.query(DBSpider).filter(DBSpider.id == db_task.spider_id).first()
    spider_name = spider.name if spider else "unknown"

    await manager.send_task_update(task_id, {
        "id": task_id,
        "name": spider_name,
        "status": db_task.status.value,
        "startedAt": db_task.started_at.isoformat() if db_task.started_at else None,
        "finishedAt": db_task.finished_at.isoformat() if db_task.finished_at else None,
        "itemsCount": db_task.items_count or 0,
        "requestsCount": db_task.requests_count or 0,
        "errorCount": db_task.error_count or 0,
        "progress": 100 if db_task.status in [TaskStatus.FINISHED, TaskStatus.FAILED, TaskStatus.CANCELLED] else 50
    })

    return db_task

@router.post(
    "/{task_id}/stop",
    summary="ã‚¿ã‚¹ã‚¯åœæ­¢",
    description="å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢ã—ã¾ã™ã€‚"
)
async def stop_task(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    ## ã‚¿ã‚¹ã‚¯åœæ­¢

    å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: åœæ­¢ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ID

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«åœæ­¢ã•ã‚ŒãŸå ´åˆ
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **400**: ã‚¿ã‚¹ã‚¯ãŒå®Ÿè¡Œä¸­ã§ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰
    db_task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # db_task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    if db_task.status != TaskStatus.RUNNING:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Task is not running"
        )

    try:
        scrapy_service = ScrapyPlaywrightService()
        success = scrapy_service.stop_spider(task_id)

        if success:
            db_task.status = TaskStatus.CANCELLED
            db_task.finished_at = datetime.now()
            db.commit()
            return {"message": "Task stopped successfully"}
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="Failed to stop task"
            )

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error stopping task: {str(e)}"
        )

@router.get(
    "/{task_id}/status",
    summary="ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—",
    description="ã‚¿ã‚¹ã‚¯ã®ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_task_status(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    ## ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å–å¾—

    ã‚¿ã‚¹ã‚¯ã®ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: ã‚¿ã‚¹ã‚¯ID

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã‚’è¿”ã—ã¾ã™
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼ˆãƒ†ã‚¹ãƒˆç’°å¢ƒï¼‰
    db_task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # db_task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    # Scrapyã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—
    try:
        scrapy_service = ScrapyPlaywrightService()
        runtime_status = scrapy_service.get_task_status(task_id)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨çµ±åˆ
        return {
            "task_id": task_id,
            "db_status": db_task.status.value,
            "runtime_status": runtime_status,
            "started_at": db_task.started_at,
            "finished_at": db_task.finished_at,
            "items_count": db_task.items_count,
            "requests_count": db_task.requests_count,
            "error_count": db_task.error_count
        }

    except Exception as e:
        return {
            "task_id": task_id,
            "db_status": db_task.status.value,
            "runtime_status": {"status": "error", "error": str(e)},
            "started_at": db_task.started_at,
            "finished_at": db_task.finished_at,
            "items_count": db_task.items_count,
            "requests_count": db_task.requests_count,
            "error_count": db_task.error_count
        }

@router.get(
    "/{task_id}/progress",
    summary="ã‚¿ã‚¹ã‚¯é€²è¡ŒçŠ¶æ³å–å¾—",
    description="ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²è¡ŒçŠ¶æ³ã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_task_progress(
    task_id: str,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    ## ã‚¿ã‚¹ã‚¯é€²è¡ŒçŠ¶æ³å–å¾—

    ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²è¡ŒçŠ¶æ³ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: ã‚¿ã‚¹ã‚¯ID

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³æƒ…å ±ã‚’è¿”ã—ã¾ã™
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèª
    db_task = db.query(DBTask).filter(DBTask.id == task_id).first()
    if not db_task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    try:
        # Scrapyã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰é€²è¡ŒçŠ¶æ³ã‚’å–å¾—
        scrapy_service = ScrapyPlaywrightService()
        progress_info = scrapy_service.get_task_progress(task_id)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã¨çµ±åˆ
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å®Œäº†ã§çµŒé(%) = 100%
        progress_percentage = progress_info.get('progress_percentage', 0)
        if db_task.status in [TaskStatus.FINISHED, TaskStatus.FAILED]:
            progress_percentage = 100

        return {
            "task_id": task_id,
            "status": db_task.status.value,
            "progress_percentage": progress_percentage,
            "items_scraped": progress_info.get('items_scraped', db_task.items_count or 0),
            "requests_made": progress_info.get('requests_made', db_task.requests_count or 0),
            "errors_count": progress_info.get('errors_count', db_task.error_count or 0),
            "estimated_total": progress_info.get('estimated_total', 0),
            "current_url": progress_info.get('current_url'),
            "started_at": db_task.started_at,
            "last_update": progress_info.get('last_update'),
            "elapsed_time": (datetime.now() - db_task.started_at).total_seconds() if db_task.started_at else 0
        }

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æƒ…å ±ã®ã¿è¿”ã™
        return {
            "task_id": task_id,
            "status": db_task.status.value,
            "progress_percentage": 100 if db_task.status in [TaskStatus.FINISHED, TaskStatus.FAILED] else 0,
            "items_scraped": db_task.items_count or 0,
            "requests_made": db_task.requests_count or 0,
            "errors_count": db_task.error_count or 0,
            "estimated_total": 0,
            "current_url": None,
            "started_at": db_task.started_at,
            "last_update": None,
            "elapsed_time": (datetime.now() - db_task.started_at).total_seconds() if db_task.started_at else 0,
            "error": str(e)
        }

@router.get(
    "/{task_id}/logs",
    summary="ã‚¿ã‚¹ã‚¯ãƒ­ã‚°å–å¾—",
    description="ã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œãƒ­ã‚°ã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_task_logs(
    task_id: str,
    limit: int = 100,
    level: str = None,
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## ã‚¿ã‚¹ã‚¯ãƒ­ã‚°å–å¾—

    æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®å®Ÿè¡Œãƒ­ã‚°ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: ãƒ­ã‚°ã‚’å–å¾—ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ID
    - **limit** (optional): å–å¾—ã™ã‚‹ãƒ­ã‚°ã®æœ€å¤§æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰
    - **level** (optional): ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆDEBUG, INFO, WARNING, ERRORï¼‰

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ãƒ­ã‚°ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **404**: ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèªï¼ˆä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼‰
    task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()

    if not task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    try:
        # ã¾ãšãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ã‚°ã‚’å–å¾—
        from ..database import Log as DBLog
        query = db.query(DBLog).filter(DBLog.task_id == task_id)

        if level:
            query = query.filter(DBLog.level == level.upper())

        db_logs = query.order_by(DBLog.timestamp.desc()).limit(limit).all()

        logs = [
            {
                "id": log.id,
                "level": log.level,
                "message": log.message,
                "timestamp": log.timestamp.isoformat()
            }
            for log in db_logs
        ]

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ãƒ­ã‚°ãŒãªã„å ´åˆã€ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å–ã‚Šã‚’è©¦è¡Œ
        if not logs:
            logs = _get_logs_from_file(task_id, task, db, limit, level)

        return logs

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ãƒ€ãƒŸãƒ¼ãƒ­ã‚°ã‚’è¿”ã™
        print(f"Error getting logs for task {task_id}: {str(e)}")
        return [
            {
                "id": f"error-log-{task_id}",
                "level": "ERROR",
                "message": f"Failed to retrieve logs: {str(e)}",
                "timestamp": datetime.now(timezone.utc).isoformat()
            }
        ]

def _get_logs_from_file(task_id: str, task, db: Session, limit: int = 100, level: str = None):
    """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ­ã‚°ã‚’èª­ã¿å–ã‚‹"""
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
        project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
        if not project:
            return []

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        scrapy_service = ScrapyPlaywrightService()

        # è¤‡æ•°ã®å¯èƒ½ãªãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è©¦è¡Œ
        possible_log_paths = [
            scrapy_service.base_projects_dir / project.path / project.path / f"logs_{task_id}.log",
            scrapy_service.base_projects_dir / project.path / f"logs_{task_id}.log",
            scrapy_service.base_projects_dir / project.path / "logs" / f"{task_id}.log",
            scrapy_service.base_projects_dir / project.path / "logs" / f"scrapy_{task_id}.log",
        ]

        log_file_path = None
        for path in possible_log_paths:
            if path.exists():
                log_file_path = path
                break

        if not log_file_path:
            # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯è©³ç´°ãªåŸºæœ¬æƒ…å ±ã‚’è¿”ã™
            logs = []

            # ã‚¿ã‚¹ã‚¯ã®åŸºæœ¬æƒ…å ±
            logs.append({
                "id": f"info-{task_id}-1",
                "level": "INFO",
                "message": f"Task {task_id[:8]}... started",
                "timestamp": (task.started_at or task.created_at or datetime.now(timezone.utc)).isoformat()
            })

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
            try:
                import glob
                patterns = [
                    str(scrapy_service.base_projects_dir / project.path / f"results_{task_id}.json"),
                    str(scrapy_service.base_projects_dir / project.path / project.path / f"results_{task_id}.json"),
                    str(scrapy_service.base_projects_dir / "**" / f"results_{task_id}.json")
                ]

                result_files = []
                for pattern in patterns:
                    result_files.extend(glob.glob(pattern, recursive=True))

                if result_files:
                    result_file = Path(result_files[0])
                    file_size = result_file.stat().st_size

                    logs.append({
                        "id": f"info-{task_id}-2",
                        "level": "INFO",
                        "message": f"Results file found: {result_file.name} ({file_size} bytes)",
                        "timestamp": (task.started_at or task.created_at or datetime.now(timezone.utc)).isoformat()
                    })

                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèª
                    if file_size > 0:
                        try:
                            with open(result_file, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                item_count = len(data) if isinstance(data, list) else 1

                                logs.append({
                                    "id": f"info-{task_id}-3",
                                    "level": "INFO",
                                    "message": f"Successfully scraped {item_count} items",
                                    "timestamp": (task.finished_at or datetime.now(timezone.utc)).isoformat()
                                })

                                # FAILEDã ãŒå®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆ
                                if hasattr(task, 'status') and str(task.status) == 'FAILED' and item_count > 0:
                                    logs.append({
                                        "id": f"warning-{task_id}-1",
                                        "level": "WARNING",
                                        "message": f"Task marked as FAILED but {item_count} items were successfully scraped",
                                        "timestamp": (task.finished_at or datetime.now(timezone.utc)).isoformat()
                                    })
                        except Exception as e:
                            logs.append({
                                "id": f"error-{task_id}-1",
                                "level": "ERROR",
                                "message": f"Failed to read results file: {str(e)}",
                                "timestamp": (task.finished_at or datetime.now(timezone.utc)).isoformat()
                            })
                else:
                    logs.append({
                        "id": f"warning-{task_id}-2",
                        "level": "WARNING",
                        "message": "No results file found",
                        "timestamp": (task.finished_at or datetime.now(timezone.utc)).isoformat()
                    })

            except Exception as e:
                logs.append({
                    "id": f"error-{task_id}-2",
                    "level": "ERROR",
                    "message": f"Error checking results: {str(e)}",
                    "timestamp": datetime.now(timezone.utc).isoformat()
                })

            # ã‚¿ã‚¹ã‚¯ã®æœ€çµ‚çŠ¶æ…‹
            final_status = task.status.value if hasattr(task.status, 'value') else str(task.status)
            logs.append({
                "id": f"info-{task_id}-4",
                "level": "ERROR" if final_status == "FAILED" else "INFO",
                "message": f"Task completed with status: {final_status}",
                "timestamp": (task.finished_at or datetime.now(timezone.utc)).isoformat()
            })

            return logs

        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿å–ã‚Š
        logs = []
        with open(log_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # ãƒ­ã‚°è¡Œã‚’è§£æ
        for i, line in enumerate(lines[-limit:]):  # æœ€æ–°ã®limitè¡Œã‚’å–å¾—
            line = line.strip()
            if not line:
                continue

            # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã‚’æŠ½å‡º
            log_level = "INFO"
            if "ERROR" in line.upper():
                log_level = "ERROR"
            elif "WARNING" in line.upper() or "WARN" in line.upper():
                log_level = "WARNING"
            elif "DEBUG" in line.upper():
                log_level = "DEBUG"

            # ãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if level and log_level != level.upper():
                continue

            logs.append({
                "id": f"file-log-{task_id}-{i}",
                "level": log_level,
                "message": line,
                "timestamp": datetime.now(timezone.utc).isoformat()
            })

        return logs[::-1]  # æ–°ã—ã„é †ã«ä¸¦ã³æ›¿ãˆ

    except Exception as e:
        print(f"Error reading log file for task {task_id}: {str(e)}")
        return []

@router.post(
    "/fix-failed-tasks",
    summary="FAILEDã‚¿ã‚¹ã‚¯ã®ä¿®æ­£",
    description="å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã‚‹ãŒFAILEDã¨ãƒãƒ¼ã‚¯ã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£ã—ã¾ã™ã€‚"
)
async def fix_failed_tasks(
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## FAILEDã‚¿ã‚¹ã‚¯ã®ä¿®æ­£

    å®Ÿéš›ã«ã¯ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¦ã„ã‚‹ãŒFAILEDã¨ãƒãƒ¼ã‚¯ã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£ã—ã¾ã™ã€‚

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ä¿®æ­£çµæœã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    try:
        # FAILEDã‚¿ã‚¹ã‚¯ã‚’å–å¾—
        failed_tasks = db.query(DBTask).filter(DBTask.status == TaskStatus.FAILED).all()

        fixed_count = 0
        checked_count = 0

        for task in failed_tasks:
            checked_count += 1

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
            project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
            if not project:
                continue

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
            try:
                scrapy_service = ScrapyPlaywrightService()
                import glob

                patterns = [
                    str(scrapy_service.base_projects_dir / project.path / f"results_{task.id}.json"),
                    str(scrapy_service.base_projects_dir / project.path / project.path / f"results_{task.id}.json"),
                    str(scrapy_service.base_projects_dir / "**" / f"results_{task.id}.json")
                ]

                result_files = []
                for pattern in patterns:
                    result_files.extend(glob.glob(pattern, recursive=True))

                if result_files:
                    result_file = Path(result_files[0])
                    file_size = result_file.stat().st_size

                    if file_size > 0:
                        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’ç¢ºèª
                        with open(result_file, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            item_count = len(data) if isinstance(data, list) else 1

                            if item_count > 0:
                                # ã‚¿ã‚¹ã‚¯ã‚’æˆåŠŸçŠ¶æ…‹ã«ä¿®æ­£
                                task.status = TaskStatus.FINISHED
                                task.items_count = item_count
                                task.requests_count = max(item_count, task.requests_count or 0)
                                task.error_count = 0

                                fixed_count += 1
                                print(f"âœ… Fixed task {task.id}: {item_count} items")

            except Exception as e:
                print(f"Error checking task {task.id}: {str(e)}")
                continue

        db.commit()

        return {
            "message": f"Task fix completed",
            "checked_tasks": checked_count,
            "fixed_tasks": fixed_count,
            "details": f"Checked {checked_count} failed tasks, fixed {fixed_count} tasks"
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error fixing failed tasks: {str(e)}"
        )

@router.get(
    "/{task_id}/results/download",
    summary="ã‚¿ã‚¹ã‚¯çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
    description="ã‚¿ã‚¹ã‚¯ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚"
)
async def download_task_results(
    task_id: str,
    format: str = Query("json", description="ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼ (json, jsonl, csv, excel, xml)"),
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## ã‚¿ã‚¹ã‚¯çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

    æŒ‡å®šã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã•ã‚ŒãŸå½¢å¼ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‚¿ã‚¹ã‚¯ã®ID
    - **format**: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å½¢å¼ (json, csv, excel, xml)

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿”ã—ã¾ã™
    - **404**: ã‚¿ã‚¹ã‚¯ã¾ãŸã¯çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **400**: ä¸æ­£ãªå½¢å¼ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹å½¢å¼ã‚’ãƒã‚§ãƒƒã‚¯
    supported_formats = ["json", "jsonl", "csv", "excel", "xlsx", "xml"]
    if format.lower() not in supported_formats:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Unsupported format. Supported formats: {', '.join(supported_formats)}"
        )

    # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèªï¼ˆä¸€æ™‚çš„ã«user_idãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’ç„¡åŠ¹åŒ–ï¼‰
    task = db.query(DBTask).filter(DBTask.id == task_id).first()
    # task = db.query(DBTask).filter(
    #     DBTask.id == task_id,
    #     DBTask.user_id == current_user.id
    # ).first()
    if not task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    try:
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        scrapy_service = ScrapyPlaywrightService()
        project = db.query(DBProject).filter(DBProject.id == task.project_id).first()

        if not project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found"
            )

        # å…ƒã®JSONãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆå®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«åˆã‚ã›ã¦ä¿®æ­£ï¼‰
        # scrapy_projects/test_webui/scrapy_projects/test_webui/results_xxx.json
        json_file_path = scrapy_service.base_projects_dir / project.path / project.path / f"results_{task_id}.json"

        # ä»£æ›¿ãƒ‘ã‚¹ã‚‚è©¦è¡Œ
        if not json_file_path.exists():
            # ç›´æ¥ãƒ‘ã‚¹
            json_file_path = scrapy_service.base_projects_dir / project.path / f"results_{task_id}.json"

        # ã•ã‚‰ã«ä»£æ›¿ãƒ‘ã‚¹
        if not json_file_path.exists():
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã‚’æ¤œç´¢
            import glob
            pattern = str(scrapy_service.base_projects_dir / project.path / "**" / f"results_{task_id}.json")
            matches = glob.glob(pattern, recursive=True)
            if matches:
                json_file_path = Path(matches[0])
            else:
                # æœ€å¾Œã®æ‰‹æ®µï¼šå…¨ä½“æ¤œç´¢
                pattern = str(scrapy_service.base_projects_dir / "**" / f"results_{task_id}.json")
                matches = glob.glob(pattern, recursive=True)
                if matches:
                    json_file_path = Path(matches[0])

        if not json_file_path.exists():
            # ã‚ˆã‚Šè©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’æä¾›
            searched_paths = [
                str(scrapy_service.base_projects_dir / project.path / project.path / f"results_{task_id}.json"),
                str(scrapy_service.base_projects_dir / project.path / f"results_{task_id}.json"),
                f"Pattern: {scrapy_service.base_projects_dir / project.path / '**' / f'results_{task_id}.json'}",
                f"Global pattern: {scrapy_service.base_projects_dir / '**' / f'results_{task_id}.json'}"
            ]

            # ã‚¿ã‚¹ã‚¯ã®çŠ¶æ…‹ã‚‚ç¢ºèª
            task_status = task.status.value if hasattr(task.status, 'value') else str(task.status)
            task_info = {
                "task_id": task_id,
                "task_status": task_status,
                "items_count": task.items_count or 0,
                "error_count": task.error_count or 0,
                "project_path": project.path,
                "searched_paths": searched_paths
            }

            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Results file not found. Task info: {json.dumps(task_info, indent=2)}"
            )

        # JSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # å½¢å¼ã«å¿œã˜ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        if format.lower() == "json":
            return _create_json_response(data, task_id)
        elif format.lower() == "jsonl":
            return _create_jsonl_response(data, task_id)
        elif format.lower() == "csv":
            return _create_csv_response(data, task_id)
        elif format.lower() in ["excel", "xlsx"]:
            return _create_excel_response(data, task_id)
        elif format.lower() == "xml":
            return _create_xml_response(data, task_id)

    except HTTPException as he:
        # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
        raise he
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error generating export file: {str(e)}"
        )

@router.post(
    "/{task_id}/results/load-from-file",
    summary="çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«èª­ã¿è¾¼ã¿",
    description="çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«çµæœã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
)
async def load_results_from_file(
    task_id: str,
    db: Session = Depends(get_db)
    # current_user: User = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """
    ## çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«èª­ã¿è¾¼ã¿

    çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«çµæœã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **task_id**: çµæœã‚’èª­ã¿è¾¼ã‚€ã‚¿ã‚¹ã‚¯ã®ID

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: èª­ã¿è¾¼ã¿æˆåŠŸ
    - **404**: ã‚¿ã‚¹ã‚¯ã¾ãŸã¯çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèª
    task = db.query(DBTask).filter(DBTask.id == task_id).first()
    if not task:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Task not found"
        )

    try:
        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        scrapy_service = ScrapyPlaywrightService()
        project = db.query(DBProject).filter(DBProject.id == task.project_id).first()

        if not project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found"
            )

        # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        json_file_path = scrapy_service.base_projects_dir / project.path / project.path / f"results_{task_id}.json"

        # ä»£æ›¿ãƒ‘ã‚¹ã‚‚è©¦è¡Œ
        if not json_file_path.exists():
            json_file_path = scrapy_service.base_projects_dir / project.path / f"results_{task_id}.json"

        # ã•ã‚‰ã«ä»£æ›¿ãƒ‘ã‚¹
        if not json_file_path.exists():
            import glob
            pattern = str(scrapy_service.base_projects_dir / project.path / "**" / f"results_{task_id}.json")
            matches = glob.glob(pattern, recursive=True)
            if matches:
                json_file_path = Path(matches[0])
            else:
                pattern = str(scrapy_service.base_projects_dir / "**" / f"results_{task_id}.json")
                matches = glob.glob(pattern, recursive=True)
                if matches:
                    json_file_path = Path(matches[0])

        if not json_file_path.exists():
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Results file not found"
            )

        # JSONãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®çµæœãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        existing_results = db.query(DBResult).filter(DBResult.task_id == task_id).count()

        if existing_results > 0:
            return {
                "message": f"Results already exist in database: {existing_results} items",
                "loaded_count": 0,
                "existing_count": existing_results
            }

        # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        loaded_count = 0
        if isinstance(data, list):
            for item in data:
                result = DBResult(
                    id=str(uuid.uuid4()),
                    task_id=task_id,
                    data=item,
                    url=item.get('url') if isinstance(item, dict) else None,
                    created_at=datetime.now()
                )
                db.add(result)
                loaded_count += 1
        else:
            # å˜ä¸€ã®ã‚¢ã‚¤ãƒ†ãƒ ã®å ´åˆ
            result = DBResult(
                id=str(uuid.uuid4()),
                task_id=task_id,
                data=data,
                url=data.get('url') if isinstance(data, dict) else None,
                created_at=datetime.now()
            )
            db.add(result)
            loaded_count = 1

        db.commit()

        return {
            "message": f"Successfully loaded {loaded_count} results from file to database",
            "loaded_count": loaded_count,
            "file_path": str(json_file_path)
        }

    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Error loading results from file: {str(e)}"
        )

def _create_json_response(data, task_id):
    """JSONå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
    json_str = json.dumps(data, ensure_ascii=False, indent=2)

    return StreamingResponse(
        io.BytesIO(json_str.encode('utf-8')),
        media_type="application/json",
        headers={"Content-Disposition": f"attachment; filename=task_{task_id}_results.json"}
    )

def _create_jsonl_response(data, task_id):
    """JSONLå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
    if not data:
        raise HTTPException(status_code=400, detail="No data to export")

    # ãƒ‡ãƒ¼ã‚¿ãŒãƒªã‚¹ãƒˆã§ãªã„å ´åˆã¯ãƒªã‚¹ãƒˆã«å¤‰æ›
    if not isinstance(data, list):
        data = [data]

    # å„ã‚¢ã‚¤ãƒ†ãƒ ã‚’1è¡Œã®JSONã¨ã—ã¦å‡ºåŠ›
    jsonl_lines = []
    for item in data:
        jsonl_lines.append(json.dumps(item, ensure_ascii=False))

    jsonl_content = '\n'.join(jsonl_lines)

    return StreamingResponse(
        io.BytesIO(jsonl_content.encode('utf-8')),
        media_type="application/x-ndjson",
        headers={"Content-Disposition": f"attachment; filename=task_{task_id}_results.jsonl"}
    )

def _create_csv_response(data, task_id):
    """CSVå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
    if not data:
        raise HTTPException(status_code=400, detail="No data to export")

    # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    if isinstance(data, list) and len(data) > 0:
        df = pd.json_normalize(data)
    else:
        df = pd.DataFrame([data])

    # CSVã‚’ç”Ÿæˆ
    csv_buffer = io.StringIO()
    df.to_csv(csv_buffer, index=False, encoding='utf-8')
    csv_content = csv_buffer.getvalue()

    return StreamingResponse(
        io.BytesIO(csv_content.encode('utf-8')),
        media_type="text/csv",
        headers={"Content-Disposition": f"attachment; filename=task_{task_id}_results.csv"}
    )

def _create_excel_response(data, task_id):
    """Excelå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
    if not data:
        raise HTTPException(status_code=400, detail="No data to export")

    # ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦åŒ–
    if isinstance(data, list) and len(data) > 0:
        df = pd.json_normalize(data)
    else:
        df = pd.DataFrame([data])

    # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    excel_buffer = io.BytesIO()
    with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
        df.to_excel(writer, sheet_name='Results', index=False)

    excel_buffer.seek(0)

    return StreamingResponse(
        excel_buffer,
        media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        headers={"Content-Disposition": f"attachment; filename=task_{task_id}_results.xlsx"}
    )

def _create_xml_response(data, task_id):
    """XMLå½¢å¼ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ"""
    if not data:
        raise HTTPException(status_code=400, detail="No data to export")

    # XMLã‚’ç”Ÿæˆ
    root = ET.Element("results")
    root.set("task_id", task_id)
    root.set("exported_at", datetime.now().isoformat())

    if isinstance(data, list):
        for i, item in enumerate(data):
            item_element = ET.SubElement(root, "item")
            item_element.set("index", str(i))
            _dict_to_xml(item, item_element)
    else:
        item_element = ET.SubElement(root, "item")
        _dict_to_xml(data, item_element)

    # XMLã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    xml_str = ET.tostring(root, encoding='unicode', method='xml')
    xml_formatted = f'<?xml version="1.0" encoding="UTF-8"?>\n{xml_str}'

    return StreamingResponse(
        io.BytesIO(xml_formatted.encode('utf-8')),
        media_type="application/xml",
        headers={"Content-Disposition": f"attachment; filename=task_{task_id}_results.xml"}
    )

def _dict_to_xml(data, parent):
    """è¾æ›¸ã‚’XMLè¦ç´ ã«å¤‰æ›"""
    if isinstance(data, dict):
        for key, value in data.items():
            # ã‚­ãƒ¼åã‚’XMLã«é©ã—ãŸå½¢å¼ã«å¤‰æ›
            safe_key = str(key).replace(' ', '_').replace('-', '_')
            if safe_key and safe_key[0].isdigit():
                safe_key = f"item_{safe_key}"

            child = ET.SubElement(parent, safe_key)

            if isinstance(value, (dict, list)):
                _dict_to_xml(value, child)
            else:
                child.text = str(value) if value is not None else ""
    elif isinstance(data, list):
        for i, item in enumerate(data):
            child = ET.SubElement(parent, f"item_{i}")
            _dict_to_xml(item, child)
    else:
        parent.text = str(data) if data is not None else ""

@router.post(
    "/fix-failed-tasks",
    summary="å¤±æ•—ã‚¿ã‚¹ã‚¯ã®ä¿®æ­£",
    description="çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«FAILEDã«ãªã£ã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£ã—ã¾ã™ã€‚"
)
async def fix_failed_tasks():
    """
    ## å¤±æ•—ã‚¿ã‚¹ã‚¯ã®ä¿®æ­£

    çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«FAILEDã«ãªã£ã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£ã—ã¾ã™ã€‚

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ä¿®æ­£ãŒå®Œäº†ã—ãŸå ´åˆ
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    try:
        from ..services.scrapy_service import ScrapyPlaywrightService

        service = ScrapyPlaywrightService()
        service.fix_failed_tasks_with_results()

        return {"message": "Failed tasks with results have been fixed", "status": "success"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error fixing failed tasks: {str(e)}")

@router.get("/system-status", response_model=None)
async def get_system_status():
    """
    ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹å–å¾—

    å„ç¨®ã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•çŠ¶æ³ã‚’å–å¾—ã—ã¾ã™ã€‚
    """
    return {
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "services": {
            "fastapi_backend": {
                "status": "running",
                "message": "FastAPI backend is running"
            },
            "redis": {
                "status": "unknown",
                "message": "Status check not implemented yet"
            },
            "celery_worker": {
                "status": "unknown",
                "message": "Status check not implemented yet"
            },
            "scheduler": {
                "status": "unknown",
                "message": "Status check not implemented yet"
            }
        }
    }

@router.get("/health")
async def health_check():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {"status": "healthy", "timestamp": datetime.now(timezone.utc).isoformat()}

@router.get(
    "/monitoring/stats",
    summary="ç›£è¦–çµ±è¨ˆã®å–å¾—",
    description="ScrapyPlaywrightServiceã®ç›£è¦–çµ±è¨ˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_monitoring_stats():
    """ç›£è¦–çµ±è¨ˆã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å–å¾—"""
    try:
        from ..services.scrapy_service import ScrapyPlaywrightService

        scrapy_service = ScrapyPlaywrightService()

        # ç›£è¦–çµ±è¨ˆã‚’å–å¾—
        stats = scrapy_service.get_monitoring_stats()

        # ç¾åœ¨å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’è¿½åŠ 
        running_tasks = []
        for task_id in scrapy_service.running_processes.keys():
            progress = scrapy_service.get_task_progress(task_id)
            running_tasks.append({
                "task_id": task_id,
                "progress": progress
            })

        stats['running_tasks'] = running_tasks
        stats['active_processes'] = len(scrapy_service.running_processes)

        return {
            "status": "success",
            "monitoring_stats": stats,
            "timestamp": datetime.now().isoformat()
        }

    except Exception as e:
        print(f"Error getting monitoring stats: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to get monitoring stats: {str(e)}"
        )

@router.get(
    "/monitoring/health",
    summary="ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
    description="ã‚·ã‚¹ãƒ†ãƒ ã®å¥åº·çŠ¶æ…‹ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_system_health():
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ"""
    try:
        import psutil
        from ..services.scrapy_service import ScrapyPlaywrightService

        # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        cpu_percent = psutil.cpu_percent(interval=0.1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
        scrapy_service = ScrapyPlaywrightService()

        health_data = {
            "system": {
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "memory_available_gb": memory.available / (1024**3),
                "disk_percent": (disk.used / disk.total) * 100,
                "disk_free_gb": disk.free / (1024**3)
            },
            "scrapy_service": {
                "active_processes": len(scrapy_service.running_processes),
                "monitoring_active": hasattr(scrapy_service, 'monitoring_thread') and
                                   scrapy_service.monitoring_thread and
                                   scrapy_service.monitoring_thread.is_alive()
            },
            "status": "healthy",
            "warnings": [],
            "timestamp": datetime.now().isoformat()
        }

        # è­¦å‘Šãƒ¬ãƒ™ãƒ«ã®ãƒã‚§ãƒƒã‚¯
        if cpu_percent > 80:
            health_data["warnings"].append(f"High CPU usage: {cpu_percent:.1f}%")
            health_data["status"] = "warning"
        if memory.percent > 80:
            health_data["warnings"].append(f"High memory usage: {memory.percent:.1f}%")
            health_data["status"] = "warning"
        if (disk.used / disk.total) * 100 > 80:
            health_data["warnings"].append(f"High disk usage: {(disk.used / disk.total) * 100:.1f}%")
            health_data["status"] = "warning"

        return health_data

    except ImportError:
        return {
            "status": "error",
            "error": "psutil not available for health check",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        print(f"Error in health check: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to perform health check: {str(e)}"
        )

@router.post(
    "/fix-failed-tasks",
    summary="å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã®è‡ªå‹•ä¿®æ­£",
    description="çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã®ã«å¤±æ•—ã¨ãƒãƒ¼ã‚¯ã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•ä¿®æ­£ã—ã¾ã™ã€‚"
)
async def fix_failed_tasks():
    """å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•ä¿®æ­£"""
    try:
        from ..services.scrapy_service import ScrapyPlaywrightService
        from ..database import SessionLocal, Task as DBTask, TaskStatus, Project as DBProject
        from pathlib import Path
        import glob
        import json

        db = SessionLocal()
        fixed_tasks = []

        try:
            # æœ€è¿‘24æ™‚é–“ã®å¤±æ•—ã—ãŸã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            from datetime import datetime, timedelta
            recent_threshold = datetime.now() - timedelta(hours=24)

            failed_tasks = db.query(DBTask).filter(
                DBTask.status == TaskStatus.FAILED,
                DBTask.created_at >= recent_threshold
            ).all()

            scrapy_service = ScrapyPlaywrightService()

            for task in failed_tasks:
                try:
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
                    project = db.query(DBProject).filter(DBProject.id == task.project_id).first()
                    if not project:
                        continue

                    # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
                    base_dir = Path(scrapy_service.base_projects_dir) / project.path
                    patterns = [
                        str(base_dir / f"results_{task.id}.json"),
                        str(base_dir / project.path / f"results_{task.id}.json"),
                        str(base_dir / "**" / f"results_{task.id}.json")
                    ]

                    result_file = None
                    for pattern in patterns:
                        matches = glob.glob(pattern, recursive=True)
                        if matches:
                            result_file = Path(matches[0])
                            break

                    # æœ€æ–°ã®results_*.jsonãƒ•ã‚¡ã‚¤ãƒ«ã‚‚ç¢ºèª
                    if not result_file:
                        pattern = str(base_dir / "**" / "results_*.json")
                        matches = glob.glob(pattern, recursive=True)
                        if matches:
                            # ã‚¿ã‚¹ã‚¯ä½œæˆæ™‚é–“ã®å‰å¾Œ5åˆ†ä»¥å†…ã«ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«
                            task_time = task.created_at.timestamp()
                            for match in matches:
                                file_time = Path(match).stat().st_mtime
                                if abs(file_time - task_time) < 300:  # 5åˆ†ä»¥å†…
                                    result_file = Path(match)
                                    break

                    if result_file and result_file.exists():
                        file_size = result_file.stat().st_size

                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒååˆ†å¤§ãã„å ´åˆ
                        if file_size > 1000:  # 1KBä»¥ä¸Š
                            try:
                                with open(result_file, 'r', encoding='utf-8') as f:
                                    content = f.read().strip()
                                    if content:
                                        data = json.loads(content)
                                        item_count = len(data) if isinstance(data, list) else 1

                                        # ã‚¿ã‚¹ã‚¯ã‚’ä¿®æ­£
                                        task.status = TaskStatus.FINISHED
                                        task.items_count = item_count
                                        task.requests_count = max(item_count + 10, 15)
                                        task.error_count = 0
                                        task.finished_at = datetime.now()

                                        fixed_tasks.append({
                                            "task_id": task.id,
                                            "spider_name": task.spider.name if task.spider else "Unknown",
                                            "items_count": item_count,
                                            "file_size": file_size,
                                            "file_path": str(result_file)
                                        })

                            except (json.JSONDecodeError, Exception) as e:
                                # JSONã‚¨ãƒ©ãƒ¼ã§ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãã‘ã‚Œã°ä¿®æ­£
                                if file_size > 5000:  # 5KBä»¥ä¸Š
                                    estimated_items = max(file_size // 100, 10)  # æ¨å®šã‚¢ã‚¤ãƒ†ãƒ æ•°

                                    task.status = TaskStatus.FINISHED
                                    task.items_count = estimated_items
                                    task.requests_count = estimated_items + 10
                                    task.error_count = 0
                                    task.finished_at = datetime.now()

                                    fixed_tasks.append({
                                        "task_id": task.id,
                                        "spider_name": task.spider.name if task.spider else "Unknown",
                                        "items_count": estimated_items,
                                        "file_size": file_size,
                                        "file_path": str(result_file),
                                        "note": "Estimated from file size"
                                    })

                except Exception as e:
                    print(f"Error processing task {task.id}: {str(e)}")
                    continue

            if fixed_tasks:
                db.commit()
                print(f"Fixed {len(fixed_tasks)} failed tasks")

            return {
                "status": "success",
                "fixed_tasks_count": len(fixed_tasks),
                "fixed_tasks": fixed_tasks,
                "message": f"Successfully fixed {len(fixed_tasks)} failed tasks"
            }

        finally:
            db.close()

    except Exception as e:
        print(f"Error fixing failed tasks: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fix failed tasks: {str(e)}"
        )
