"""
ScrapyUI Rich Progress Extension

Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«richãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸç¾ã—ã„é€²æ—ãƒãƒ¼ã‚’è¿½åŠ ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Optional, Dict, Any
from scrapy import signals
from scrapy.crawler import Crawler
from scrapy.spiders import Spider
from scrapy.http import Request, Response
from scrapy.exceptions import NotConfigured
import pytz
from datetime import datetime

# å‹•çš„ãƒ‘ã‚¹è¿½åŠ ï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰
def _setup_dynamic_imports():
    """Rich Progress Extensionç”¨ã®å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¨­å®š"""
    try:
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ãƒ‘ã‚¹ã‚’å–å¾—
        scrapyui_root = os.environ.get('SCRAPYUI_ROOT')
        scrapyui_backend = os.environ.get('SCRAPYUI_BACKEND')

        if scrapyui_root and scrapyui_root not in sys.path:
            sys.path.insert(0, scrapyui_root)
            print(f"ğŸ”§ [RICH] Added SCRAPYUI_ROOT to sys.path: {scrapyui_root}")

        if scrapyui_backend and scrapyui_backend not in sys.path:
            sys.path.insert(0, scrapyui_backend)
            print(f"ğŸ”§ [RICH] Added SCRAPYUI_BACKEND to sys.path: {scrapyui_backend}")

        # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¨æ¸¬
        current_file = Path(__file__).absolute()
        backend_path = current_file.parent.parent.parent  # backend/app/scrapy_extensions/../../../ = backend
        scrapyui_path = backend_path.parent  # backend/../ = scrapyui root

        if str(backend_path) not in sys.path:
            sys.path.insert(0, str(backend_path))
            print(f"ğŸ”§ [RICH] Added backend path to sys.path: {backend_path}")

        if str(scrapyui_path) not in sys.path:
            sys.path.insert(0, str(scrapyui_path))
            print(f"ğŸ”§ [RICH] Added scrapyui path to sys.path: {scrapyui_path}")

        print(f"ğŸ”§ [RICH] Current sys.path: {sys.path[:5]}...")  # æœ€åˆã®5ã¤ã ã‘è¡¨ç¤º

    except Exception as e:
        print(f"âš ï¸ [RICH] Dynamic import setup error: {e}")

# å‹•çš„ã‚¤ãƒ³ãƒãƒ¼ãƒˆè¨­å®šã‚’å®Ÿè¡Œ
_setup_dynamic_imports()

# ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³è¨­å®š
TIMEZONE = pytz.timezone('Asia/Tokyo')

try:
    from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn, SpinnerColumn
    from rich.console import Console
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class RichProgressExtension:
    """
    Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ç”¨Riché€²æ—ãƒãƒ¼æ‹¡å¼µæ©Ÿèƒ½
    
    Features:
    - ç¾ã—ã„é€²æ—ãƒãƒ¼è¡¨ç¤º
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæƒ…å ±
    - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè¡¨ç¤ºå½¢å¼
    - WebSocketçµŒç”±ã§ã®é€²æ—é€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    
    def __init__(self, crawler: Crawler):
        if not RICH_AVAILABLE:
            raise NotConfigured("Rich library is not installed. Run: pip install rich")

        self.crawler = crawler
        self.settings = crawler.settings

        # Riché€²æ—ãƒãƒ¼è¨­å®š
        self.console = Console()
        self.progress = None
        self.live = None
        self.task_id: Optional[TaskID] = None

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'requests_count': 0,
            'responses_count': 0,
            'items_count': 0,
            'errors_count': 0,
            'start_time': None,
            'finish_time': None,
            'total_urls': 0
        }

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.stats_file = None
        self.task_id_str = None

        # è¨­å®š
        self.enabled = self.settings.getbool('RICH_PROGRESS_ENABLED', True)
        self.show_stats = self.settings.getbool('RICH_PROGRESS_SHOW_STATS', True)
        self.update_interval = self.settings.getfloat('RICH_PROGRESS_UPDATE_INTERVAL', 2.0)  # 2ç§’é–“éš”ã«ç·©å’Œ
        self.websocket_enabled = self.settings.getbool('RICH_PROGRESS_WEBSOCKET', False)

        if not self.enabled:
            raise NotConfigured("Rich progress bar is disabled")
    
    @classmethod
    def from_crawler(cls, crawler: Crawler):
        """Crawlerã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        # Rich Progress WebSocketãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯Noneã‚’è¿”ã™
        if not crawler.settings.getbool('RICH_PROGRESS_WEBSOCKET', True):
            return None

        extension = cls(crawler)

        # ã‚·ã‚°ãƒŠãƒ«ã‚’æ¥ç¶šï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰
        try:
            # åŸºæœ¬ã‚·ã‚°ãƒŠãƒ«ã‚’æ¥ç¶š
            crawler.signals.connect(extension.spider_opened, signal=signals.spider_opened)
            crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
            crawler.signals.connect(extension.request_scheduled, signal=signals.request_scheduled)
            crawler.signals.connect(extension.response_received, signal=signals.response_received)
            crawler.signals.connect(extension.item_scraped, signal=signals.item_scraped)
            crawler.signals.connect(extension.spider_error, signal=signals.spider_error)

            # çµ±è¨ˆæ›´æ–°ç”¨ã®è¿½åŠ ã‚·ã‚°ãƒŠãƒ«ã‚‚æ¥ç¶š
            try:
                crawler.signals.connect(extension.request_reached_downloader, signal=signals.request_reached_downloader)
                crawler.signals.connect(extension.response_downloaded, signal=signals.response_downloaded)
            except AttributeError:
                # ä¸€éƒ¨ã®ã‚·ã‚°ãƒŠãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ç„¡è¦–
                pass

            print("ğŸ”§ Rich Progress Extension signals connected successfully")
        except Exception as e:
            print(f"âŒ Failed to connect Rich Progress Extension signals: {e}")

        return extension
    
    def spider_opened(self, spider: Spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†"""
        self.stats['start_time'] = time.time()

        # ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã§è©¦è¡Œï¼‰
        self.task_id_str = (
            os.environ.get('SCRAPY_TASK_ID') or
            getattr(self.crawler, 'task_id', None) or
            getattr(spider, 'task_id', None) or
            f"task_{int(time.time())}"
        )

        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«task_idã‚’è¨­å®šï¼ˆç¢ºå®Ÿã«åˆ©ç”¨å¯èƒ½ã«ã™ã‚‹ï¼‰
        if not hasattr(spider, 'task_id'):
            spider.task_id = self.task_id_str

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¨­å®šï¼ˆè¤‡æ•°ã®æ–¹æ³•ã§è©¦è¡Œï¼‰
        project_path = (
            os.environ.get('SCRAPY_PROJECT_PATH') or
            getattr(spider, 'project_path', None) or
            str(Path.cwd())
        )

        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«project_pathã‚’è¨­å®š
        if not hasattr(spider, 'project_path'):
            spider.project_path = project_path

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
        project_dir = Path(project_path)
        self.stats_file = project_dir / f"stats_{self.task_id_str}.json"

        # start_urlsã®æ•°ã‚’å–å¾—
        if hasattr(spider, 'start_urls'):
            self.stats['total_urls'] = len(spider.start_urls)

        # åˆæœŸçµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        self._save_stats()

        # Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–
        self._initialize_progress(spider)

        spider.logger.info(f"ğŸ¨ Riché€²æ—ãƒãƒ¼é–‹å§‹: {spider.name}")
        spider.logger.info(f"ğŸ“Š çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«: {self.stats_file}")
        spider.logger.info(f"ğŸ”§ Task ID: {self.task_id_str}")
        spider.logger.info(f"ğŸ“ Project path: {project_path}")
    
    def spider_closed(self, spider: Spider, reason: str):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†"""
        print(f"ğŸ”¥ [RICH PROGRESS] spider_closed called with reason: {reason}")
        spider.logger.info(f"ğŸ”¥ [RICH PROGRESS] spider_closed called with reason: {reason}")

        # çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        self.stats['finish_time'] = time.time()

        # Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸ
        self._sync_with_scrapy_stats()

        # æœ€çµ‚çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        self._save_stats()

        # ã‚¿ã‚¹ã‚¯IDã‚’ç¢ºå®Ÿã«å–å¾—ï¼ˆè¤‡æ•°ã®æ–¹æ³•ã§è©¦è¡Œï¼‰
        task_id = (
            getattr(spider, 'task_id', None) or
            self.task_id_str or
            os.environ.get('SCRAPY_TASK_ID')
        )

        print(f"ğŸ”¥ [RICH PROGRESS] Task ID found: {task_id}")
        spider.logger.info(f"ğŸ¯ Spider closed with reason '{reason}' - Task ID: {task_id}")

        # å®Œäº†é€šçŸ¥ã¨ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç™ºå‹•ï¼ˆç†ç”±ã«é–¢ä¿‚ãªãå®Ÿè¡Œï¼‰
        if task_id:
            print(f"ğŸ”¥ [RICH PROGRESS] Starting bulk insert for task: {task_id}")
            spider.logger.info(f"ğŸš€ Triggering Rich progress completion for task {task_id}")

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«task_idã‚’è¨­å®šï¼ˆå¿µã®ãŸã‚ï¼‰
            if not hasattr(spider, 'task_id'):
                spider.task_id = task_id

            # Rich progresså®Œäº†é€šçŸ¥ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’ç™ºå‹•
            try:
                self._trigger_bulk_insert_on_completion(spider)
                print(f"ğŸ”¥ [RICH PROGRESS] Bulk insert completed for task: {task_id}")
            except Exception as e:
                print(f"ğŸ”¥ [RICH PROGRESS] Bulk insert error: {e}")
                spider.logger.error(f"âŒ Rich progress bulk insert error: {e}")
        else:
            print(f"ğŸ”¥ [RICH PROGRESS] No task ID found - skipping bulk insert")
            spider.logger.warning("ğŸ” Task ID not found - skipping Rich progress completion")
            spider.logger.warning(f"ğŸ” Debug info: spider.task_id={getattr(spider, 'task_id', None)}, self.task_id_str={self.task_id_str}, env={os.environ.get('SCRAPY_TASK_ID')}")

        if self.live:
            self.live.stop()

        if self.progress:
            self.progress.stop()

        # æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º
        self._show_final_stats(spider, reason)
        print(f"ğŸ”¥ [RICH PROGRESS] spider_closed completed")

    def _trigger_bulk_insert_on_completion(self, spider):
        """Rich progresså®Œäº†é€šçŸ¥ã§ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’ç™ºå‹•"""
        try:
            print(f"ğŸ”¥ [RICH PROGRESS] _trigger_bulk_insert_on_completion started")

            task_id = getattr(spider, 'task_id', None)
            if not task_id:
                print(f"ğŸ”¥ [RICH PROGRESS] Task ID not found - skipping bulk insert")
                spider.logger.warning("ğŸ” Task ID not found - skipping bulk insert")
                return

            print(f"ğŸ”¥ [RICH PROGRESS] Task ID found: {task_id}")
            spider.logger.info(f"ğŸš€ Rich progress completion triggered - starting bulk insert for task {task_id}")

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’å–å¾—
            project_path = getattr(spider, 'project_path', None)
            if not project_path:
                # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¨æ¸¬
                import os
                project_path = os.getcwd()

            print(f"ğŸ”¥ [RICH PROGRESS] Project path: {project_path}")
            spider.logger.info(f"ğŸ“ Project path: {project_path}")

            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’æ§‹ç¯‰ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
            from pathlib import Path

            # å¯èƒ½ãªãƒ•ã‚¡ã‚¤ãƒ«åãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆresultsãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã‚’å„ªå…ˆï¼‰
            possible_files = [
                # æ–°ã—ã„å½¢å¼ï¼ˆresults/ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ï¼‰
                f"results/{task_id}.jsonl",
                f"results/{task_id}.json",
                f"results/results_{task_id}.jsonl",
                # å¾“æ¥ã®å½¢å¼ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆï¼‰
                f"results_{task_id}.jsonl",
                f"{task_id}.jsonl",
                f"ranking_results.jsonl",
                f"{spider.name}_results.jsonl"
            ]

            print(f"ğŸ”¥ [RICH PROGRESS] Checking possible files: {possible_files}")

            jsonl_file_path = None
            for filename in possible_files:
                file_path = Path(project_path) / filename
                print(f"ğŸ”¥ [RICH PROGRESS] Checking: {file_path} (exists: {file_path.exists()})")
                if file_path.exists():
                    jsonl_file_path = file_path
                    print(f"ğŸ”¥ [RICH PROGRESS] âœ… Found result file: {file_path}")
                    break

            if not jsonl_file_path:
                print(f"ğŸ”¥ [RICH PROGRESS] No JSONL file found in any pattern")
                spider.logger.warning(f"ğŸ“„ No JSONL file found for task {task_id}")
                return

            print(f"ğŸ”¥ [RICH PROGRESS] Found JSONL file: {jsonl_file_path}")
            spider.logger.info(f"ğŸ“„ Found JSONL file: {jsonl_file_path}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°ã‚’ç¢ºèª
            file_size = jsonl_file_path.stat().st_size
            with open(jsonl_file_path, 'r', encoding='utf-8') as f:
                lines = [line.strip() for line in f if line.strip()]

            print(f"ğŸ”¥ [RICH PROGRESS] File size: {file_size} bytes, Lines: {len(lines)}")
            spider.logger.info(f"ğŸ“Š File size: {file_size} bytes, Lines: {len(lines)}")

            if len(lines) == 0:
                print(f"ğŸ”¥ [RICH PROGRESS] No data lines found in JSONL file")
                spider.logger.warning("ğŸ“„ No data lines found in JSONL file")
                return

            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
            print(f"ğŸ”¥ [RICH PROGRESS] Starting _execute_bulk_insert")
            self._execute_bulk_insert(task_id, lines, spider)
            print(f"ğŸ”¥ [RICH PROGRESS] _execute_bulk_insert completed")

        except Exception as e:
            print(f"ğŸ”¥ [RICH PROGRESS] Error in _trigger_bulk_insert_on_completion: {e}")
            spider.logger.error(f"âŒ Bulk insert trigger error: {e}")
            import traceback
            print(f"ğŸ”¥ [RICH PROGRESS] Traceback: {traceback.format_exc()}")
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")

    def _execute_bulk_insert(self, task_id: str, lines: list, spider):
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆæ ¹æœ¬å¯¾å¿œç‰ˆï¼‰"""
        try:
            spider.logger.info(f"ğŸ”„ Starting JSONL bulk insert for {len(lines)} lines")

            # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèªã¨ä½œæˆï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰
            task_id = self._ensure_task_exists(task_id, spider)

            # ç›´æ¥ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’å®Ÿè¡Œ
            inserted_count = self._bulk_insert_from_jsonl_lines(task_id, lines, spider)

            spider.logger.info(f"âœ… JSONL bulk insert completed: {inserted_count}/{len(lines)} items inserted")

            # å¿µã®ãŸã‚é‡è¤‡ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ
            spider.logger.info(f"ğŸ§¹ Starting post-insert duplicate cleanup for task {task_id}")
            cleanup_result = self._cleanup_duplicate_records(task_id, spider)

            # WebSocketé€šçŸ¥ã‚’é€ä¿¡
            self._send_completion_websocket_notification(task_id, inserted_count, spider, cleanup_result)

        except Exception as e:
            spider.logger.error(f"âŒ JSONL bulk insert execution error: {e}")
            import traceback
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")

    def _bulk_insert_from_jsonl_lines(self, task_id: str, lines: list, spider) -> int:
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œã‹ã‚‰ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        try:
            from ..database import SessionLocal, Result
            import json
            import uuid
            import hashlib
            from datetime import datetime

            spider.logger.info(f"ğŸ“Š Processing {len(lines)} JSONL lines for bulk insert")

            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œã‚’è§£æ
            items_data = []
            for line_num, line in enumerate(lines, 1):
                try:
                    if line.strip():
                        item_data = json.loads(line.strip())
                        items_data.append(item_data)
                except json.JSONDecodeError as e:
                    spider.logger.warning(f"âš ï¸ JSON decode error at line {line_num}: {e}")

            if not items_data:
                spider.logger.warning(f"âš ï¸ No valid items found in JSONL lines")
                return 0

            spider.logger.info(f"ğŸ“¦ Found {len(items_data)} valid items in JSONL lines")

            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
            db = SessionLocal()
            try:
                bulk_data = []
                skipped_count = 0

                for item_data in items_data:
                    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
                    data_hash = self._generate_data_hash(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ã‚¿ã‚¹ã‚¯å†…ï¼‰
                    existing = db.query(Result).filter(
                        Result.task_id == task_id,
                        Result.data_hash == data_hash
                    ).first()

                    if existing:
                        skipped_count += 1
                        spider.logger.debug(f"âš ï¸ Duplicate data skipped: {data_hash[:8]}...")
                        continue

                    result_id = str(uuid.uuid4())
                    bulk_data.append({
                        'id': result_id,
                        'task_id': task_id,
                        'data': item_data,
                        'data_hash': data_hash,
                        'item_acquired_datetime': datetime.now(TIMEZONE),
                        'created_at': datetime.now(TIMEZONE)
                    })

                # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
                inserted_count = 0
                if bulk_data:
                    db.bulk_insert_mappings(Result, bulk_data)
                    db.commit()
                    inserted_count = len(bulk_data)
                    spider.logger.info(f"âœ… Bulk insert completed: {inserted_count} items inserted, {skipped_count} duplicates skipped")
                else:
                    spider.logger.info(f"âš ï¸ No new data to insert, {skipped_count} duplicates skipped")

                # ã‚¿ã‚¹ã‚¯ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ›´æ–°
                self._update_task_item_count(task_id, db, spider)

                return inserted_count

            except Exception as e:
                db.rollback()
                spider.logger.error(f"âŒ Bulk insert error: {e}")
                raise
            finally:
                db.close()

        except Exception as e:
            spider.logger.error(f"âŒ Bulk insert from JSONL lines error: {e}")
            return 0

    def _generate_data_hash(self, item_data: dict) -> str:
        """ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        try:
            import json
            import hashlib

            # item_typeã«å¿œã˜ã¦é©åˆ‡ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é¸æŠ
            item_type = item_data.get('item_type', 'unknown')

            if item_type == 'ranking_product':
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°å•†å“ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'ranking_position': item_data.get('ranking_position'),
                    'page_number': item_data.get('page_number'),
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'source_url': item_data.get('source_url')
                }
            elif item_type == 'ranking_product_detail':
                # å•†å“è©³ç´°ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'description': item_data.get('description'),
                    'detail_scraped_at': item_data.get('detail_scraped_at')
                }
            else:
                # ãã®ä»–ã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                hash_data = item_data.copy()

            # è¾æ›¸ã‚’ã‚½ãƒ¼ãƒˆã—ã¦JSONæ–‡å­—åˆ—ã«å¤‰æ›
            hash_string = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
        except Exception as e:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥
            data_str = json.dumps(item_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()

    def _update_task_item_count(self, task_id: str, db, spider):
        """ã‚¿ã‚¹ã‚¯ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ›´æ–°"""
        try:
            from ..database import Task, Result
            from datetime import datetime

            # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = db.query(Task).filter(Task.id == task_id).first()
            if task:
                # çµæœæ•°ã‚’å–å¾—
                result_count = db.query(Result).filter(Result.task_id == task_id).count()

                # ã‚¿ã‚¹ã‚¯ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ›´æ–°
                task.items_count = result_count
                task.updated_at = datetime.now(TIMEZONE)

                db.commit()
                spider.logger.info(f"ğŸ“Š Task item count updated: {result_count} items")

        except Exception as e:
            spider.logger.error(f"âŒ Task item count update error: {e}")

    def _cleanup_duplicate_records(self, task_id: str, spider):
        """é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
        try:
            spider.logger.info(f"ğŸ§¹ Starting duplicate cleanup for task {task_id}")

            from ..database import get_db, Result as DBResult
            from sqlalchemy import func

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
            db_gen = get_db()
            db = next(db_gen)

            try:
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‰ã®ä»¶æ•°ã‚’ç¢ºèª
                before_count = db.query(DBResult).filter(DBResult.task_id == task_id).count()
                spider.logger.info(f"ğŸ“Š Before cleanup: {before_count} records")

                # é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ç‰¹å®šï¼ˆdata_hashãŒåŒã˜ã‚‚ã®ã‚’æ¤œç´¢ï¼‰
                duplicate_subquery = (
                    db.query(DBResult.data_hash)
                    .filter(DBResult.task_id == task_id)
                    .group_by(DBResult.data_hash)
                    .having(func.count(DBResult.data_hash) > 1)
                    .subquery()
                )

                # é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã«æœ€æ–°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å‰Šé™¤
                duplicates_to_delete = []
                duplicate_hashes = db.query(duplicate_subquery.c.data_hash).all()

                spider.logger.info(f"ğŸ” Found {len(duplicate_hashes)} duplicate hash groups")

                for (hash_value,) in duplicate_hashes:
                    # åŒã˜ãƒãƒƒã‚·ãƒ¥ã‚’æŒã¤ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆä½œæˆæ—¥æ™‚é †ï¼‰
                    duplicate_records = (
                        db.query(DBResult)
                        .filter(DBResult.task_id == task_id)
                        .filter(DBResult.data_hash == hash_value)
                        .order_by(DBResult.created_at.desc())
                        .all()
                    )

                    # æœ€æ–°ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ä»¥å¤–ã‚’å‰Šé™¤å¯¾è±¡ã«è¿½åŠ 
                    if len(duplicate_records) > 1:
                        records_to_delete = duplicate_records[1:]  # æœ€æ–°ä»¥å¤–
                        duplicates_to_delete.extend(records_to_delete)

                        spider.logger.info(f"ğŸ—‘ï¸ Hash {hash_value[:8]}...: keeping 1, deleting {len(records_to_delete)} duplicates")

                # é‡è¤‡ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤
                deleted_count = 0
                if duplicates_to_delete:
                    for record in duplicates_to_delete:
                        db.delete(record)
                        deleted_count += 1

                    db.commit()
                    spider.logger.info(f"âœ… Deleted {deleted_count} duplicate records")
                else:
                    spider.logger.info(f"âœ… No duplicate records found to delete")

                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¾Œã®ä»¶æ•°ã‚’ç¢ºèª
                after_count = db.query(DBResult).filter(DBResult.task_id == task_id).count()
                spider.logger.info(f"ğŸ“Š After cleanup: {after_count} records")

                # çµæœã‚’ã¾ã¨ã‚ã‚‹
                cleanup_result = {
                    'before_count': before_count,
                    'after_count': after_count,
                    'deleted_count': deleted_count,
                    'duplicate_groups': len(duplicate_hashes)
                }

                spider.logger.info(f"ğŸ§¹ Cleanup completed: {before_count} â†’ {after_count} (-{deleted_count})")

                return cleanup_result

            finally:
                db.close()

        except Exception as e:
            spider.logger.error(f"âŒ Duplicate cleanup error: {e}")
            import traceback
            spider.logger.error(f"âŒ Traceback: {traceback.format_exc()}")
            return {
                'before_count': 0,
                'after_count': 0,
                'deleted_count': 0,
                'duplicate_groups': 0,
                'error': str(e)
            }

    def _send_completion_websocket_notification(self, task_id: str, items_inserted: int, spider, cleanup_result=None):
        """å®Œäº†é€šçŸ¥ã®WebSocketé€ä¿¡"""
        try:
            # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            completion_data = {
                'taskId': task_id,
                'status': 'completed',
                'itemsScraped': items_inserted,
                'requestsCount': self.stats['requests_count'],
                'errorCount': self.stats['errors_count'],
                'elapsedTime': int(self.stats.get('finish_time', time.time()) - self.stats.get('start_time', time.time())),
                'progressPercentage': 100.0,
                'message': f'Rich progress completed - {items_inserted} items bulk inserted',
                'bulkInsertCompleted': True
            }

            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœã‚’è¿½åŠ 
            if cleanup_result:
                completion_data.update({
                    'cleanupCompleted': True,
                    'cleanupResult': cleanup_result,
                    'finalItemCount': cleanup_result.get('after_count', items_inserted),
                    'duplicatesRemoved': cleanup_result.get('deleted_count', 0)
                })

                # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›´æ–°
                deleted_count = cleanup_result.get('deleted_count', 0)
                if deleted_count > 0:
                    completion_data['message'] = f'Rich progress completed - {items_inserted} items inserted, {deleted_count} duplicates removed'
                else:
                    completion_data['message'] = f'Rich progress completed - {items_inserted} items inserted, no duplicates found'

            spider.logger.info(f"ğŸ“¡ Sending completion WebSocket notification: {completion_data}")

            # WebSocketé€ä¿¡ï¼ˆéåŒæœŸï¼‰
            import asyncio
            from ..api.websocket_progress import broadcast_rich_progress_update

            try:
                # éåŒæœŸã§WebSocketé€ä¿¡
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(
                        broadcast_rich_progress_update(task_id, completion_data)
                    )
                else:
                    loop.run_until_complete(
                        broadcast_rich_progress_update(task_id, completion_data)
                    )
                spider.logger.info("ğŸ“¡ Completion WebSocket notification sent successfully")
            except Exception as ws_error:
                spider.logger.warning(f"ğŸ“¡ WebSocket notification failed: {ws_error}")

        except Exception as e:
            spider.logger.error(f"âŒ Completion notification error: {e}")

    def _ensure_task_exists(self, task_id: str, spider) -> str:
        """ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèªã¨ä½œæˆï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰"""
        try:
            from ..database import SessionLocal, Task
            from datetime import datetime

            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèª
                existing_task = db.query(Task).filter(Task.id == task_id).first()

                if existing_task:
                    spider.logger.info(f"âœ… Task {task_id} already exists")
                    return task_id

                # ã‚¿ã‚¹ã‚¯ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
                spider.logger.warning(f"âš ï¸ Task {task_id} not found, creating new task")

                new_task = Task(
                    id=task_id,
                    spider_name=spider.name,
                    project_name=getattr(spider, 'project_name', 'unknown'),
                    status='RUNNING',
                    items_count=0,
                    requests_count=0,
                    errors_count=0,
                    created_at=datetime.now(TIMEZONE),
                    started_at=datetime.now(TIMEZONE),
                    updated_at=datetime.now(TIMEZONE)
                )

                db.add(new_task)
                db.commit()

                spider.logger.info(f"âœ… Created new task: {task_id}")
                return task_id

            finally:
                db.close()

        except Exception as e:
            spider.logger.error(f"âŒ Task creation error: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…ƒã®ã‚¿ã‚¹ã‚¯IDã‚’è¿”ã™
            return task_id

    def request_scheduled(self, request, spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚ã®å‡¦ç†ï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰"""
        try:
            # çµ±è¨ˆã‚’ç›´æ¥æ›´æ–°
            self.stats['requests_count'] += 1
            spider.logger.debug(f"ğŸ“¤ Request scheduled: {self.stats['requests_count']}")

            # Scrapyã®çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—ã—ã¦åŒæœŸ
            self._sync_with_scrapy_stats()
            self._update_progress()
            self._save_stats()
        except Exception as e:
            spider.logger.error(f"âŒ Error in request_scheduled: {e}")

    def request_reached_downloader(self, request, spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ã«åˆ°é”ã—ãŸæ™‚ã®å‡¦ç†ï¼ˆè¿½åŠ ã‚·ã‚°ãƒŠãƒ«ï¼‰"""
        try:
            spider.logger.debug(f"ğŸ”„ Request reached downloader: {request.url}")
            self._sync_with_scrapy_stats()
            self._update_progress()
        except Exception as e:
            spider.logger.error(f"âŒ Error in request_reached_downloader: {e}")

    def response_received(self, response, request, spider):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡æ™‚ã®å‡¦ç†ï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰"""
        try:
            # çµ±è¨ˆã‚’ç›´æ¥æ›´æ–°
            self.stats['responses_count'] += 1
            spider.logger.debug(f"ğŸ“¥ Response received: {self.stats['responses_count']}")

            # Scrapyã®çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—ã—ã¦åŒæœŸ
            self._sync_with_scrapy_stats()
            self._update_progress()
            self._save_stats()
        except Exception as e:
            spider.logger.error(f"âŒ Error in response_received: {e}")

    def response_downloaded(self, response, request, spider):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã®å‡¦ç†ï¼ˆè¿½åŠ ã‚·ã‚°ãƒŠãƒ«ï¼‰"""
        try:
            spider.logger.debug(f"âœ… Response downloaded: {response.url}")
            self._sync_with_scrapy_stats()
            self._update_progress()
        except Exception as e:
            spider.logger.error(f"âŒ Error in response_downloaded: {e}")

    def item_scraped(self, item, response, spider):
        """ã‚¢ã‚¤ãƒ†ãƒ å–å¾—æ™‚ã®å‡¦ç†ï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰"""
        try:
            # çµ±è¨ˆã‚’ç›´æ¥æ›´æ–°
            self.stats['items_count'] += 1
            spider.logger.debug(f"ğŸ“¦ Item scraped: {self.stats['items_count']}")

            # Scrapyã®çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—ã—ã¦åŒæœŸ
            self._sync_with_scrapy_stats()
            self._update_progress()
            self._save_stats()
        except Exception as e:
            spider.logger.error(f"âŒ Error in item_scraped: {e}")

    def spider_error(self, failure, response, spider):
        """ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†ï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰"""
        try:
            # çµ±è¨ˆã‚’ç›´æ¥æ›´æ–°
            self.stats['errors_count'] += 1
            spider.logger.debug(f"âŒ Error occurred: {self.stats['errors_count']}")

            # Scrapyã®çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—ã—ã¦åŒæœŸ
            self._sync_with_scrapy_stats()
            self._update_progress()
            self._save_stats()
        except Exception as e:
            spider.logger.error(f"âŒ Error in spider_error: {e}")

    def _sync_with_scrapy_stats(self):
        """Scrapyã®çµ±è¨ˆã‚·ã‚¹ãƒ†ãƒ ã¨åŒæœŸï¼ˆæ ¹æœ¬ä¿®æ­£ç‰ˆï¼‰"""
        try:
            if hasattr(self, 'crawler') and hasattr(self.crawler, 'stats'):
                scrapy_stats = self.crawler.stats.get_stats()

                # Scrapyã®çµ±è¨ˆã‹ã‚‰å®Ÿéš›ã®å€¤ã‚’å–å¾—
                scrapy_requests = scrapy_stats.get('downloader/request_count', 0)
                scrapy_responses = scrapy_stats.get('downloader/response_count', 0)
                scrapy_items = scrapy_stats.get('item_scraped_count', 0)
                scrapy_errors = scrapy_stats.get('spider_exceptions', 0)

                # çµ±è¨ˆã‚’åŒæœŸï¼ˆScrapyã®å€¤ã‚’å„ªå…ˆï¼‰
                if scrapy_requests > 0:
                    self.stats['requests_count'] = scrapy_requests
                if scrapy_responses > 0:
                    self.stats['responses_count'] = scrapy_responses
                if scrapy_items > 0:
                    self.stats['items_count'] = scrapy_items
                if scrapy_errors > 0:
                    self.stats['errors_count'] = scrapy_errors

                print(f"ğŸ”„ Stats synced - R:{scrapy_requests}, Res:{scrapy_responses}, I:{scrapy_items}, E:{scrapy_errors}")

        except Exception as e:
            print(f"âŒ Error syncing with Scrapy stats: {e}")
    
    def _initialize_progress(self, spider: Spider):
        """Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–"""
        # ã‚«ã‚¹ã‚¿ãƒ é€²æ—ãƒãƒ¼ã‚«ãƒ©ãƒ 
        columns = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=40),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("â€¢"),
            TextColumn("[bold green]{task.completed}/{task.total}"),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
        ]
        
        self.progress = Progress(*columns, console=self.console)
        
        # ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
        total = max(self.stats['total_urls'], 1)  # æœ€ä½1ã«è¨­å®š
        self.task_id = self.progress.add_task(
            f"ğŸ•·ï¸ {spider.name}",
            total=total
        )
        
        if self.show_stats:
            # ãƒ©ã‚¤ãƒ–è¡¨ç¤ºã§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨é€²æ—ãƒãƒ¼ã‚’çµ„ã¿åˆã‚ã›ï¼ˆãƒªãƒ•ãƒ¬ãƒƒã‚·ãƒ¥ãƒ¬ãƒ¼ãƒˆã‚’ç·©å’Œï¼‰
            self.live = Live(self._create_layout(), console=self.console, refresh_per_second=2)
            self.live.start()
        else:
            self.progress.start()
    
    def _create_layout(self):
        """è¡¨ç¤ºãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ"""
        # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        stats_table = Table(title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ", show_header=True, header_style="bold magenta")
        stats_table.add_column("é …ç›®", style="cyan", width=15)
        stats_table.add_column("å€¤", style="green", width=10)
        
        # çµŒéæ™‚é–“ã‚’è¨ˆç®—
        elapsed_time = 0
        if self.stats['start_time']:
            import time
            elapsed_time = time.time() - self.stats['start_time']
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        stats_table.add_row("ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", str(self.stats['requests_count']))
        stats_table.add_row("ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹", str(self.stats['responses_count']))
        stats_table.add_row("ğŸ“¦ ã‚¢ã‚¤ãƒ†ãƒ ", str(self.stats['items_count']))
        stats_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼", str(self.stats['errors_count']))
        stats_table.add_row("â±ï¸ çµŒéæ™‚é–“", f"{elapsed_time:.1f}ç§’")
        
        # é€Ÿåº¦è¨ˆç®—
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            stats_table.add_row("ğŸš€ å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’çµ„ã¿åˆã‚ã›
        layout = Table.grid()
        layout.add_row(Panel(self.progress, title="ğŸ¯ é€²æ—çŠ¶æ³", border_style="blue"))
        layout.add_row(Panel(stats_table, title="ğŸ“ˆ è©³ç´°çµ±è¨ˆ", border_style="green"))
        
        return layout
    
    def _update_progress(self):
        """é€²æ—ãƒãƒ¼ã‚’æ›´æ–°"""
        if not self.progress or not self.task_id:
            return
        
        # é€²æ—ã‚’æ›´æ–°ï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        completed = self.stats['items_count']
        
        # å‹•çš„ã«ç·æ•°ã‚’èª¿æ•´ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒåˆæœŸäºˆæƒ³ã‚’è¶…ãˆãŸå ´åˆï¼‰
        if self.stats['requests_count'] > self.stats['total_urls']:
            total = max(self.stats['requests_count'], self.stats['total_urls'])
            self.progress.update(self.task_id, total=total)
        
        self.progress.update(self.task_id, completed=completed)
        
        # WebSocketé€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.websocket_enabled:
            self._send_websocket_update()
    
    def _send_websocket_update(self):
        """WebSocketçµŒç”±ã§é€²æ—ã‚’é€šçŸ¥"""
        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                import time
                elapsed_time = time.time() - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time

            # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            progress_data = {
                'taskId': getattr(self.crawler, 'task_id', 'unknown'),
                'status': 'running',
                'itemsScraped': self.stats['items_count'],
                'requestsCount': self.stats['requests_count'],
                'errorCount': self.stats['errors_count'],
                'elapsedTime': int(elapsed_time),
                'progressPercentage': self._calculate_progress_percentage(),
                'itemsPerSecond': round(items_per_second, 2),
                'requestsPerSecond': round(requests_per_second, 2),
                'totalPages': self.stats['total_urls'],
                'currentPage': self.stats['responses_count']
            }

            # ScrapyUIã®WebSocketãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«é€ä¿¡
            try:
                import asyncio
                from ..api.websocket_progress import broadcast_rich_progress_update

                # éåŒæœŸã§WebSocketé€ä¿¡
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
                else:
                    loop.run_until_complete(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
            except Exception as ws_error:
                # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
                pass

        except Exception as e:
            # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass
    
    def _calculate_progress_percentage(self) -> float:
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        if self.stats['total_urls'] == 0:
            return 0.0
        
        # ã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—
        return min(100.0, (self.stats['items_count'] / self.stats['total_urls']) * 100)
    
    def _show_final_stats(self, spider: Spider, reason: str):
        """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
        import time
        
        elapsed_time = 0
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_table = Table(title=f"ğŸ {spider.name} å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ", show_header=True, header_style="bold yellow")
        final_table.add_column("é …ç›®", style="cyan", width=20)
        final_table.add_column("å€¤", style="green", width=15)
        
        final_table.add_row("ğŸ“¤ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", str(self.stats['requests_count']))
        final_table.add_row("ğŸ“¥ ç·ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°", str(self.stats['responses_count']))
        final_table.add_row("ğŸ“¦ ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°", str(self.stats['items_count']))
        final_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼æ•°", str(self.stats['errors_count']))
        final_table.add_row("â±ï¸ ç·å®Ÿè¡Œæ™‚é–“", f"{elapsed_time:.2f}ç§’")
        final_table.add_row("ğŸ çµ‚äº†ç†ç”±", reason)
        
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            final_table.add_row("ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        self.console.print(Panel(final_table, title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†", border_style="yellow"))

    def _save_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.stats_file:
            return

        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                current_time = self.stats['finish_time'] or time.time()
                elapsed_time = current_time - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            items_per_minute = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time
                items_per_minute = items_per_second * 60

            # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡è¨ˆç®—
            total_responses = self.stats['responses_count']
            success_rate = 0
            error_rate = 0
            if total_responses > 0:
                success_rate = ((total_responses - self.stats['errors_count']) / total_responses) * 100
                error_rate = (self.stats['errors_count'] / total_responses) * 100

            # Scrapyæ¨™æº–å½¢å¼ã®çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
            scrapy_stats = {
                # åŸºæœ¬çµ±è¨ˆ
                'item_scraped_count': self.stats['items_count'],
                'downloader/request_count': self.stats['requests_count'],
                'response_received_count': self.stats['responses_count'],
                'spider_exceptions': self.stats['errors_count'],

                # æ™‚é–“æƒ…å ±
                'elapsed_time_seconds': elapsed_time,
                'start_time': self.stats['start_time'],
                'finish_time': self.stats['finish_time'],

                # é€Ÿåº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                'items_per_second': items_per_second,
                'requests_per_second': requests_per_second,
                'items_per_minute': items_per_minute,

                # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡
                'success_rate': success_rate,
                'error_rate': error_rate,

                # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'downloader/response_status_count/200': max(0, self.stats['responses_count'] - self.stats['errors_count']),
                'downloader/response_status_count/404': 0,
                'downloader/response_status_count/500': self.stats['errors_count'],

                # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'log_count/DEBUG': 0,
                'log_count/INFO': self.stats['items_count'],
                'log_count/WARNING': 0,
                'log_count/ERROR': self.stats['errors_count'],
                'log_count/CRITICAL': 0,

                # Rich progressæ‹¡å¼µçµ±è¨ˆ
                'rich_progress_enabled': True,
                'rich_progress_version': '1.0.0'
            }

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(scrapy_stats, f, indent=2, ensure_ascii=False)

        except Exception as e:
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass

    def _sync_with_scrapy_stats(self):
        """Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            if hasattr(self.crawler, 'stats'):
                scrapy_stats = self.crawler.stats

                # Scrapyã®çµ±è¨ˆæƒ…å ±ã‚’å¸¸ã«å„ªå…ˆï¼ˆå®Ÿéš›ã®HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¨ä¸€è‡´ã•ã›ã‚‹ï¼‰
                items_count = scrapy_stats.get_value('item_scraped_count', 0)
                requests_count = scrapy_stats.get_value('downloader/request_count', 0)
                responses_count = scrapy_stats.get_value('response_received_count', 0)
                errors_count = scrapy_stats.get_value('spider_exceptions', 0)

                # çµ±è¨ˆãŒæ›´æ–°ã•ã‚ŒãŸå ´åˆã®ã¿åæ˜ 
                if items_count > 0:
                    self.stats['items_count'] = items_count
                if requests_count > 0:
                    self.stats['requests_count'] = requests_count
                if responses_count > 0:
                    self.stats['responses_count'] = responses_count
                if errors_count > 0:
                    self.stats['errors_count'] = errors_count

                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
                if items_count > 0 or requests_count > 0:
                    print(f"ğŸ“Š Stats sync: items={items_count}, requests={requests_count}, responses={responses_count}")

        except Exception as e:
            # åŒæœŸã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            print(f"âš ï¸ Stats sync error: {e}")
            pass




# è¨­å®šä¾‹ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã§è¨˜è¼‰
"""
# settings.pyã«è¿½åŠ ã™ã‚‹è¨­å®šä¾‹

# Riché€²æ—ãƒãƒ¼ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_ENABLED = True

# è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
RICH_PROGRESS_SHOW_STATS = True

# æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰- å¤§å¹…ã«ç·©å’Œ
RICH_PROGRESS_UPDATE_INTERVAL = 2.0

# WebSocketé€šçŸ¥ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_WEBSOCKET = True

# æ‹¡å¼µæ©Ÿèƒ½ã‚’ç™»éŒ²
EXTENSIONS = {
    'app.scrapy_extensions.rich_progress_extension.RichProgressExtension': 500,
}
"""
