#!/usr/bin/env python3
"""
ScrapyUI Spider Manager - Watchdog Service
scrapy crawlwithwatchdog ã®ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹ç‰ˆå®Ÿè£…
"""

import asyncio
import json
import logging
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Callable

import aiofiles
import aioredis
import asyncpg
from pydantic import BaseModel

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WatchdogTask(BaseModel):
    task_id: str
    project_id: str
    spider_id: str
    project_path: str
    spider_name: str
    output_file: str
    settings: Dict = {}
    created_at: str
    started_at: Optional[str] = None
    finished_at: Optional[str] = None
    status: str = "PENDING"
    items_count: int = 0
    requests_count: int = 0
    error_message: Optional[str] = None

class WatchdogMonitor:
    """ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŒ¿å…¥"""
    
    def __init__(self, task: WatchdogTask, db_pool: asyncpg.Pool, 
                 redis: aioredis.Redis, websocket_callback: Optional[Callable] = None):
        self.task = task
        self.db_pool = db_pool
        self.redis = redis
        self.websocket_callback = websocket_callback
        
        self.is_monitoring = False
        self.processed_lines = 0
        self.last_file_size = 0
        self.last_websocket_time = 0
        self.websocket_interval = 15.0  # 15ç§’é–“éš”
        
    async def start_monitoring(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹"""
        self.is_monitoring = True
        output_file = Path(self.task.output_file)
        
        logger.info(f"ğŸ” Starting file monitoring: {output_file}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        max_wait = 30  # 30ç§’å¾…æ©Ÿ
        wait_count = 0
        
        while not output_file.exists() and wait_count < max_wait:
            await asyncio.sleep(1)
            wait_count += 1
        
        if not output_file.exists():
            logger.warning(f"âš ï¸ Output file not created within {max_wait}s: {output_file}")
            return
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ãƒ«ãƒ¼ãƒ—
        while self.is_monitoring:
            try:
                await self._check_file_changes(output_file)
                await asyncio.sleep(2)  # 2ç§’é–“éš”ã§ãƒã‚§ãƒƒã‚¯
            except Exception as e:
                logger.error(f"âŒ Monitoring error: {e}")
                await asyncio.sleep(5)
    
    async def _check_file_changes(self, file_path: Path):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦æ–°ã—ã„è¡Œã‚’å‡¦ç†"""
        try:
            current_size = file_path.stat().st_size
            
            if current_size > self.last_file_size:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¢—åŠ ã—ãŸå ´åˆã€æ–°ã—ã„è¡Œã‚’èª­ã¿å–ã‚Š
                async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                    await f.seek(self.last_file_size)
                    new_content = await f.read()
                
                if new_content.strip():
                    lines = new_content.strip().split('\n')
                    for line in lines:
                        if line.strip():
                            await self._process_new_line(line.strip())
                
                self.last_file_size = current_size
                
        except Exception as e:
            logger.error(f"âŒ File check error: {e}")
    
    async def _process_new_line(self, line: str):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ã—ã¦ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥"""
        try:
            # JSONè¡Œã‚’ãƒ‘ãƒ¼ã‚¹
            data = json.loads(line)
            self.processed_lines += 1
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥
            await self._insert_result_to_db(data)
            
            # ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°
            await self._update_task_stats()
            
            # WebSocketé€šçŸ¥ï¼ˆé »åº¦åˆ¶é™ã‚ã‚Šï¼‰
            await self._send_websocket_notification()
            
        except json.JSONDecodeError:
            logger.warning(f"âš ï¸ Invalid JSON line: {line[:100]}...")
        except Exception as e:
            logger.error(f"âŒ Line processing error: {e}")
    
    async def _insert_result_to_db(self, data: Dict):
        """çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æŒ¿å…¥"""
        try:
            async with self.db_pool.acquire() as conn:
                # é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
                import hashlib
                data_str = json.dumps(data, sort_keys=True, ensure_ascii=False)
                data_hash = hashlib.md5(data_str.encode('utf-8')).hexdigest()
                
                # çµæœæŒ¿å…¥
                await conn.execute("""
                    INSERT INTO results (id, task_id, data, hash, created_at)
                    VALUES ($1, $2, $3, $4, $5)
                    ON CONFLICT (hash) DO NOTHING
                """, 
                f"{self.task.task_id}_{self.processed_lines}",
                self.task.task_id,
                json.dumps(data),
                data_hash,
                datetime.now()
                )
                
        except Exception as e:
            logger.error(f"âŒ Database insert error: {e}")
    
    async def _update_task_stats(self):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°"""
        try:
            # RedisçµŒç”±ã§ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°
            await self.redis.hset(f"task:{self.task.task_id}", mapping={
                "items_count": self.processed_lines,
                "status": "RUNNING",
                "updated_at": datetime.now().isoformat()
            })
            
        except Exception as e:
            logger.error(f"âŒ Stats update error: {e}")
    
    async def _send_websocket_notification(self):
        """WebSocketé€šçŸ¥é€ä¿¡ï¼ˆé »åº¦åˆ¶é™ã‚ã‚Šï¼‰"""
        try:
            current_time = time.time()
            
            if current_time - self.last_websocket_time >= self.websocket_interval:
                if self.websocket_callback:
                    notification_data = {
                        "task_id": self.task.task_id,
                        "type": "progress_update",
                        "items_processed": self.processed_lines,
                        "timestamp": datetime.now().isoformat()
                    }
                    
                    await self.websocket_callback(notification_data)
                    self.last_websocket_time = current_time
                
                # RedisçµŒç”±ã§WebSocketé€šçŸ¥
                await self.redis.publish("events:spider_progress", json.dumps({
                    "task_id": self.task.task_id,
                    "items_processed": self.processed_lines,
                    "timestamp": datetime.now().isoformat()
                }))
                
        except Exception as e:
            logger.error(f"âŒ WebSocket notification error: {e}")
    
    def stop_monitoring(self):
        """ç›£è¦–åœæ­¢"""
        self.is_monitoring = False
        logger.info(f"ğŸ›‘ Monitoring stopped: {self.processed_lines} items processed")

class WatchdogSpiderService:
    """scrapy crawlwithwatchdog ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹å®Ÿè£…"""
    
    def __init__(self):
        self.db_pool: Optional[asyncpg.Pool] = None
        self.redis: Optional[aioredis.Redis] = None
        self.active_monitors: Dict[str, WatchdogMonitor] = {}
        self.base_projects_path = Path("/app/scrapy_projects")
        
    async def initialize(self):
        """åˆæœŸåŒ–"""
        try:
            # Redisæ¥ç¶š
            self.redis = aioredis.from_url(
                "redis://localhost:6379",
                encoding="utf-8",
                decode_responses=True
            )
            
            # PostgreSQLæ¥ç¶š
            self.db_pool = await asyncpg.create_pool(
                "postgresql://user:password@localhost:5432/scrapyui",
                min_size=2,
                max_size=10
            )
            
            logger.info("ğŸ”— Watchdog Spider Service initialized")
            
        except Exception as e:
            logger.error(f"âŒ Failed to initialize: {e}")
            raise
    
    async def execute_spider_with_watchdog(self, task: WatchdogTask, 
                                         websocket_callback: Optional[Callable] = None) -> Dict:
        """watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ"""
        try:
            logger.info(f"ğŸš€ Starting spider with watchdog: {task.spider_name}")
            
            # ã‚¿ã‚¹ã‚¯é–‹å§‹çŠ¶æ…‹ã«æ›´æ–°
            task.status = "RUNNING"
            task.started_at = datetime.now().isoformat()
            
            await self.redis.hset(f"task:{task.task_id}", mapping={
                "status": "RUNNING",
                "started_at": task.started_at
            })
            
            # ç›£è¦–é–‹å§‹
            monitor = WatchdogMonitor(task, self.db_pool, self.redis, websocket_callback)
            self.active_monitors[task.task_id] = monitor
            
            # ç›£è¦–ã‚’åˆ¥ã‚¿ã‚¹ã‚¯ã§é–‹å§‹
            monitor_task = asyncio.create_task(monitor.start_monitoring())
            
            # Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            scrapy_result = await self._execute_scrapy_process(task)
            
            # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç›£è¦–åœæ­¢
            await asyncio.sleep(5)
            monitor.stop_monitoring()
            
            # ç›£è¦–ã‚¿ã‚¹ã‚¯çµ‚äº†å¾…æ©Ÿ
            try:
                await asyncio.wait_for(monitor_task, timeout=10)
            except asyncio.TimeoutError:
                monitor_task.cancel()
            
            # æœ€çµ‚çµ±è¨ˆæ›´æ–°
            await self._finalize_task_stats(task, monitor.processed_lines)
            
            # ç›£è¦–ãƒªã‚¹ãƒˆã‹ã‚‰å‰Šé™¤
            if task.task_id in self.active_monitors:
                del self.active_monitors[task.task_id]
            
            return {
                "success": scrapy_result["success"],
                "task_id": task.task_id,
                "items_processed": monitor.processed_lines,
                "return_code": scrapy_result.get("return_code", 0),
                "output_file": task.output_file
            }
            
        except Exception as e:
            logger.error(f"âŒ Spider execution error: {e}")
            
            # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã«æ›´æ–°
            await self.redis.hset(f"task:{task.task_id}", mapping={
                "status": "FAILED",
                "error": str(e),
                "finished_at": datetime.now().isoformat()
            })
            
            return {
                "success": False,
                "task_id": task.task_id,
                "error": str(e)
            }
    
    async def _execute_scrapy_process(self, task: WatchdogTask) -> Dict:
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ"""
        try:
            project_path = self.base_projects_path / task.project_path
            
            # ã‚³ãƒãƒ³ãƒ‰æ§‹ç¯‰
            cmd = [
                sys.executable, "-m", "scrapy", "crawl", task.spider_name,
                "-o", task.output_file,
                "-s", "FEED_FORMAT=jsonlines",
                "-s", "LOG_LEVEL=INFO"
            ]
            
            # è¨­å®šè¿½åŠ 
            for key, value in task.settings.items():
                cmd.extend(["-s", f"{key}={value}"])
            
            logger.info(f"ğŸ•·ï¸ Executing: {' '.join(cmd)}")
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=project_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            stdout, stderr = await process.communicate()
            
            return {
                "success": process.returncode == 0,
                "return_code": process.returncode,
                "stdout": stdout.decode('utf-8', errors='ignore'),
                "stderr": stderr.decode('utf-8', errors='ignore')
            }
            
        except Exception as e:
            logger.error(f"âŒ Scrapy process error: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    async def _finalize_task_stats(self, task: WatchdogTask, items_count: int):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆã®æœ€çµ‚æ›´æ–°"""
        try:
            task.status = "COMPLETED"
            task.finished_at = datetime.now().isoformat()
            task.items_count = items_count
            
            await self.redis.hset(f"task:{task.task_id}", mapping={
                "status": "COMPLETED",
                "finished_at": task.finished_at,
                "items_count": items_count
            })
            
            # å®Œäº†ã‚¤ãƒ™ãƒ³ãƒˆç™ºè¡Œ
            await self.redis.publish("events:spider_completed", json.dumps({
                "task_id": task.task_id,
                "status": "COMPLETED",
                "items_count": items_count,
                "timestamp": task.finished_at
            }))
            
            logger.info(f"âœ… Task completed: {task.task_id} ({items_count} items)")
            
        except Exception as e:
            logger.error(f"âŒ Finalization error: {e}")
    
    async def stop_task(self, task_id: str) -> bool:
        """å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã‚’åœæ­¢"""
        try:
            if task_id in self.active_monitors:
                monitor = self.active_monitors[task_id]
                monitor.stop_monitoring()
                del self.active_monitors[task_id]
                
                await self.redis.hset(f"task:{task_id}", mapping={
                    "status": "CANCELLED",
                    "finished_at": datetime.now().isoformat()
                })
                
                logger.info(f"ğŸ›‘ Task stopped: {task_id}")
                return True
            
            return False
            
        except Exception as e:
            logger.error(f"âŒ Stop task error: {e}")
            return False
    
    async def get_active_tasks(self) -> List[str]:
        """ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚¿ã‚¹ã‚¯ä¸€è¦§ã‚’å–å¾—"""
        return list(self.active_monitors.keys())
    
    async def cleanup(self):
        """ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        # å…¨ç›£è¦–åœæ­¢
        for monitor in self.active_monitors.values():
            monitor.stop_monitoring()
        
        self.active_monitors.clear()
        
        # æ¥ç¶šã‚¯ãƒ­ãƒ¼ã‚º
        if self.redis:
            await self.redis.close()
        if self.db_pool:
            await self.db_pool.close()
        
        logger.info("ğŸ§¹ Watchdog Spider Service cleanup completed")
