"""
è»½é‡ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

Rich Progress Extensionã®ä»£æ›¿ã¨ã—ã¦ã€ã‚·ãƒ³ãƒ—ãƒ«ã§å®‰å®šã—ãŸçµ±è¨ˆåé›†ã‚’æä¾›
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, Any, Optional, List
from scrapy import signals
from scrapy.http import Request, Response
from scrapy.spiders import Spider
from scrapy.exceptions import NotConfigured


class LightweightProgressExtension:
    """è»½é‡ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤ºã‚¨ã‚¯ã‚¹ãƒ†ãƒ³ã‚·ãƒ§ãƒ³"""
    
    def __init__(self, crawler):
        """åˆæœŸåŒ–"""
        self.crawler = crawler
        self.settings = crawler.settings
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿
        self.stats = {
            'requests_count': 0,
            'responses_count': 0,
            'items_count': 0,
            'errors_count': 0,
            'start_time': None,
            'last_update': None,
            'spider_name': '',
            'task_id': '',
            'status': 'STARTING'
        }
        
        # è¨­å®š
        self.task_id = (
            self.settings.get('TASK_ID', '') or
            os.environ.get('SCRAPY_TASK_ID', '') or
            f"task_{int(time.time())}"
        )
        self.update_interval = 2.0  # 2ç§’é–“éš”ã§æ›´æ–°
        self.stats_file = None

        # WebSocketç„¡åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯
        self.websocket_enabled = self.settings.getbool('LIGHTWEIGHT_PROGRESS_WEBSOCKET', True)

        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒãƒƒãƒ•ã‚¡
        self.item_buffer = []
        self.bulk_insert_size = 100  # 100ä»¶ã”ã¨ã«ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
        self.bulk_insert_enabled = self.settings.getbool('LIGHTWEIGHT_BULK_INSERT', True)

        # è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†è¨­å®š
        self.auto_file_management = self.settings.getbool('AUTO_FILE_MANAGEMENT', True)
        self.max_file_lines = self.settings.getint('MAX_JSONL_LINES', 500)
        self.keep_sessions = self.settings.getint('KEEP_SESSIONS', 1)
        self.auto_cleanup_interval = self.settings.getint('AUTO_CLEANUP_INTERVAL_HOURS', 1)

        print(f"ğŸ”§ Lightweight Progress Extension initialized for task: {self.task_id}")
        if self.auto_file_management:
            print(f"ğŸ—‚ï¸ Auto file management enabled: max_lines={self.max_file_lines}, keep_sessions={self.keep_sessions}")

    @classmethod
    def from_crawler(cls, crawler):
        """Crawlerã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        # WebSocketç„¡åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯
        if not crawler.settings.getbool('LIGHTWEIGHT_PROGRESS_WEBSOCKET', True):
            print("ğŸ”§ Lightweight Progress Extension disabled by settings")
            return None
            
        extension = cls(crawler)
        
        # ã‚·ã‚°ãƒŠãƒ«ã‚’æ¥ç¶šï¼ˆã‚·ãƒ³ãƒ—ãƒ«ãªæ¥ç¶šï¼‰
        try:
            crawler.signals.connect(extension.spider_opened, signal=signals.spider_opened)
            crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
            crawler.signals.connect(extension.request_scheduled, signal=signals.request_scheduled)
            crawler.signals.connect(extension.response_received, signal=signals.response_received)
            crawler.signals.connect(extension.item_scraped, signal=signals.item_scraped)
            crawler.signals.connect(extension.spider_error, signal=signals.spider_error)
            
            print("ğŸ”§ Lightweight Progress Extension signals connected")
        except Exception as e:
            print(f"âŒ Failed to connect Lightweight Progress Extension signals: {e}")
        
        return extension

    def spider_opened(self, spider: Spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†"""
        try:
            self.stats['spider_name'] = spider.name
            self.stats['start_time'] = time.time()
            self.stats['last_update'] = time.time()
            self.stats['status'] = 'RUNNING'
            
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
            if self.task_id:
                stats_dir = os.path.join(os.getcwd(), 'stats')
                os.makedirs(stats_dir, exist_ok=True)
                self.stats_file = os.path.join(stats_dir, f"{self.task_id}_stats.json")
            
            self._save_stats()
            print(f"ğŸ•·ï¸ Lightweight Progress started for spider: {spider.name}")
            
        except Exception as e:
            print(f"âŒ Error in spider_opened: {e}")

    def spider_closed(self, spider: Spider, reason: str):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†"""
        try:
            # æ®‹ã‚Šã®ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            if self.bulk_insert_enabled and self.item_buffer:
                print(f"ğŸ”„ Final bulk insert: {len(self.item_buffer)} items")
                self._perform_bulk_insert()

            self.stats['status'] = 'COMPLETED' if reason == 'finished' else 'FAILED'
            self.stats['last_update'] = time.time()

            # æœ€çµ‚çµ±è¨ˆã‚’ä¿å­˜
            self._save_stats()

            # è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚’å®Ÿè¡Œ
            if self.auto_file_management:
                self._perform_auto_file_management()

            # çµ±è¨ˆã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
            elapsed = time.time() - self.stats['start_time'] if self.stats['start_time'] else 0
            print(f"ğŸ Lightweight Progress completed:")
            print(f"   ğŸ“¤ Requests: {self.stats['requests_count']}")
            print(f"   ğŸ“¥ Responses: {self.stats['responses_count']}")
            print(f"   ğŸ“¦ Items: {self.stats['items_count']}")
            print(f"   âŒ Errors: {self.stats['errors_count']}")
            print(f"   â±ï¸ Duration: {elapsed:.1f}s")
            print(f"   ğŸ¯ Status: {self.stats['status']}")
            
        except Exception as e:
            print(f"âŒ Error in spider_closed: {e}")

    def request_scheduled(self, request: Request, spider: Spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚ã®å‡¦ç†"""
        try:
            self.stats['requests_count'] += 1
            self._update_if_needed()
        except Exception as e:
            print(f"âŒ Error in request_scheduled: {e}")

    def response_received(self, response: Response, request: Request, spider: Spider):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡æ™‚ã®å‡¦ç†"""
        try:
            self.stats['responses_count'] += 1
            self._update_if_needed()
        except Exception as e:
            print(f"âŒ Error in response_received: {e}")

    def item_scraped(self, item: Dict[str, Any], response: Response, spider: Spider):
        """ã‚¢ã‚¤ãƒ†ãƒ å–å¾—æ™‚ã®å‡¦ç†ï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå¯¾å¿œï¼‰"""
        try:
            self.stats['items_count'] += 1

            # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆæ©Ÿèƒ½
            if self.bulk_insert_enabled:
                # ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ 
                item_data = dict(item)
                item_data['scraped_at'] = datetime.now().isoformat()
                item_data['task_id'] = self.task_id
                item_data['spider_name'] = spider.name
                self.item_buffer.append(item_data)

                # ãƒãƒƒãƒ•ã‚¡ãŒæº€æ¯ã«ãªã£ãŸã‚‰ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
                if len(self.item_buffer) >= self.bulk_insert_size:
                    self._perform_bulk_insert()

            self._update_if_needed()
        except Exception as e:
            print(f"âŒ Error in item_scraped: {e}")

    def spider_error(self, failure, response: Response, spider: Spider):
        """ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†"""
        try:
            self.stats['errors_count'] += 1
            self._update_if_needed()
        except Exception as e:
            print(f"âŒ Error in spider_error: {e}")

    def _update_if_needed(self):
        """å¿…è¦ã«å¿œã˜ã¦çµ±è¨ˆã‚’æ›´æ–°"""
        try:
            current_time = time.time()
            if (current_time - self.stats['last_update']) >= self.update_interval:
                self.stats['last_update'] = current_time
                self._save_stats()
                
                # ç°¡å˜ãªé€²æ—è¡¨ç¤º
                elapsed = current_time - self.stats['start_time'] if self.stats['start_time'] else 0
                rate = self.stats['items_count'] / elapsed if elapsed > 0 else 0
                print(f"ğŸ“Š Progress: R:{self.stats['requests_count']} | "
                      f"Res:{self.stats['responses_count']} | "
                      f"I:{self.stats['items_count']} | "
                      f"E:{self.stats['errors_count']} | "
                      f"Rate:{rate:.2f}/s")
                
        except Exception as e:
            print(f"âŒ Error in _update_if_needed: {e}")

    def _save_stats(self):
        """çµ±è¨ˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            if self.stats_file:
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
                stats_with_timestamp = self.stats.copy()
                stats_with_timestamp['timestamp'] = datetime.now().isoformat()
                
                # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                with open(self.stats_file, 'w', encoding='utf-8') as f:
                    json.dump(stats_with_timestamp, f, ensure_ascii=False, indent=2)
                
                # WebSocketé€ä¿¡ï¼ˆæœ‰åŠ¹ãªå ´åˆï¼‰
                if self.websocket_enabled:
                    self._send_websocket_update(stats_with_timestamp)
                    
        except Exception as e:
            print(f"âŒ Error saving stats: {e}")

    def _send_websocket_update(self, stats: Dict[str, Any]):
        """WebSocketçµŒç”±ã§çµ±è¨ˆã‚’é€ä¿¡"""
        try:
            # WebSocketé€ä¿¡ã®å®Ÿè£…ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
            # ç¾åœ¨ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã®ã¿
            pass
        except Exception as e:
            print(f"âŒ Error sending WebSocket update: {e}")

    def _perform_bulk_insert(self):
        """ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚’å®Ÿè¡Œ"""
        try:
            if not self.item_buffer:
                return

            print(f"ğŸ“¦ Performing bulk insert: {len(self.item_buffer)} items")

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã®å–å¾—
            try:
                import sys
                import uuid
                import hashlib
                sys.path.append('/home/igtmtakan/workplace/python/scrapyUI/backend')
                from app.database import SessionLocal, Result as DBResult

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—
                db = SessionLocal()

                # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                bulk_data = []
                skipped_duplicates = 0

                for item_data in self.item_buffer:
                    # æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
                    data_hash = self._generate_improved_data_hash(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ã‚¿ã‚¹ã‚¯å†…ï¼‰
                    existing = db.query(DBResult).filter(
                        DBResult.task_id == self.task_id,
                        DBResult.data_hash == data_hash
                    ).first()

                    if existing:
                        skipped_duplicates += 1
                        print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                        continue

                    result_data = DBResult(
                        id=str(uuid.uuid4()),
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )
                    bulk_data.append(result_data)

                # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ
                if bulk_data:
                    db.bulk_save_objects(bulk_data)
                    db.commit()
                    print(f"âœ… Bulk insert completed: {len(bulk_data)} items")
                    if skipped_duplicates > 0:
                        print(f"âš ï¸ Skipped {skipped_duplicates} duplicate items")
                else:
                    print("âš ï¸ No new items to insert (all duplicates)")

                db.close()

                # ãƒãƒƒãƒ•ã‚¡ã‚’ã‚¯ãƒªã‚¢
                self.item_buffer.clear()

            except Exception as db_error:
                print(f"âŒ Database bulk insert error: {db_error}")
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
                self._save_items_to_file()

        except Exception as e:
            print(f"âŒ Error in bulk insert: {e}")

    def _save_items_to_file(self):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹éšœå®³æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        try:
            if not self.item_buffer:
                return

            backup_file = f"backup_items_{self.task_id}_{int(time.time())}.jsonl"
            with open(backup_file, 'w', encoding='utf-8') as f:
                for item in self.item_buffer:
                    f.write(json.dumps(item, ensure_ascii=False) + '\n')

            print(f"ğŸ’¾ Items saved to backup file: {backup_file}")
            self.item_buffer.clear()

        except Exception as e:
            print(f"âŒ Error saving items to file: {e}")

    def _generate_improved_data_hash(self, item_data: Dict[str, Any]) -> str:
        """æ”¹è‰¯ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆé‡è¤‡é˜²æ­¢å¼·åŒ–ï¼‰"""
        try:
            import hashlib

            # é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’ä½¿ç”¨ã—ã¦ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
            key_fields = ['title', 'product_url', 'ranking_position', 'price', 'rating']
            hash_data = {}

            for field in key_fields:
                if field in item_data and item_data[field] is not None:
                    hash_data[field] = str(item_data[field]).strip()

            # URLã‹ã‚‰ASINã‚’æŠ½å‡ºã—ã¦ãƒãƒƒã‚·ãƒ¥ã«å«ã‚ã‚‹
            product_url = item_data.get('product_url', '')
            if '/dp/' in product_url:
                asin = product_url.split('/dp/')[1].split('/')[0]
                hash_data['asin'] = asin

            # ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¾æ›¸ã‹ã‚‰æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
            data_str = str(sorted(hash_data.items()))
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()

        except Exception as e:
            print(f"âŒ Error generating data hash: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
            import hashlib
            data_str = str(sorted(item_data.items()))
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()

    def _perform_auto_file_management(self):
        """è‡ªå‹•ãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ã‚’å®Ÿè¡Œ"""
        try:
            print("ğŸ—‚ï¸ Starting auto file management...")

            # ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
            import glob
            jsonl_files = glob.glob("*.jsonl")

            for jsonl_file in jsonl_files:
                self._manage_jsonl_file(jsonl_file)

        except Exception as e:
            print(f"âŒ Error in auto file management: {e}")

    def _manage_jsonl_file(self, jsonl_file):
        """å€‹åˆ¥ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç®¡ç†"""
        try:
            import os
            from pathlib import Path

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨è¡Œæ•°ã‚’ãƒã‚§ãƒƒã‚¯
            if not os.path.exists(jsonl_file):
                return

            line_count = self._count_file_lines(jsonl_file)
            file_size = os.path.getsize(jsonl_file)

            print(f"ğŸ“„ Checking {jsonl_file}: {line_count:,} lines, {file_size:,} bytes")

            # è¡Œæ•°ãŒä¸Šé™ã‚’è¶…ãˆã¦ã„ã‚‹å ´åˆã€ã¾ãŸã¯å¸¸ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            if line_count > self.max_file_lines:
                print(f"ğŸ§¹ File {jsonl_file} exceeds max lines ({line_count:,} > {self.max_file_lines:,})")
                self._cleanup_jsonl_file(jsonl_file)
            elif line_count > 100:  # 100è¡Œã‚’è¶…ãˆãŸã‚‰å¸¸ã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                print(f"ğŸ§¹ File {jsonl_file} cleanup triggered ({line_count:,} lines)")
                self._cleanup_jsonl_file(jsonl_file)
            else:
                print(f"âœ… File {jsonl_file} is within limits")

        except Exception as e:
            print(f"âŒ Error managing file {jsonl_file}: {e}")

    def _count_file_lines(self, file_path):
        """ãƒ•ã‚¡ã‚¤ãƒ«ã®è¡Œæ•°ã‚’åŠ¹ç‡çš„ã«ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return sum(1 for _ in f)
        except Exception:
            return 0

    def _cleanup_jsonl_file(self, jsonl_file):
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            import subprocess
            import sys

            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ç®¡ç†ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            backend_path = '/home/igtmtakan/workplace/python/scrapyUI/backend'
            tool_path = os.path.join(backend_path, 'jsonl_file_manager.py')

            if os.path.exists(tool_path):
                cmd = [
                    sys.executable, tool_path, jsonl_file,
                    '--clean', '--keep-sessions', str(self.keep_sessions)
                ]

                print(f"ğŸ”§ Running cleanup: {' '.join(cmd)}")
                result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)

                if result.returncode == 0:
                    print(f"âœ… Successfully cleaned up {jsonl_file}")
                    print(result.stdout)
                else:
                    print(f"âŒ Cleanup failed for {jsonl_file}: {result.stderr}")
            else:
                print(f"âš ï¸ Cleanup tool not found: {tool_path}")

        except Exception as e:
            print(f"âŒ Error cleaning up {jsonl_file}: {e}")
