"""
ScrapyUI ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒæŠ½å‡ºã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è‡ªå‹•çš„ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
"""
import os
import sys
import json
import uuid
from datetime import datetime
from typing import Dict, Any, Optional
from pathlib import Path

# ScrapyUIã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒ‘ã‚¹ã‚’è¿½åŠ 
scrapy_ui_backend = Path(__file__).parent.parent.parent
sys.path.insert(0, str(scrapy_ui_backend))

try:
    from sqlalchemy import create_engine
    from sqlalchemy.orm import sessionmaker
    from app.database import Result as DBResult
    from app.database import Base
except ImportError as e:
    print(f"âš ï¸ ScrapyUIãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ­ã‚°å‡ºåŠ›ã®ã¿
    DBResult = None
    Base = None


class ScrapyUIDatabasePipeline:
    """
    ScrapyUI ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒæŠ½å‡ºã—ãŸã‚¢ã‚¤ãƒ†ãƒ ã‚’è‡ªå‹•çš„ã«ScrapyUIã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ã—ã¾ã™ã€‚
    """
    
    def __init__(self, database_url: str = None, task_id: str = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            database_url: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URL
            task_id: ã‚¿ã‚¹ã‚¯ID
        """
        self.database_url = database_url
        self.task_id = task_id
        self.engine = None
        self.session_factory = None
        self.session = None
        self.items_saved = 0
        self.crawl_start_datetime = None
        
    @classmethod
    def from_crawler(cls, crawler):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
        
        Args:
            crawler: Scrapyã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            
        Returns:
            ScrapyUIDatabasePipeline: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        # è¨­å®šã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URLã¨ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—
        database_url = crawler.settings.get('SCRAPYUI_DATABASE_URL')
        task_id = crawler.settings.get('SCRAPYUI_TASK_ID')
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚‚å–å¾—ã‚’è©¦è¡Œ
        if not database_url:
            database_url = os.getenv('SCRAPYUI_DATABASE_URL')
        if not task_id:
            task_id = os.getenv('SCRAPYUI_TASK_ID')
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URL
        if not database_url:
            db_path = scrapy_ui_backend / "database" / "scrapy_ui.db"
            database_url = f"sqlite:///{db_path}"
        
        return cls(database_url=database_url, task_id=task_id)
    
    def open_spider(self, spider):
        """
        ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†
        
        Args:
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        try:
            # ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¹ã‚¿ãƒ¼ãƒˆæ—¥æ™‚ã‚’è¨˜éŒ²
            self.crawl_start_datetime = datetime.now().isoformat()
            
            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¹ã‚¿ãƒ¼ãƒˆæ—¥æ™‚ã‚’è¨­å®š
            if hasattr(spider, 'crawl_start_datetime'):
                spider.crawl_start_datetime = self.crawl_start_datetime
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚’åˆæœŸåŒ–
            if DBResult and Base:
                self.engine = create_engine(self.database_url)
                Base.metadata.create_all(self.engine)
                self.session_factory = sessionmaker(bind=self.engine)
                self.session = self.session_factory()
                
                spider.logger.info(f"âœ… ScrapyUI ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
                spider.logger.info(f"   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹URL: {self.database_url}")
                spider.logger.info(f"   ã‚¿ã‚¹ã‚¯ID: {self.task_id}")
                spider.logger.info(f"   ã‚¯ãƒ­ãƒ¼ãƒ«ã‚¹ã‚¿ãƒ¼ãƒˆ: {self.crawl_start_datetime}")
            else:
                spider.logger.warning("âš ï¸ ScrapyUIãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                
        except Exception as e:
            spider.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
    
    def close_spider(self, spider):
        """
        ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†
        
        Args:
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        try:
            if self.session:
                self.session.close()
                
            spider.logger.info(f"âœ… ScrapyUI ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†")
            spider.logger.info(f"   ä¿å­˜ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.items_saved}ä»¶")
            
        except Exception as e:
            spider.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_item(self, item, spider):
        """
        ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†
        
        Args:
            item: æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ 
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            
        Returns:
            item: å‡¦ç†ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ 
        """
        try:
            if not self.session or not DBResult:
                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãã®ã¾ã¾è¿”ã™
                return item
            
            # ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¾æ›¸ã«å¤‰æ›
            if hasattr(item, '_values'):
                # Scrapy Itemã®å ´åˆ
                item_dict = dict(item)
            elif isinstance(item, dict):
                # è¾æ›¸ã®å ´åˆ
                item_dict = item.copy()
            else:
                # ãã®ä»–ã®å ´åˆã¯æ–‡å­—åˆ—åŒ–
                item_dict = {"data": str(item)}
            
            # æ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
            current_time = datetime.now().isoformat()
            item_dict['crawl_start_datetime'] = self.crawl_start_datetime
            item_dict['item_acquired_datetime'] = current_time
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
            db_result = DBResult(
                id=str(uuid.uuid4()),
                task_id=self.task_id,
                data=item_dict,
                url=item_dict.get('url', ''),
                created_at=datetime.now(),
                crawl_start_datetime=datetime.fromisoformat(self.crawl_start_datetime) if self.crawl_start_datetime else None,
                item_acquired_datetime=datetime.fromisoformat(current_time)
            )
            
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            self.session.add(db_result)
            self.session.commit()
            
            self.items_saved += 1
            
            # å®šæœŸçš„ã«ãƒ­ã‚°å‡ºåŠ›
            if self.items_saved % 10 == 0:
                spider.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜æ¸ˆã¿: {self.items_saved}ä»¶")
            
            return item
            
        except Exception as e:
            spider.logger.error(f"âŒ ã‚¢ã‚¤ãƒ†ãƒ ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
            spider.logger.error(f"   ã‚¢ã‚¤ãƒ†ãƒ : {item}")
            
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ã‚¢ã‚¤ãƒ†ãƒ ã¯è¿”ã™
            return item


class ScrapyUIJSONPipeline:
    """
    ScrapyUI JSONå‡ºåŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³
    
    ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜ã¨ä¸¦è¡Œã—ã¦JSONLå½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚‚è¡Œã„ã¾ã™ã€‚
    """
    
    def __init__(self, file_path: str = None):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            file_path: å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.file_path = file_path
        self.file = None
        self.items_exported = 0
    
    @classmethod
    def from_crawler(cls, crawler):
        """
        ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‹ã‚‰ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
        
        Args:
            crawler: Scrapyã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            
        Returns:
            ScrapyUIJSONPipeline: ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        # è¨­å®šã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        file_path = crawler.settings.get('SCRAPYUI_JSON_FILE')
        
        # ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ã‚‚å–å¾—ã‚’è©¦è¡Œ
        if not file_path:
            file_path = os.getenv('SCRAPYUI_JSON_FILE')
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        if not file_path:
            task_id = crawler.settings.get('SCRAPYUI_TASK_ID', 'unknown')
            file_path = f"results_{task_id}.jsonl"
        
        return cls(file_path=file_path)
    
    def open_spider(self, spider):
        """
        ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†
        
        Args:
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        try:
            self.file = open(self.file_path, 'w', encoding='utf-8')
            spider.logger.info(f"âœ… JSONå‡ºåŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹: {self.file_path}")
        except Exception as e:
            spider.logger.error(f"âŒ JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«é–‹å§‹ã‚¨ãƒ©ãƒ¼: {e}")
    
    def close_spider(self, spider):
        """
        ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†
        
        Args:
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        """
        try:
            if self.file:
                self.file.close()
            spider.logger.info(f"âœ… JSONå‡ºåŠ›ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ‚äº†: {self.items_exported}ä»¶å‡ºåŠ›")
        except Exception as e:
            spider.logger.error(f"âŒ JSONå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«çµ‚äº†ã‚¨ãƒ©ãƒ¼: {e}")
    
    def process_item(self, item, spider):
        """
        ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†
        
        Args:
            item: æŠ½å‡ºã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ 
            spider: Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            
        Returns:
            item: å‡¦ç†ã•ã‚ŒãŸã‚¢ã‚¤ãƒ†ãƒ 
        """
        try:
            if self.file:
                # ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¾æ›¸ã«å¤‰æ›
                if hasattr(item, '_values'):
                    item_dict = dict(item)
                elif isinstance(item, dict):
                    item_dict = item
                else:
                    item_dict = {"data": str(item)}
                
                # JSONLå½¢å¼ã§å‡ºåŠ›
                line = json.dumps(item_dict, ensure_ascii=False) + '\n'
                self.file.write(line)
                self.file.flush()
                
                self.items_exported += 1
            
            return item
            
        except Exception as e:
            spider.logger.error(f"âŒ JSONå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")
            return item
