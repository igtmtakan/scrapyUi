#!/usr/bin/env python3
import requests
import json

# APIãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://localhost:8000"

def login():
    """ãƒ­ã‚°ã‚¤ãƒ³"""
    login_data = {
        'email': 'admin@scrapyui.com',
        'password': 'admin123456'
    }

    print('ğŸ” ãƒ­ã‚°ã‚¤ãƒ³ä¸­...')
    response = requests.post(
        f'{BASE_URL}/api/auth/login',
        json=login_data,
        headers={'Content-Type': 'application/json'}
    )

    if response.status_code == 200:
        result = response.json()
        token = result['access_token']
        print('âœ… ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ')
        return token
    else:
        print(f'âŒ ãƒ­ã‚°ã‚¤ãƒ³å¤±æ•—: {response.status_code}')
        print(response.text)
        return None

def create_project(token):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã¾ãŸã¯æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå–å¾—"""
    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}

    # ã¾ãšæ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç¢ºèª
    print('ğŸ“ æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ç¢ºèªä¸­...')
    projects_response = requests.get(f'{BASE_URL}/api/projects/', headers=headers)

    if projects_response.status_code == 200:
        projects = projects_response.json()
        for project in projects:
            if project['name'] == 'admin_omocha20':
                print(f'âœ… æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨: {project["id"]}')
                print(f'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project["name"]}')
                print(f'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {project["path"]}')
                return project

    # æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
    project_data = {
        'name': 'admin_omocha20',
        'description': 'Amazon software bestsellers scraping project',
        'settings': {}
    }

    print('ğŸ“ æ–°è¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆä¸­...')
    response = requests.post(
        f'{BASE_URL}/api/projects/',
        json=project_data,
        headers=headers
    )

    if response.status_code == 201:
        project = response.json()
        print(f'âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆæˆåŠŸ: {project["id"]}')
        print(f'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå: {project["name"]}')
        print(f'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {project["path"]}')
        return project
    else:
        print(f'âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆå¤±æ•—: {response.status_code}')
        print(response.text)
        return None

def create_spider(token, project_id):
    """omocha20ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆã¾ãŸã¯æ—¢å­˜ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å–å¾—"""
    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}

    # ã¾ãšæ—¢å­˜ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèª
    print('ğŸ•·ï¸ æ—¢å­˜ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ç¢ºèªä¸­...')
    spiders_response = requests.get(f'{BASE_URL}/api/spiders/?project_id={project_id}', headers=headers)

    if spiders_response.status_code == 200:
        spiders = spiders_response.json()
        for spider in spiders:
            if spider['name'] == 'omocha20_jsonl':
                print(f'âœ… æ—¢å­˜ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨: {spider["id"]}')
                print(f'ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å: {spider["name"]}')
                return spider

    # æ—¢å­˜ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒãªã„å ´åˆã¯æ–°è¦ä½œæˆ
    print('ğŸ•·ï¸ æ–°è¦ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆä¸­...')
    spider_code = '''import scrapy
import re
import urllib.parse

class Omocha20JsonlSpider(scrapy.Spider):
    name = "omocha20_jsonl"
    allowed_domains = ["amazon.co.jp"]
    start_urls = ["https://www.amazon.co.jp/gp/bestsellers/software/ref=zg_bs_nav_software_0"]

    custom_settings = {
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 2,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
    }

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url=url,
                callback=self.parse
            )

    def parse(self, response):
        """ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰å•†å“ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º"""
        # å•†å“ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        product_links = response.css('a.a-link-normal.aok-block::attr(href)').getall()

        for link in product_links:
            if link and '/dp/' in link:
                full_url = urllib.parse.urljoin(response.url, link)
                yield scrapy.Request(
                    url=full_url,
                    callback=self.parse_product
                )

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
        next_page_links = response.css('ul.a-pagination li.a-last a::attr(href)').getall()
        if not next_page_links:
            next_page_links = response.css('a[aria-label="æ¬¡ã¸"]::attr(href)').getall()

        for next_link in next_page_links:
            if next_link:
                next_url = urllib.parse.urljoin(response.url, next_link)
                yield scrapy.Request(
                    url=next_url,
                    callback=self.parse
                )

    def parse_product(self, response):
        """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡º"""
        # ã‚¿ã‚¤ãƒˆãƒ«
        title = response.css('#title span::text').get()
        if not title:
            title = response.css('#productTitle::text').get()

        # è©•ä¾¡
        rating = response.css('span.a-icon-alt::text').re_first(r'5ã¤æ˜Ÿã®ã†ã¡([\\d.]+)')
        if not rating:
            rating = response.css('[data-hook="rating-out-of-text"]::text').re_first(r'([\\d.]+)')

        # ä¾¡æ ¼
        price = response.css('.a-price-whole::text').get()
        if not price:
            price = response.css('.a-offscreen::text').get()

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°
        reviews = response.css('[data-hook="total-review-count"]::text').get()
        if not reviews:
            reviews = response.css('a[href*="#customerReviews"] span::text').get()

        # ç”»åƒURL
        image_url = response.css('img.a-dynamic-image.a-stretch-vertical::attr(src)').get()
        if not image_url:
            image_url = response.css('#landingImage::attr(src)').get()

        yield {
            'url': response.url,
            'title': title.strip() if title else None,
            'rating': rating,
            'price': price.strip() if price else None,
            'reviews': reviews.strip() if reviews else None,
            'image_url': image_url
        }
'''

    spider_data = {
        'name': 'omocha20_jsonl',
        'code': spider_code,
        'project_id': project_id,
        'template': 'custom',
        'settings': {}
    }

    print('ğŸ•·ï¸ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆä¸­...')
    response = requests.post(
        f'{BASE_URL}/api/spiders/',
        json=spider_data,
        headers=headers
    )

    if response.status_code == 201:
        spider = response.json()
        print(f'âœ… ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆæˆåŠŸ: {spider["id"]}')
        print(f'ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å: {spider["name"]}')
        return spider
    else:
        print(f'âŒ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆå¤±æ•—: {response.status_code}')
        print(response.text)
        return None

def run_spider(token, project_id, spider_id):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œï¼ˆã‚¿ã‚¹ã‚¯ä½œæˆï¼‰"""
    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}

    run_data = {
        'project_id': project_id,
        'spider_id': spider_id,
        'log_level': 'INFO',
        'settings': {
            'FEED_EXPORT_ENCODING': 'utf-8',
            'FEEDS': {
                'results.jsonl': {
                    'format': 'jsonl',
                    'encoding': 'utf8',
                    'store_empty': False,
                    'item_export_kwargs': {
                        'ensure_ascii': False
                    }
                }
            }
        }
    }

    print('ğŸš€ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œä¸­ï¼ˆã‚¿ã‚¹ã‚¯ä½œæˆï¼‰...')
    response = requests.post(
        f'{BASE_URL}/api/tasks/',
        json=run_data,
        headers=headers
    )

    if response.status_code in [200, 201, 202]:
        result = response.json()
        task_id = result.get("id") or result.get("task_id")
        print(f'âœ… ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: ã‚¿ã‚¹ã‚¯ID {task_id}')
        print(f'ğŸ“Š ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get("status", "UNKNOWN")}')
        return result
    else:
        print(f'âŒ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œå¤±æ•—: {response.status_code}')
        print(response.text)
        return None

def main():
    # ãƒ­ã‚°ã‚¤ãƒ³
    token = login()
    if not token:
        return

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆ
    project = create_project(token)
    if not project:
        return

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆ
    spider = create_spider(token, project['id'])
    if not spider:
        return

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
    result = run_spider(token, project['id'], spider['id'])
    if result:
        task_id = result.get("id") or result.get("task_id")
        print(f'ğŸ‰ omocha20ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãŒæ­£å¸¸ã«ä½œæˆãƒ»å®Ÿè¡Œã•ã‚Œã¾ã—ãŸï¼')
        print(f'ã‚¿ã‚¹ã‚¯ID: {task_id}')
        print(f'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID: {project["id"]}')
        print(f'ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ID: {spider["id"]}')
        print(f'ğŸ“Š å®Ÿè¡Œã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {result.get("status", "UNKNOWN")}')
        print(f'ğŸŒ WebUI: http://localhost:4000/projects/{project["id"]}/spiders/{spider["id"]}/edit')

if __name__ == "__main__":
    main()
