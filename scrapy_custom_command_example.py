#!/usr/bin/env python3
"""
Scrapyã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒãƒ³ãƒ‰ã§watchdogç›£è¦–ã‚’è‡ªå‹•é–‹å§‹ã™ã‚‹ä¾‹
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® management/commands/ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®
"""
import asyncio
import threading
import time
from scrapy.commands import ScrapyCommand
from scrapy.utils.conf import arglist_to_dict
from scrapy.exceptions import UsageError

class Command(ScrapyCommand):
    """watchdogç›£è¦–ä»˜ãcrawlã‚³ãƒãƒ³ãƒ‰"""
    
    requires_project = True
    default_settings = {'LOG_LEVEL': 'INFO'}

    def syntax(self):
        return "<spider> [options]"

    def short_desc(self):
        return "Run a spider with watchdog monitoring"

    def add_options(self, parser):
        ScrapyCommand.add_options(self, parser)
        parser.add_option("-o", "--output", dest="output",
                         help="dump scraped items to FILE (use - for stdout)")
        parser.add_option("-t", "--output-format", dest="output_format",
                         help="format to use for dumping items")
        parser.add_option("--task-id", dest="task_id",
                         help="task ID for monitoring")
        parser.add_option("--db-path", dest="db_path", 
                         default="backend/database/scrapy_ui.db",
                         help="database path for storing results")

    def process_options(self, args, opts):
        ScrapyCommand.process_options(self, args, opts)
        try:
            opts.spargs, opts.spkwargs = arglist_to_dict(args[1:])
        except ValueError:
            raise UsageError("Invalid -a value, use -a NAME=VALUE", print_help=False)

    def run(self, args, opts):
        if len(args) < 1:
            raise UsageError()

        spider_name = args[0]
        task_id = opts.task_id or f"custom_{int(time.time())}"
        
        print(f"ğŸš€ Starting spider with watchdog monitoring: {spider_name}")
        print(f"ğŸ“‹ Task ID: {task_id}")
        
        # watchdogç›£è¦–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
        monitor_thread = threading.Thread(
            target=self._start_watchdog_monitoring,
            args=(task_id, opts.output, opts.db_path),
            daemon=True
        )
        monitor_thread.start()
        
        # é€šå¸¸ã®Scrapyã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œ
        self.crawler_process.crawl(spider_name, **opts.spkwargs)
        self.crawler_process.start()

    def _start_watchdog_monitoring(self, task_id, output_file, db_path):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        try:
            import sys
            import os
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'backend', 'app', 'services'))
            
            from scrapy_watchdog_monitor import ScrapyWatchdogMonitor
            
            # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
            monitor = ScrapyWatchdogMonitor(
                task_id=task_id,
                project_path=os.getcwd(),
                spider_name="unknown",  # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã¯å¾Œã§è¨­å®š
                db_path=db_path
            )
            
            # éåŒæœŸç›£è¦–ã‚’é–‹å§‹
            asyncio.run(monitor._start_jsonl_monitoring(output_file))
            
        except Exception as e:
            print(f"âŒ Watchdog monitoring error: {e}")


# ä½¿ç”¨ä¾‹:
# scrapy crawlwithwatchdog spider_name -o results.jsonl --task-id=test_123
