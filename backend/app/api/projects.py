from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List
import uuid
import os
from datetime import datetime

from ..database import get_db, Project as DBProject, Spider as DBSpider, User as DBUser, UserRole
from ..models.schemas import Project, ProjectCreate, ProjectUpdate, ProjectWithSpiders, ProjectWithUser, Spider, SpiderCreate
from ..services.scrapy_service import ScrapyPlaywrightService
from ..api.auth import get_current_active_user

# ãƒ­ã‚®ãƒ³ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..utils.logging_config import get_logger, log_with_context, log_exception
from ..utils.error_handler import (
    ScrapyUIException,
    ProjectException,
    ResourceNotFoundException,
    AuthorizationException,
    ErrorCode,
    handle_exception
)

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = get_logger(__name__)

router = APIRouter(
    responses={
        404: {"description": "Not found"},
        400: {"description": "Bad request"},
        500: {"description": "Internal server error"}
    }
)


def sync_project_files_to_database(db, project_id: str, project_path: str, user_id: str):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆæ™‚ã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸï¼ˆå®Œå…¨ã‚¹ã‚­ãƒ£ãƒ³ç‰ˆï¼‰"""
    from ..database import ProjectFile
    from pathlib import Path
    from datetime import datetime
    import os

    # çµ¶å¯¾ãƒ‘ã‚¹ã§ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æŒ‡å®š
    # backend/app/api/projects.py ã‹ã‚‰ backend/ ã¾ã§2ã¤ä¸ŠãŒã£ã¦ã€ã•ã‚‰ã«1ã¤ä¸ŠãŒã£ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
    base_dir = Path(__file__).parent.parent.parent.parent  # backend/app/api/ ã‹ã‚‰4ã¤ä¸ŠãŒã£ã¦ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
    scrapy_projects_dir = base_dir / "scrapy_projects"

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆScrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ§‹é€ ã«å¯¾å¿œï¼‰
    # scrapy_projects/project_name/project_name/ ã®å½¢å¼
    project_dir = scrapy_projects_dir / project_path / project_path

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¤ã„å½¢å¼ã‚‚è©¦ã™
    if not project_dir.exists():
        project_dir = scrapy_projects_dir / project_path

    if not project_dir.exists():
        logger.warning(f"Project directory not found: {project_dir}")
        logger.info(f"   Checked paths:")
        logger.info(f"     - {scrapy_projects_dir / project_path / project_path}")
        logger.info(f"     - {scrapy_projects_dir / project_path}")
        logger.info(f"   Base directory: {base_dir}")
        logger.info(f"   Scrapy projects directory: {scrapy_projects_dir}")
        logger.info(f"   Current working directory: {os.getcwd()}")
        return

    logger.info(f"ğŸ” Starting complete file sync for project: {project_path}")
    logger.info(f"   Project directory: {project_dir}")

    synced_files = []

    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å†å¸°çš„ã«æ¤œç´¢
        for file_path in project_dir.rglob("*"):
            if file_path.is_file():
                # ç›¸å¯¾ãƒ‘ã‚¹ã‚’è¨ˆç®—
                relative_path = file_path.relative_to(project_dir)
                relative_path_str = str(relative_path).replace("\\", "/")  # Windowså¯¾å¿œ

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                if file_path.suffix == ".py":
                    file_type = "python"
                elif file_path.suffix == ".cfg":
                    file_type = "config"
                elif file_path.suffix in [".txt", ".md", ".rst"]:
                    file_type = "text"
                elif file_path.suffix in [".json", ".yaml", ".yml"]:
                    file_type = "config"
                else:
                    file_type = "other"

                logger.info(f"   ğŸ“„ Processing file: {relative_path_str} (type: {file_type})")

                try:
                    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()

                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    existing_file = db.query(ProjectFile).filter(
                        ProjectFile.project_id == project_id,
                        ProjectFile.path == relative_path_str
                    ).first()

                    if existing_file:
                        # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                        existing_file.content = content
                        existing_file.updated_at = datetime.now()
                        logger.info(f"      âœ… Updated existing file in database")
                    else:
                        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                        db_file = ProjectFile(
                            id=str(uuid.uuid4()),
                            name=file_path.name,
                            path=relative_path_str,
                            content=content,
                            file_type=file_type,
                            project_id=project_id,
                            user_id=user_id,
                            created_at=datetime.now(),
                            updated_at=datetime.now()
                        )
                        db.add(db_file)
                        logger.info(f"      âœ… Added new file to database")

                    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚³ãƒŸãƒƒãƒˆ
                    try:
                        db.commit()
                        synced_files.append(relative_path_str)
                        logger.info(f"      ğŸ’¾ Committed to database")
                    except Exception as commit_error:
                        db.rollback()
                        logger.error(f"      âŒ Failed to commit file: {str(commit_error)}")

                except UnicodeDecodeError:
                    # ãƒã‚¤ãƒŠãƒªãƒ•ã‚¡ã‚¤ãƒ«ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    logger.info(f"      â­ï¸ Skipped binary file: {relative_path_str}")
                except Exception as e:
                    logger.error(f"      âŒ Failed to process file: {str(e)}")

    except Exception as e:
        logger.error(f"âŒ Failed to scan project directory: {str(e)}")

    # æœ€çµ‚çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"âœ… Successfully synced {len(synced_files)} files to database")
    logger.info(f"   Synced files: {synced_files}")

    # ç‰¹åˆ¥ã«commandsé–¢é€£ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèª
    commands_files = [f for f in synced_files if 'commands' in f]
    if commands_files:
        logger.info(f"ğŸ”§ Commands files synced: {commands_files}")
    else:
        logger.warning(f"âš ï¸ No commands files found in synced files")

    # settings.pyã®ç¢ºèª
    settings_files = [f for f in synced_files if f.endswith('settings.py')]
    if settings_files:
        logger.info(f"âš™ï¸ Settings files synced: {settings_files}")
    else:
        logger.warning(f"âš ï¸ No settings.py found in synced files")


def sync_spider_file_to_database(db, project_id: str, project_path: str, spider_name: str, spider_code: str, user_id: str):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
    from ..database import ProjectFile
    from datetime import datetime

    try:
        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        spider_file_path = f"{project_path}/spiders/{spider_name}.py"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        existing_file = db.query(ProjectFile).filter(
            ProjectFile.project_id == project_id,
            ProjectFile.path == spider_file_path
        ).first()

        if existing_file:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            existing_file.content = spider_code
            existing_file.updated_at = datetime.now()
            logger.info(f"Updated spider file in database: {spider_file_path}")
        else:
            # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            db_file = ProjectFile(
                id=str(uuid.uuid4()),
                name=f"{spider_name}.py",
                path=spider_file_path,
                content=spider_code,
                file_type="python",
                project_id=project_id,
                user_id=user_id
            )
            db.add(db_file)
            logger.info(f"Added spider file to database: {spider_file_path}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆ
        db.commit()
        logger.info(f"Successfully synced spider file to database: {spider_name}")

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to sync spider file to database: {str(e)}")
        raise







@router.get(
    "/",
    response_model=List[ProjectWithUser],
    summary="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—",
    description="ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ"
)
async def get_projects(
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """
    ## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—

    ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    try:
        # ç®¡ç†è€…ã¯å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿
        # LEFT JOINã‚’ä½¿ç”¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‰Šé™¤ã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚‚å–å¾—
        if current_user.role == UserRole.ADMIN or current_user.role == "admin" or current_user.role == "ADMIN":
            projects = db.query(DBProject).outerjoin(DBUser).filter(DBProject.is_active == True).all()
        else:
            projects = db.query(DBProject).outerjoin(DBUser).filter(
                DBProject.user_id == current_user.id,
                DBProject.is_active == True
            ).all()

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
        result = []
        for project in projects:
            try:
                project_dict = {
                    "id": project.id,
                    "name": project.name,
                    "description": project.description,
                    "path": project.path,
                    "scrapy_version": project.scrapy_version,
                    "settings": project.settings or {},
                    "db_save_enabled": project.db_save_enabled,
                    "created_at": project.created_at,
                    "updated_at": project.updated_at,
                    "user_id": project.user_id,
                    "username": project.user.username if project.user else "Unknown User",
                    "is_active": project.is_active
                }
                result.append(project_dict)
            except Exception as e:
                print(f"âš ï¸ Error processing project {project.id}: {str(e)}")
                # å€‹åˆ¥ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ã—ã¦ç¶šè¡Œ
                continue

        print(f"ğŸ“Š Retrieved {len(result)} active projects for user {current_user.username}")
        return result

    except Exception as e:
        print(f"âŒ Error in get_projects: {str(e)}")
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ï¼ˆãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã®è¡¨ç¤ºã‚’ç¶­æŒï¼‰
        return []

@router.get("/{project_id}", response_model=ProjectWithSpiders)
async def get_project(
    project_id: str,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """ç‰¹å®šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±å«ã‚€ï¼‰"""
    project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ç®¡ç†è€…ä»¥å¤–ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
    is_admin = (current_user.role == UserRole.ADMIN or
                current_user.role == "admin" or
                current_user.role == "ADMIN")
    if not is_admin and project.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚‚å«ã‚ã¦è¿”ã™
    spiders = db.query(DBSpider).filter(DBSpider.project_id == project_id).all()

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_spiders = []
    for spider in spiders:
        spider_dict = {
            "id": spider.id,
            "name": spider.name,
            "description": spider.description or "",
            "code": spider.code or "# Empty spider code",  # ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¡ãƒ³ãƒˆã‚’è¨­å®š
            "template": spider.template,
            "framework": spider.framework or "scrapy",
            "start_urls": spider.start_urls or [],
            "settings": spider.settings or {},
            "project_id": spider.project_id,
            "created_at": spider.created_at,
            "updated_at": spider.updated_at
        }
        formatted_spiders.append(spider_dict)

    # åŒæœŸçŠ¶æ…‹ãƒã‚§ãƒƒã‚¯ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸï¼ˆè‡ªå‹•åŒæœŸã«ã‚ˆã‚Šä¸è¦ï¼‰

    project_dict = {
        "id": project.id,
        "name": project.name,
        "description": project.description,
        "path": project.path,
        "scrapy_version": project.scrapy_version,
        "settings": project.settings or {},
        "db_save_enabled": project.db_save_enabled,
        "created_at": project.created_at,
        "updated_at": project.updated_at,
        "spiders": formatted_spiders
    }

    return project_dict

@router.post("/", response_model=Project, status_code=status.HTTP_201_CREATED)
async def create_project(
    project: ProjectCreate,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""

    try:
        log_with_context(
            logger, "INFO",
            f"Creating new project: {project.name}",
            extra_data={"project_name": project.name, "description": project.description}
        )

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ï¼‰
        existing_project = db.query(DBProject).filter(
            DBProject.name == project.name,
            DBProject.user_id == current_user.id
        ).first()
        if existing_project:
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå '{project.name}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚åˆ¥ã®åå‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <username>_<projectname>
        username = current_user.username.lower().replace(' ', '_').replace('-', '_')
        project_name_clean = project.name.lower().replace(' ', '_').replace('-', '_')
        project_path = f"{username}_{project_name_clean}"

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
        existing_path = db.query(DBProject).filter(DBProject.path == project_path).first()
        if existing_path:
            # åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§åŒã˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®å ´åˆã¯æ—¢ã«ä¸Šã§ãƒã‚§ãƒƒã‚¯æ¸ˆã¿
            # ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§åŒã˜username_projectnameã«ãªã‚‹å ´åˆã®ã¿ã“ã“ã«åˆ°é”
            project_path = f"{project_path}_{str(uuid.uuid4())[:8]}"

        # Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆï¼ˆscrapy startproject project_name ã¨åŒã˜å‹•ä½œï¼‰
        try:
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å®Ÿéš›ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not os.getenv("TESTING", False):
                # æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: ã‚¯ãƒªãƒ¼ãƒ³ãªScrapyã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
                scrapy_service = ScrapyPlaywrightService()  # åå‰ã¯ä¿æŒã€å†…éƒ¨ã¯ã‚¯ãƒªãƒ¼ãƒ³
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼‰ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ï¼ˆè¨­å®šåï¼‰ã€DBä¿å­˜è¨­å®šã‚’æ­£ã—ãæŒ‡å®š
                # ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‹ã‚‰db_save_enabledãŒé€ä¿¡ã•ã‚Œãªã„å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§Trueã«è¨­å®š
                db_save_enabled = getattr(project, 'db_save_enabled', True)
                scrapy_service.create_project(project_path, project_path, db_save_enabled)
                logger.info(f"Clean Scrapy project created successfully (new architecture): {project_path}")
            else:
                # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã‚‚Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆWebUIè¡¨ç¤ºã®ãŸã‚ï¼‰
                scrapy_service = ScrapyPlaywrightService()  # åå‰ã¯ä¿æŒã€å†…éƒ¨ã¯ã‚¯ãƒªãƒ¼ãƒ³
                db_save_enabled = getattr(project, 'db_save_enabled', True)
                scrapy_service.create_project(project_path, project_path, db_save_enabled)
                logger.info(f"Test clean Scrapy project created successfully (new architecture): {project_path}")
        except Exception as e:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã«å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯ä¿å­˜ã™ã‚‹ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
            log_exception(
                logger, f"Warning: Failed to create Scrapy project: {str(e)}",
                extra_data={"project_name": project.name, "project_path": project_path}
            )

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        db_project = DBProject(
            id=str(uuid.uuid4()),
            name=project.name,
            description=project.description,
            path=project_path,
            scrapy_version=project.scrapy_version or "2.11.0",
            settings=project.settings or {},
            db_save_enabled=getattr(project, 'db_save_enabled', True),
            user_id=current_user.id  # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’è¨­å®š
        )

        db.add(db_project)
        db.commit()
        db.refresh(db_project)

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        try:
            # TESTINGç’°å¢ƒã§ã‚‚åŒæœŸã‚’å®Ÿè¡Œï¼ˆWebUIè¡¨ç¤ºã®ãŸã‚ï¼‰
            # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†ã‚’ç¢ºå®Ÿã«ã™ã‚‹ï¼‰
            import time
            time.sleep(1.0)  # å¾…æ©Ÿæ™‚é–“ã‚’å»¶é•·

            # ã¾ãšé€šå¸¸ã®ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸã‚’å®Ÿè¡Œ
            sync_project_files_to_database(db, db_project.id, project_path, current_user.id)
            logger.info(f"All project files synced to database for project: {project_path}")

            # pipelines.pyã®ç‰¹åˆ¥ãªåŒæœŸã¯ä¸è¦ã«ãªã£ãŸãŸã‚å‰Šé™¤

            # åŒæœŸå¾Œã®ç¢ºèª
            from ..database import ProjectFile
            synced_count = db.query(ProjectFile).filter(ProjectFile.project_id == db_project.id).count()
            pipelines_count = db.query(ProjectFile).filter(
                ProjectFile.project_id == db_project.id,
                ProjectFile.name == "pipelines.py"
            ).count()
            logger.info(f"Total files synced to database: {synced_count}")
            logger.info(f"pipelines.py files in database: {pipelines_count}")

            # pipelines.pyã®å†…å®¹ã‚’æ¤œè¨¼
            if pipelines_count > 0:
                pipelines_file = db.query(ProjectFile).filter(
                    ProjectFile.project_id == db_project.id,
                    ProjectFile.name == "pipelines.py"
                ).first()
                if pipelines_file:
                    try:
                        content = pipelines_file.content
                        if isinstance(content, bytes):
                            content = content.decode('utf-8')
                        has_scrapy_ui = 'ScrapyUIDatabasePipeline' in content
                        expected_has_scrapy_ui = db_project.db_save_enabled
                        if has_scrapy_ui == expected_has_scrapy_ui:
                            logger.info(f"âœ… pipelines.py content verification passed: DB save={expected_has_scrapy_ui}, Has ScrapyUI={has_scrapy_ui}")
                        else:
                            logger.warning(f"âš ï¸ pipelines.py content verification failed: DB save={expected_has_scrapy_ui}, Has ScrapyUI={has_scrapy_ui}")
                    except Exception as e:
                        logger.error(f"Error verifying pipelines.py content in project creation: {e}")
                        has_scrapy_ui = False
        except Exception as e:
            logger.error(f"Failed to save project files to database: {str(e)}")
            # ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã¯æˆåŠŸã¨ã™ã‚‹ï¼‰

        log_with_context(
            logger, "INFO",
            f"Project created successfully: {project.name}",
            project_id=db_project.id,
            extra_data={"project_path": project_path}
        )

        return db_project

    except HTTPException:
        # HTTPExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
        raise
    except ProjectException:
        # æ—¢ã«ProjectExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
        raise
    except Exception as e:
        error_msg = f"Failed to create project: {str(e)}"
        log_exception(
            logger, error_msg,
            extra_data={"project_name": project.name}
        )
        raise ProjectException(
            message=error_msg,
            error_code=ErrorCode.PROJECT_CREATION_FAILED,
            details={"original_error": str(e)}
        )

@router.put("/{project_id}", response_model=Project)
async def update_project(
    project_id: str,
    project_update: ProjectUpdate,
    db: Session = Depends(get_db)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ›´æ–°"""
    db_project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not db_project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®é©ç”¨
    update_data = project_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_project, field, value)

    db.commit()
    db.refresh(db_project)

    return db_project

@router.delete("/{project_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_project(
    project_id: str,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ï¼ˆè«–ç†å‰Šé™¤ï¼‰"""
    try:
        db_project = db.query(DBProject).filter(DBProject.id == project_id).first()
        if not db_project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found"
            )

        # ç®¡ç†è€…ä»¥å¤–ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿å‰Šé™¤å¯èƒ½
        is_admin = (current_user.role == UserRole.ADMIN or
                    current_user.role == "admin" or
                    current_user.role == "ADMIN")
        if not is_admin and db_project.user_id != current_user.id:
            raise HTTPException(
                status_code=status.HTTP_403_FORBIDDEN,
                detail="Access denied"
            )

        print(f"ğŸ—‘ï¸ Deleting project: {db_project.name} (ID: {project_id})")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢é€£ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤
        try:
            from ..database import Schedule as DBSchedule
            related_schedules = db.query(DBSchedule).filter(DBSchedule.project_id == project_id).all()

            if related_schedules:
                print(f"ğŸ—‘ï¸ Deleting {len(related_schedules)} schedules related to project {db_project.name}")
                for schedule in related_schedules:
                    print(f"  - Deleting schedule: {schedule.name} (ID: {schedule.id})")
                    db.delete(schedule)

            # å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ï¼‰
            db.commit()
            print(f"âœ… Successfully deleted {len(related_schedules)} related schedules")

        except Exception as e:
            print(f"âš ï¸ Error deleting related schedules: {str(e)}")
            db.rollback()
            # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤ã¯ç¶šè¡Œ

        # è«–ç†å‰Šé™¤ï¼ˆis_activeã‚’Falseã«è¨­å®šï¼‰
        db_project.is_active = False
        db.commit()

        print(f"âœ… Project {db_project.name} marked as inactive (logical deletion)")

        # ç‰©ç†çš„ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ã¯è¡Œã‚ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ä¿è­·ã®ãŸã‚ï¼‰
        # å¿…è¦ã«å¿œã˜ã¦ç®¡ç†è€…ãŒæ‰‹å‹•ã§å‰Šé™¤å¯èƒ½

        return None

    except HTTPException:
        # HTTPExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
        raise
    except Exception as e:
        print(f"âŒ Error in delete_project: {str(e)}")
        db.rollback()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete project: {str(e)}"
        )


# æ‰‹å‹•åŒæœŸã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆæ™‚ã«è‡ªå‹•åŒæœŸã•ã‚Œã‚‹ãŸã‚ã€æ‰‹å‹•åŒæœŸã¯ä¸è¦ã§ã™


@router.post("/{project_id}/spiders/", response_model=dict, status_code=status.HTTP_201_CREATED)
async def create_project_spider(
    project_id: str,
    spider_data: dict,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
    project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    spider_name = spider_data.get("name")
    if not spider_name:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider name is required"
        )

    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == project_id,
        DBSpider.name == spider_name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider with this name already exists in the project"
        )

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    db_spider = DBSpider(
        id=str(uuid.uuid4()),
        name=spider_name,
        code=spider_data.get("script", spider_data.get("code", "")),
        template=spider_data.get("template"),
        settings=spider_data.get("settings", {}),
        project_id=project_id,
        user_id=current_user.id
    )

    db.add(db_spider)
    db.commit()
    db.refresh(db_spider)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    try:
        if not os.getenv("TESTING", False):
            scrapy_service = ScrapyPlaywrightService()
            spider_code = spider_data.get("script", spider_data.get("code", ""))
            scrapy_service.save_spider_code(project.path, spider_name, spider_code)

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ
            try:
                sync_spider_file_to_database(db, project.id, project.path, spider_name, spider_code, current_user.id)
                logger.info(f"Spider file synced to database: {spider_name}")

                # åŒæœŸç¢ºèª
                from ..database import ProjectFile
                spider_file_path = f"{project.path}/spiders/{spider_name}.py"
                synced_file = db.query(ProjectFile).filter(
                    ProjectFile.project_id == project.id,
                    ProjectFile.path == spider_file_path
                ).first()
                if synced_file:
                    logger.info(f"Spider file sync confirmed: {spider_file_path}")
                else:
                    logger.warning(f"Spider file sync verification failed: {spider_file_path}")
            except Exception as sync_error:
                logger.error(f"Failed to sync spider file to database: {sync_error}")
                # åŒæœŸå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆã¯æˆåŠŸã¨ã™ã‚‹ï¼‰
    except Exception as e:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        db.delete(db_spider)
        db.commit()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save spider file: {str(e)}"
        )

    return {
        "id": db_spider.id,
        "name": db_spider.name,
        "project_id": db_spider.project_id,
        "start_urls": spider_data.get("start_urls", []),
        "created_at": db_spider.created_at.isoformat() if db_spider.created_at else None
    }
