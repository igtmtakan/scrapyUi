# ğŸ•·ï¸ Puppeteerãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä½¿ç”¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ScrapyUIã§ã¯ã€Node.js Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã¨é€£æºã—ãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€JavaScripté‡è¦ãªSPAã‚µã‚¤ãƒˆã‚„å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

## ğŸš€ åˆ©ç”¨å¯èƒ½ãªãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

### 1. **Puppeteer SPA Scraper**
- **ç”¨é€”**: åŸºæœ¬çš„ãªSPAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- **ç‰¹å¾´**: 
  - JavaScriptå®Ÿè¡Œç’°å¢ƒ
  - ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãƒ™ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿æŠ½å‡º
  - ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—
  - ã‚«ã‚¹ã‚¿ãƒ JavaScriptå®Ÿè¡Œ

### 2. **Scrapy + Puppeteer Spider**
- **ç”¨é€”**: Scrapyã¨Puppeteerã®çµ±åˆ
- **ç‰¹å¾´**:
  - Scrapyã®æ©Ÿèƒ½ã¨Puppeteerã®çµ„ã¿åˆã‚ã›
  - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ï¼ˆPuppeteerå¤±æ•—æ™‚ã¯é€šå¸¸ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
  - ãƒªãƒ³ã‚¯ãƒ•ã‚©ãƒ­ãƒ¼æ©Ÿèƒ½
  - è‡ªå‹•ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜

### 3. **E-commerce Puppeteer Spider**
- **ç”¨é€”**: ECã‚µã‚¤ãƒˆå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
- **ç‰¹å¾´**:
  - å•†å“æƒ…å ±ã®æ§‹é€ åŒ–æŠ½å‡º
  - ä¾¡æ ¼ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
  - é–¢é€£å•†å“ã®è‡ªå‹•ç™ºè¦‹
  - ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ»è©•ä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—

## ğŸ”§ å‰ææ¡ä»¶

### Node.jsã‚µãƒ¼ãƒ“ã‚¹ã®èµ·å‹•
```bash
# Node.jsã‚µãƒ¼ãƒ“ã‚¹ãŒ localhost:3001 ã§å‹•ä½œã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™
curl http://localhost:3001/api/health
```

### å¿…è¦ãªPythonãƒ‘ãƒƒã‚±ãƒ¼ã‚¸
```bash
pip install aiohttp asyncio
```

## ğŸ“ ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ãƒšãƒ¼ã‚¸ã‹ã‚‰ä½¿ç”¨

1. ScrapyUI ã® `/editor` ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ã€ŒNew Spiderã€ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
3. ã€ŒPuppeteerã€ã‚«ãƒ†ã‚´ãƒªã‚’é¸æŠ
4. ä½¿ç”¨ã—ãŸã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é¸æŠ
5. URLã‚„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ç·¨é›†
6. ã€ŒRunã€ãƒœã‚¿ãƒ³ã§å®Ÿè¡Œ

### æ–¹æ³•2: ç›´æ¥ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ”ãƒ¼

```python
# åŸºæœ¬çš„ãªPuppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ä¾‹
import asyncio
import aiohttp
import json

async def scrape_example():
    async with aiohttp.ClientSession() as session:
        request_data = {
            "url": "https://example.com",
            "extractData": {
                "selectors": {
                    "title": "h1",
                    "content": "p"
                }
            }
        }
        
        async with session.post(
            "http://localhost:3001/api/scraping/spa",
            json=request_data
        ) as response:
            data = await response.json()
            print(json.dumps(data, indent=2))

# å®Ÿè¡Œ
asyncio.run(scrape_example())
```

## âš™ï¸ è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³

### ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®š
```python
request_data = {
    "url": "https://example.com",
    "waitFor": ".content-loaded",  # å¾…æ©Ÿã™ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
    "timeout": 30000,              # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒŸãƒªç§’ï¼‰
    "viewport": {                  # ãƒ“ãƒ¥ãƒ¼ãƒãƒ¼ãƒˆè¨­å®š
        "width": 1920,
        "height": 1080
    },
    "extractData": {
        "selectors": {             # æŠ½å‡ºã™ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
            "title": "h1",
            "content": ".content",
            "links": "a[href]"
        },
        "javascript": '''          # ã‚«ã‚¹ã‚¿ãƒ JavaScript
            return {
                pageTitle: document.title,
                loadTime: performance.now()
            };
        '''
    },
    "screenshot": True             # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆå–å¾—
}
```

### Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼è¨­å®š
```python
class MySpider(scrapy.Spider):
    name = 'my_spider'
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.nodejs_url = "http://localhost:3001"  # Node.jsã‚µãƒ¼ãƒ“ã‚¹URL
    
    custom_settings = {
        'CONCURRENT_REQUESTS': 1,    # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
        'DOWNLOAD_DELAY': 2,         # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        'ROBOTSTXT_OBEY': False      # robots.txtç„¡è¦–
    }
```

## ğŸ¯ å®Ÿç”¨ä¾‹

### ECã‚µã‚¤ãƒˆã®å•†å“æƒ…å ±å–å¾—
```python
selectors = {
    "name": "h1.product-title",
    "price": ".price",
    "description": ".product-description",
    "images": "img.product-image",
    "availability": ".availability",
    "reviews": ".review-item"
}
```

### ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆã®è¨˜äº‹å–å¾—
```python
selectors = {
    "headline": "h1",
    "author": ".author",
    "date": ".publish-date",
    "content": ".article-body",
    "tags": ".tag-list a"
}
```

### SNSã®æŠ•ç¨¿å–å¾—
```python
selectors = {
    "posts": ".post-item",
    "usernames": ".username",
    "timestamps": ".timestamp",
    "content": ".post-content",
    "likes": ".like-count"
}
```

## ğŸ” ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. `ModuleNotFoundError: No module named 'scrapy_ui'`
**è§£æ±ºæ–¹æ³•**: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä¿®æ­£ç‰ˆã«æ›´æ–°ã—ã¦ãã ã•ã„ã€‚æ–°ã—ã„ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯å¤–éƒ¨ä¾å­˜é–¢ä¿‚ã‚’ä½¿ç”¨ã—ã¾ã›ã‚“ã€‚

#### 2. Node.jsã‚µãƒ¼ãƒ“ã‚¹ã«æ¥ç¶šã§ããªã„
**ç¢ºèªäº‹é …**:
- Node.jsã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹
- ãƒãƒ¼ãƒˆ3001ãŒåˆ©ç”¨å¯èƒ½ã‹
- ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®š

#### 3. ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆãŒä¿å­˜ã•ã‚Œãªã„
**ç¢ºèªäº‹é …**:
- ãƒ‡ã‚£ã‚¹ã‚¯ã®ç©ºãå®¹é‡
- ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿æ¨©é™
- base64ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼

#### 4. ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã•ã‚Œãªã„
**ç¢ºèªäº‹é …**:
- ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ãŒæ­£ã—ã„ã‹
- ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿã—ã¦ã„ã‚‹ã‹
- JavaScriptã‚¨ãƒ©ãƒ¼ãŒãªã„ã‹

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

### æ¨å¥¨è¨­å®š
```python
# Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®å ´åˆ
custom_settings = {
    'CONCURRENT_REQUESTS': 1,        # Puppeteerã¯é‡ã„ã®ã§1ã«åˆ¶é™
    'DOWNLOAD_DELAY': 2,             # 2ç§’é–“éš”
    'RANDOMIZE_DOWNLOAD_DELAY': 0.5, # ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
    'AUTOTHROTTLE_ENABLED': True,    # è‡ªå‹•ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°
    'AUTOTHROTTLE_START_DELAY': 1,
    'AUTOTHROTTLE_MAX_DELAY': 10,
    'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0
}
```

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ç›£è¦–
```python
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–
import psutil
import os

def check_memory():
    process = psutil.Process(os.getpid())
    memory_mb = process.memory_info().rss / 1024 / 1024
    print(f"Memory usage: {memory_mb:.2f} MB")
```

## ğŸ›¡ï¸ ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è€ƒæ…®äº‹é …

### 1. ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®éµå®ˆ
- é©åˆ‡ãªé…å»¶è¨­å®š
- robots.txtã®ç¢ºèª
- ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ç¢ºèª

### 2. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è¨­å®š
```python
headers = {
    'User-Agent': 'ScrapyUI-Bot/1.0 (+https://your-domain.com/bot)'
}
```

### 3. ãƒ—ãƒ­ã‚­ã‚·ã®ä½¿ç”¨
```python
# ãƒ—ãƒ­ã‚­ã‚·è¨­å®šä¾‹
request_data = {
    "url": "https://example.com",
    "proxy": "http://proxy-server:port",
    # ... ãã®ä»–ã®è¨­å®š
}
```

## ğŸ“ˆ ç›£è¦–ã¨ãƒ­ã‚°

### ãƒ­ã‚°è¨­å®š
```python
import logging

# ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)

logger = logging.getLogger(__name__)
```

### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†
```python
# æˆåŠŸç‡ã®è¿½è·¡
success_count = 0
total_count = 0

def track_success(success):
    global success_count, total_count
    total_count += 1
    if success:
        success_count += 1
    
    success_rate = (success_count / total_count) * 100
    print(f"Success rate: {success_rate:.2f}%")
```

## ğŸ”— é–¢é€£ãƒªãƒ³ã‚¯

- [ScrapyUI Documentation](/)
- [Node.js Puppeteer Service](/nodejs)
- [Scrapy Documentation](https://docs.scrapy.org/)
- [Puppeteer Documentation](https://pptr.dev/)

---

**æ³¨æ„**: Puppeteerãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹éš›ã¯ã€å¯¾è±¡ã‚µã‚¤ãƒˆã®åˆ©ç”¨è¦ç´„ã‚’å¿…ãšç¢ºèªã—ã€é©åˆ‡ãªãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚
