import React from 'react'
import { Globe, Zap, ShoppingCart, Search } from 'lucide-react'
import { Template } from '../types'

export const playwrightServiceTemplates: Template[] = [
  {
    id: 'playwright-service-basic',
    name: 'Playwright Service Basic Spider',
    description: 'æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ãŸåŸºæœ¬ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼',
    icon: <Globe className="w-5 h-5" />,
    category: 'browser-automation',
    code: `#!/usr/bin/env python3
"""
Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹å¯¾å¿œã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼
æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨
"""

import scrapy
import json
import requests
from datetime import datetime

def debug_print(message):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] ğŸ•·ï¸ SPIDER: {message}")

class PlaywrightServiceSpider(scrapy.Spider):
    name = "playwright_service_spider"
    allowed_domains = ["example.com"]
    start_urls = ["https://example.com"]
    
    # Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
    playwright_service_url = "http://localhost:8004"
    
    custom_settings = {
        'ROBOTSTXT_OBEY': False,
        'DOWNLOAD_DELAY': 2,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 1,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'FEEDS': {
            'results.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
                'store_empty': False,
                'overwrite': True,
            },
        },
        'ITEM_PIPELINES': {
            'backend.app.pipelines.database_pipeline.DatabasePipeline': 300,
        },
    }
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        debug_print(f"Playwright Service Spider initialized")
        debug_print(f"Playwright Service URL: {self.playwright_service_url}")
    
    def start_requests(self):
        """Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ãŸstart_requests"""
        debug_print("Starting spider with Playwright service integration")
        
        for url in self.start_urls:
            # Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            content = self.fetch_with_playwright_sync(url)
            
            if content:
                debug_print("âœ… Playwright fetch successful, creating Scrapy response")
                
                # Scrapyãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
                yield scrapy.http.HtmlResponse(
                    url=url,
                    body=content.encode('utf-8'),
                    encoding='utf-8',
                    request=scrapy.Request(url=url, callback=self.parse)
                )
            else:
                debug_print("âŒ Playwright fetch failed, using fallback")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®Scrapyãƒªã‚¯ã‚¨ã‚¹ãƒˆ
                yield scrapy.Request(url=url, callback=self.parse_fallback)
    
    def fetch_with_playwright_sync(self, url: str) -> str:
        """åŒæœŸç‰ˆã®Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹å‘¼ã³å‡ºã—"""
        debug_print(f"Fetching {url} via Playwright service")
        
        request_data = {
            "url": url,
            "browser_type": "chromium",
            "headless": True,
            "wait_for": "domcontentloaded",
            "timeout": 30000,
            "javascript_code": "window.scrollTo(0, document.body.scrollHeight);"
        }
        
        try:
            response = requests.post(
                f"{self.playwright_service_url}/execute",
                json=request_data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                
                if result.get("success"):
                    debug_print(f"âœ… Playwright fetch successful: {result.get('title', 'No title')}")
                    debug_print(f"   Execution time: {result.get('execution_time', 0):.2f}s")
                    debug_print(f"   Content length: {len(result.get('content', ''))}")
                    return result.get("content", "")
                else:
                    debug_print(f"âŒ Playwright fetch failed: {result.get('error', 'Unknown error')}")
                    return None
            else:
                debug_print(f"âŒ HTTP error: {response.status_code}")
                return None
                
        except Exception as e:
            debug_print(f"âŒ Exception during Playwright fetch: {e}")
            return None
    
    def parse(self, response):
        """Playwrightå–å¾—ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è§£æ"""
        debug_print(f"Parsing Playwright content from: {response.url}")
        debug_print(f"Response length: {len(response.text)}")
        
        # ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
        title = response.css('title::text').get()
        debug_print(f"Page title: {title}")
        
        # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        yield {
            'url': response.url,
            'title': title,
            'method': 'playwright_service',
            'crawl_start_datetime': datetime.now().isoformat(),
            'item_acquired_datetime': datetime.now().isoformat()
        }
        
        debug_print("âœ… Parsing completed")
    
    def parse_fallback(self, response):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ‘ãƒ¼ã‚¹é–¢æ•°"""
        debug_print(f"Using fallback parsing for: {response.url}")
        
        title = response.css('title::text').get()
        debug_print(f"Fallback page title: {title}")
        
        yield {
            'url': response.url,
            'title': title,
            'method': 'fallback',
            'crawl_start_datetime': datetime.now().isoformat(),
            'item_acquired_datetime': datetime.now().isoformat()
        }
        
        debug_print("âœ… Fallback parsing completed")`
  },
  {
    id: 'playwright-service-amazon',
    name: 'Amazon Ranking Spider (Playwright Service)',
    description: 'Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ãŸAmazonãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼',
    icon: <ShoppingCart className="w-5 h-5" />,
    category: 'e-commerce',
    code: `#!/usr/bin/env python3
"""
Amazon ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ï¼ˆPlaywrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ç‰ˆï¼‰
æ–°ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦Amazonãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å–å¾—
"""

import scrapy
import json
import requests
from datetime import datetime

def debug_print(message):
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] ğŸ›’ AMAZON: {message}")

class AmazonRankingPlaywrightSpider(scrapy.Spider):
    name = "amazon_ranking_playwright"
    allowed_domains = ["amazon.co.jp"]
    start_urls = ["https://www.amazon.co.jp/gp/bestsellers/toys/"]
    
    # Playwrightå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹è¨­å®š
    playwright_service_url = "http://localhost:8004"
    
    custom_settings = {
        'ROBOTSTXT_OBEY': False,
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
        'CONCURRENT_REQUESTS': 1,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'FEEDS': {
            'amazon_ranking_results.jsonl': {
                'format': 'jsonlines',
                'encoding': 'utf8',
                'store_empty': False,
                'overwrite': True,
            },
        },
        'ITEM_PIPELINES': {
            'backend.app.pipelines.database_pipeline.DatabasePipeline': 300,
        },
    }
    
    def start_requests(self):
        """Amazonå°‚ç”¨ã®Playwrightè¨­å®šã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹"""
        debug_print("Starting Amazon ranking spider with Playwright service")
        
        for url in self.start_urls:
            content = self.fetch_amazon_with_playwright(url)
            
            if content:
                debug_print("âœ… Amazon page fetched successfully")
                yield scrapy.http.HtmlResponse(
                    url=url,
                    body=content.encode('utf-8'),
                    encoding='utf-8',
                    request=scrapy.Request(url=url, callback=self.parse_ranking)
                )
            else:
                debug_print("âŒ Amazon fetch failed")
                yield scrapy.Request(url=url, callback=self.parse_fallback)
    
    def fetch_amazon_with_playwright(self, url: str) -> str:
        """Amazonå°‚ç”¨ã®Playwrightè¨­å®š"""
        debug_print(f"Fetching Amazon page: {url}")
        
        request_data = {
            "url": url,
            "browser_type": "chromium",
            "headless": True,
            "wait_for": "domcontentloaded",
            "timeout": 30000,
            "javascript_code": """
                // ãƒšãƒ¼ã‚¸ã‚’ä¸‹ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                window.scrollTo(0, document.body.scrollHeight);
                
                // å°‘ã—å¾…æ©Ÿ
                await new Promise(resolve => setTimeout(resolve, 2000));
                
                // å†åº¦ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                window.scrollTo(0, document.body.scrollHeight);
                
                console.log('Amazon page scrolled and loaded');
            """
        }
        
        try:
            response = requests.post(
                f"{self.playwright_service_url}/execute",
                json=request_data,
                timeout=60
            )
            
            if response.status_code == 200:
                result = response.json()
                if result.get("success"):
                    debug_print(f"âœ… Amazon fetch successful: {result.get('execution_time', 0):.2f}s")
                    return result.get("content", "")
                else:
                    debug_print(f"âŒ Amazon fetch error: {result.get('error')}")
            
            return None
            
        except Exception as e:
            debug_print(f"âŒ Exception: {e}")
            return None
    
    def parse_ranking(self, response):
        """Amazonãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒšãƒ¼ã‚¸ã®è§£æ"""
        debug_print(f"Parsing Amazon ranking page: {response.url}")
        
        # ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ã‚¤ãƒ†ãƒ ã‚’æ¤œç´¢
        ranking_items = response.css('div[data-asin]')
        debug_print(f"Found {len(ranking_items)} ranking items")
        
        for i, item in enumerate(ranking_items[:60], 1):  # ä¸Šä½60ä½ã¾ã§
            try:
                asin = item.css('::attr(data-asin)').get()
                if not asin:
                    continue
                
                title = item.css('img::attr(alt)').get() or item.css('a span::text').get()
                image_url = item.css('img::attr(src)').get()
                product_url = item.css('a::attr(href)').get()
                
                if product_url and product_url.startswith('/'):
                    product_url = f"https://www.amazon.co.jp{product_url}"
                
                # ä¾¡æ ¼æƒ…å ±
                price_element = item.css('span.a-price-whole::text').get()
                price = None
                if price_element:
                    try:
                        price = int(price_element.replace(',', ''))
                    except ValueError:
                        pass
                
                # ãƒ¬ãƒ“ãƒ¥ãƒ¼æƒ…å ±
                rating_element = item.css('span.a-icon-alt::text').get()
                rating = None
                if rating_element and '5ã¤æ˜Ÿã®ã†ã¡' in rating_element:
                    try:
                        rating = float(rating_element.split('5ã¤æ˜Ÿã®ã†ã¡')[1].split('ã®')[0])
                    except (IndexError, ValueError):
                        pass
                
                yield {
                    'rank': i,
                    'asin': asin,
                    'title': title,
                    'price': price,
                    'rating': rating,
                    'image_url': image_url,
                    'product_url': product_url,
                    'category': 'toys',
                    'crawl_start_datetime': datetime.now().isoformat(),
                    'item_acquired_datetime': datetime.now().isoformat(),
                    'url': response.url,
                    'method': 'playwright_service'
                }
                
                debug_print(f"Extracted item {i}: {title[:50] if title else 'No title'}...")
                
            except Exception as e:
                debug_print(f"âŒ Error extracting item {i}: {e}")
                continue
        
        debug_print(f"âœ… Completed Amazon ranking extraction")
    
    def parse_fallback(self, response):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        debug_print("Using fallback parsing for Amazon")
        
        yield {
            'title': 'Amazon fallback item',
            'url': response.url,
            'method': 'fallback',
            'crawl_start_datetime': datetime.now().isoformat(),
            'item_acquired_datetime': datetime.now().isoformat()
        }`
  }
]
