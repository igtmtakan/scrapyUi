"""
ScrapyTaskManager - çµ±ä¸€çš„ãªScrapyå®Ÿè¡Œç®¡ç†ã‚¯ãƒ©ã‚¹

ã“ã®ã‚¯ãƒ©ã‚¹ã¯Scrapyã®å®Ÿè¡Œã‚’çµ±ä¸€çš„ã«ç®¡ç†ã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ï¼š
- ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–ã¨ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°
- ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†ï¼ˆPENDING â†’ RUNNING â†’ COMPLETED/FAILEDï¼‰
- çµæœã®è‡ªå‹•åŒæœŸã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åæ˜ 
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨è©³ç´°ãƒ­ã‚°è¨˜éŒ²
- WebSocketé€šçŸ¥çµ±åˆ
"""

import asyncio
import json
import subprocess
import threading
import time
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, Any, Optional, Callable
import uuid

from sqlalchemy.orm import Session
from ..database import SessionLocal, Task, TaskStatus
from .scrapy_realtime_engine import ScrapyRealtimeEngine
from .realtime_websocket_manager import realtime_websocket_manager, RealtimeProgressFormatter


class ProgressTracker:
    """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¿½è·¡ã‚¯ãƒ©ã‚¹"""

    def __init__(self):
        self.items_count = 0
        self.requests_count = 0
        self.errors_count = 0
        self.start_time = None
        self.last_update = None
        self.estimated_total = 0

    def update(self, items: int = None, requests: int = None, errors: int = None):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’æ›´æ–°"""
        if items is not None:
            self.items_count = items
        if requests is not None:
            self.requests_count = requests
        if errors is not None:
            self.errors_count = errors
        self.last_update = datetime.now(timezone.utc)

    def get_progress_percentage(self) -> float:
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        if self.estimated_total <= 0:
            # å‹•çš„æ¨å®š: ã‚¢ã‚¤ãƒ†ãƒ æ•°ã«åŸºã¥ã„ã¦æ¨å®š
            if self.items_count > 0:
                self.estimated_total = max(100, self.items_count + 20)
            else:
                return 5.0  # é–‹å§‹æ™‚ã®åŸºæœ¬é€²æ—

        progress = min(95.0, (self.items_count / self.estimated_total) * 100)
        return progress

    def get_efficiency(self) -> float:
        """åŠ¹ç‡ï¼ˆitems/minï¼‰ã‚’è¨ˆç®—"""
        if not self.start_time or self.items_count == 0:
            return 0.0

        elapsed_minutes = (datetime.now(timezone.utc) - self.start_time).total_seconds() / 60
        if elapsed_minutes > 0:
            return self.items_count / elapsed_minutes
        return 0.0


class ScrapyTaskManager:
    """
    Scrapyã‚¿ã‚¹ã‚¯ã®çµ±ä¸€ç®¡ç†ã‚¯ãƒ©ã‚¹

    æ©Ÿèƒ½:
    - Scrapyãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œã¨ç›£è¦–
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¿½è·¡
    - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç®¡ç†ã¨WebSocketé€šçŸ¥
    - çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•åŒæœŸ
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨å¾©æ—§
    """

    def __init__(self, task_id: str, spider_config: Dict[str, Any],
                 progress_callback: Optional[Callable] = None,
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.spider_config = spider_config
        self.progress_callback = progress_callback
        self.websocket_callback = websocket_callback

        # çŠ¶æ…‹ç®¡ç†
        self.status = TaskStatus.PENDING
        self.progress = ProgressTracker()
        self.process = None
        self.monitoring_thread = None
        self.is_cancelled = False

        # ãƒ‘ã‚¹è¨­å®š
        self.project_path = Path(spider_config.get('project_path', ''))
        self.result_file = self.project_path / f"results_{task_id}.json"
        self.log_file = self.project_path / f"logs_{task_id}.log"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.db_session = None

    async def execute(self) -> Dict[str, Any]:
        """
        Scrapyã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ

        Returns:
            Dict[str, Any]: å®Ÿè¡Œçµæœ
        """
        try:
            self.db_session = SessionLocal()

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å®Ÿè¡Œä¸­ã«æ›´æ–°
            await self._update_status(TaskStatus.RUNNING)

            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–é–‹å§‹
            self.progress.start_time = datetime.now(timezone.utc)
            self._start_monitoring()

            # Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            success = await self._execute_scrapy()

            # å®Œäº†å‡¦ç†
            await self._handle_completion(success)

            return {
                'success': success,
                'task_id': self.task_id,
                'items_count': self.progress.items_count,
                'requests_count': self.progress.requests_count,
                'errors_count': self.progress.errors_count,
                'result_file': str(self.result_file) if self.result_file.exists() else None
            }

        except Exception as e:
            await self._handle_error(e)
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }
        finally:
            if self.db_session:
                self.db_session.close()

    async def cancel(self) -> bool:
        """ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        self.is_cancelled = True

        if self.process and self.process.poll() is None:
            self.process.terminate()

        await self._update_status(TaskStatus.CANCELLED)
        return True

    def get_current_progress(self) -> Dict[str, Any]:
        """ç¾åœ¨ã®ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹æƒ…å ±ã‚’å–å¾—"""
        return {
            'task_id': self.task_id,
            'status': self.status.value if hasattr(self.status, 'value') else str(self.status),
            'progress_percentage': self.progress.get_progress_percentage(),
            'items_count': self.progress.items_count,
            'requests_count': self.progress.requests_count,
            'errors_count': self.progress.errors_count,
            'efficiency': self.progress.get_efficiency(),
            'last_update': self.progress.last_update.isoformat() if self.progress.last_update else None,
            'elapsed_time': (datetime.now(timezone.utc) - self.progress.start_time).total_seconds() if self.progress.start_time else 0
        }

    async def _execute_scrapy(self) -> bool:
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ä½¿ç”¨ï¼‰"""
        try:
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            use_realtime = self.spider_config.get('use_realtime_engine', True)

            if use_realtime:
                return await self._execute_scrapy_realtime()
            else:
                return await self._execute_scrapy_subprocess()

        except Exception as e:
            print(f"Error executing Scrapy: {e}")
            return False

    async def _execute_scrapy_realtime(self) -> bool:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã§Scrapyã‚’å®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ Using Scrapy Realtime Engine for {self.spider_config['spider_name']}")

            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ
            realtime_engine = ScrapyRealtimeEngine(
                progress_callback=self._on_realtime_progress,
                websocket_callback=self.websocket_callback
            )

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼è¨­å®šã‚’æº–å‚™ï¼ˆè¤‡æ•°å½¢å¼å‡ºåŠ›å¯¾å¿œï¼‰
            settings = self.spider_config.get('settings', {})

            # è¤‡æ•°å½¢å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åŒæ™‚å‡ºåŠ›ã™ã‚‹è¨­å®š
            base_filename = f"results_{self.task_id}"
            feeds_config = {
                str(self.project_path / f"{base_filename}.jsonl"): {
                    'format': 'jsonlines',
                    'encoding': 'utf8',
                    'store_empty': False,
                    'item_export_kwargs': {
                        'ensure_ascii': False,
                    }
                },
                str(self.project_path / f"{base_filename}.json"): {
                    'format': 'json',
                    'encoding': 'utf8',
                    'store_empty': False,
                    'item_export_kwargs': {
                        'ensure_ascii': False,
                        'indent': 2
                    }
                },
                str(self.project_path / f"{base_filename}.csv"): {
                    'format': 'csv',
                    'encoding': 'utf8',
                    'store_empty': False,
                },
                str(self.project_path / f"{base_filename}.xml"): {
                    'format': 'xml',
                    'encoding': 'utf8',
                    'store_empty': False,
                }
            }

            settings.update({
                'FEEDS': feeds_config
            })

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
            result = realtime_engine.run_spider(
                spider_name=self.spider_config['spider_name'],
                project_path=str(self.project_path),
                settings=settings
            )

            success = result.get('success', False)

            if success:
                print(f"âœ… Realtime engine execution completed successfully")

                # çµ±è¨ˆæƒ…å ±ã‚’ã‚¿ã‚¹ã‚¯ã«åæ˜ 
                items_count = result.get('items_count', 0)
                requests_count = result.get('requests_count', 0)
                errors_count = result.get('errors_count', 0)

                print(f"ğŸ“Š Updating task statistics: items={items_count}, requests={requests_count}, errors={errors_count}")

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
                await self._update_task_completion(
                    items_count=items_count,
                    requests_count=requests_count,
                    errors_count=errors_count,
                    success=True
                )

                return True
            else:
                print(f"âŒ Realtime engine execution failed: {result.get('error', 'Unknown error')}")
                print(f"ğŸ”„ Falling back to standard Scrapy subprocess execution")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
                return await self._execute_scrapy_subprocess()

        except Exception as e:
            print(f"Error in realtime Scrapy execution: {e}")
            return False

    async def _execute_scrapy_subprocess(self) -> bool:
        """å¾“æ¥ã®ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã§Scrapyã‚’å®Ÿè¡Œ"""
        try:
            # Scrapyã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
            cmd = self._build_scrapy_command()

            # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            self.process = subprocess.Popen(
                cmd,
                cwd=str(self.project_path),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )

            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = self.process.communicate()

            # çµæœã‚’è©•ä¾¡
            success = self.process.returncode == 0 and self.result_file.exists()

            if not success:
                print(f"Scrapy execution failed: {stderr}")

            return success

        except Exception as e:
            print(f"Error executing Scrapy: {e}")
            return False

    def _on_realtime_progress(self, progress_data: Dict[str, Any]):
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            # é€²æ—ãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã¦æ›´æ–°
            if 'items_count' in progress_data:
                self.progress.update(items=progress_data['items_count'])

            if 'requests_count' in progress_data:
                self.progress.update(requests=progress_data['requests_count'])

            if 'errors_count' in progress_data:
                self.progress.update(errors=progress_data['errors_count'])

            # WebSocketé€šçŸ¥ã‚’é€ä¿¡
            self._send_websocket_notification(progress_data)

            # è©³ç´°ãƒ­ã‚°å‡ºåŠ›
            progress_type = progress_data.get('type', 'stats')
            if progress_type == 'item_processed':
                print(f"ğŸ“¦ Item {progress_data.get('item_count', 0)} processed")
            elif progress_type == 'download_complete':
                print(f"â¬‡ï¸ Downloaded: {progress_data.get('url', 'unknown')}")
            elif progress_type == 'download_error':
                print(f"âŒ Download error: {progress_data.get('error', 'unknown')}")

            # å¤–éƒ¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
            if self.progress_callback:
                self.progress_callback(self.get_current_progress())

        except Exception as e:
            print(f"Error in realtime progress callback: {e}")

    def _send_websocket_notification(self, progress_data: Dict[str, Any]):
        """WebSocketé€šçŸ¥ã‚’é€ä¿¡"""
        try:
            progress_type = progress_data.get('type', 'stats')

            if progress_type == 'item_processed':
                # ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†é€šçŸ¥
                formatted_data = RealtimeProgressFormatter.format_item_progress(progress_data)
                realtime_websocket_manager.notify_item_processed(self.task_id, formatted_data)

            elif progress_type in ['download_start', 'download_complete', 'download_error']:
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—é€šçŸ¥
                formatted_data = RealtimeProgressFormatter.format_download_progress(progress_data)
                realtime_websocket_manager.notify_download_progress(self.task_id, formatted_data)

            else:
                # ä¸€èˆ¬çš„ãªé€²æ—é€šçŸ¥
                formatted_data = RealtimeProgressFormatter.format_task_progress(progress_data)
                realtime_websocket_manager.notify_progress(self.task_id, formatted_data)

        except Exception as e:
            print(f"Error sending WebSocket notification: {e}")

    def _build_scrapy_command(self) -> list:
        """Scrapyã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆè¤‡æ•°å½¢å¼å‡ºåŠ›å¯¾å¿œï¼‰"""
        cmd = [
            'python3', '-m', 'scrapy', 'crawl',
            self.spider_config['spider_name'],
            '-L', 'DEBUG',  # ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã§ã‚ˆã‚Šè©³ç´°ãªãƒ­ã‚°
            '-s', 'LOG_LEVEL=DEBUG',
            '-s', 'ROBOTSTXT_OBEY=False',
            '-s', 'LOGSTATS_INTERVAL=5',  # 5ç§’é–“éš”ã§çµ±è¨ˆå‡ºåŠ›
            '-s', 'LOG_FILE=' + str(self.log_file),  # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
        ]

        # è¤‡æ•°å½¢å¼å‡ºåŠ›è¨­å®šã‚’è¿½åŠ 
        base_filename = f"results_{self.task_id}"

        # æœ€åˆã®ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆJSONLï¼‰ã‚’ãƒ¡ã‚¤ãƒ³ã®å‡ºåŠ›ã¨ã—ã¦è¨­å®š
        cmd.extend(['-o', str(self.project_path / f'{base_filename}.jsonl')])
        cmd.extend(['-t', 'jsonlines'])

        # è¿½åŠ ã®å‡ºåŠ›å½¢å¼ã‚’FEEDSè¨­å®šã§è¿½åŠ 
        feeds_config = f"FEEDS={{{str(self.project_path / f'{base_filename}.json')}:{{'format':'json'}},{str(self.project_path / f'{base_filename}.csv')}:{{'format':'csv'}},{str(self.project_path / f'{base_filename}.xml')}:{{'format':'xml'}}}}"
        cmd.extend(['-s', feeds_config])

        # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’è¿½åŠ 
        settings = self.spider_config.get('settings', {})
        for key, value in settings.items():
            if key != 'FEEDS':  # FEEDSè¨­å®šã¯ä¸Šã§è¨­å®šæ¸ˆã¿
                cmd.extend(['-s', f'{key}={value}'])

        return cmd

    def _start_monitoring(self):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        self.monitoring_thread = threading.Thread(
            target=self._monitor_progress,
            daemon=True
        )
        self.monitoring_thread.start()

    def _monitor_progress(self):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ç›£è¦–ãƒ«ãƒ¼ãƒ—ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—è¿½è·¡ï¼‰"""
        while not self.is_cancelled and (not self.process or self.process.poll() is None):
            try:
                # 1. çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é€²æ—ã‚’èª­ã¿å–ã‚Š
                if self.result_file.exists():
                    items_count = self._count_items_in_file()
                    self.progress.update(items=items_count)

                # 2. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã‚’è§£æ
                if self.log_file.exists():
                    log_stats = self._parse_scrapy_log()
                    if log_stats:
                        self.progress.update(
                            requests=log_stats.get('requests', self.progress.requests_count),
                            errors=log_stats.get('errors', self.progress.errors_count)
                        )

                # 3. ãƒ—ãƒ­ã‚»ã‚¹å‡ºåŠ›ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æƒ…å ±ã‚’å–å¾—
                if self.process and hasattr(self.process, 'stdout'):
                    realtime_stats = self._read_process_output()
                    if realtime_stats:
                        self.progress.update(
                            requests=realtime_stats.get('requests', self.progress.requests_count),
                            items=realtime_stats.get('items', self.progress.items_count)
                        )

                # 4. ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ï¼ˆåŒæœŸçš„ã«å®Ÿè¡Œï¼‰
                self._notify_progress_sync()

                time.sleep(1)  # 1ç§’é–“éš”ã§é«˜é »åº¦ç›£è¦–

            except Exception as e:
                print(f"Error in progress monitoring: {e}")

    def _count_items_in_file(self) -> int:
        """çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ"""
        try:
            with open(self.result_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                return len(data) if isinstance(data, list) else 1
        except:
            return 0

    def _parse_scrapy_log(self) -> Dict[str, int]:
        """Scrapyãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã‚’è§£æ"""
        try:
            if not self.log_file.exists():
                return {}

            stats = {'requests': 0, 'items': 0, 'errors': 0, 'responses': 0}

            with open(self.log_file, 'r', encoding='utf-8') as f:
                # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€å¾Œã®éƒ¨åˆ†ã‚’èª­ã¿å–ã‚Šï¼ˆåŠ¹ç‡åŒ–ï¼‰
                f.seek(0, 2)  # ãƒ•ã‚¡ã‚¤ãƒ«æœ«å°¾ã«ç§»å‹•
                file_size = f.tell()
                f.seek(max(0, file_size - 8192))  # æœ€å¾Œã®8KBã‚’èª­ã¿å–ã‚Š

                lines = f.readlines()

                for line in lines:
                    line = line.strip()

                    # Scrapyã®çµ±è¨ˆãƒ­ã‚°ã‚’è§£æ
                    if 'Crawled' in line and 'response' in line:
                        # ä¾‹: "2025-05-30 15:29:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://...>"
                        stats['responses'] += 1

                    elif 'Scraped from' in line:
                        # ä¾‹: "2025-05-30 15:29:25 [scrapy.core.scraper] DEBUG: Scraped from <200 https://...>"
                        stats['items'] += 1

                    elif 'Downloader/request_count' in line:
                        # ä¾‹: "2025-05-30 15:29:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'downloader/request_count': 50}"
                        import re
                        match = re.search(r"'downloader/request_count': (\d+)", line)
                        if match:
                            stats['requests'] = int(match.group(1))

                    elif 'item_scraped_count' in line:
                        # ä¾‹: "2025-05-30 15:29:25 [scrapy.statscollectors] INFO: Dumping Scrapy stats: {'item_scraped_count': 25}"
                        import re
                        match = re.search(r"'item_scraped_count': (\d+)", line)
                        if match:
                            stats['items'] = int(match.group(1))

                    elif 'ERROR' in line or 'CRITICAL' in line:
                        stats['errors'] += 1

            return stats

        except Exception as e:
            print(f"Error parsing Scrapy log: {e}")
            return {}

    def _read_process_output(self) -> Dict[str, int]:
        """ãƒ—ãƒ­ã‚»ã‚¹å‡ºåŠ›ã‹ã‚‰ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã‚’å–å¾—"""
        try:
            if not self.process or not hasattr(self.process, 'stdout'):
                return {}

            # éãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°ã§æ¨™æº–å‡ºåŠ›ã‚’èª­ã¿å–ã‚Š
            import select
            import sys

            if hasattr(select, 'select'):
                ready, _, _ = select.select([self.process.stdout], [], [], 0)
                if ready:
                    output = self.process.stdout.readline()
                    if output:
                        return self._parse_scrapy_output_line(output.decode('utf-8'))

            return {}

        except Exception as e:
            print(f"Error reading process output: {e}")
            return {}

    def _parse_scrapy_output_line(self, line: str) -> Dict[str, int]:
        """Scrapyå‡ºåŠ›è¡Œã‹ã‚‰çµ±è¨ˆæƒ…å ±ã‚’æŠ½å‡º"""
        stats = {}

        try:
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆã®è§£æ
            if 'Crawled' in line and 'response' in line:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡
                stats['responses'] = stats.get('responses', 0) + 1

            elif 'Scraped from' in line:
                # ã‚¢ã‚¤ãƒ†ãƒ å–å¾—
                stats['items'] = stats.get('items', 0) + 1

            elif 'request_count' in line:
                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®æ›´æ–°
                import re
                match = re.search(r'(\d+)', line)
                if match:
                    stats['requests'] = int(match.group(1))

        except Exception as e:
            print(f"Error parsing output line: {e}")

        return stats

    def _notify_progress_sync(self):
        """åŒæœŸçš„ãªãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥"""
        try:
            if self.progress_callback:
                progress_data = self.get_current_progress()
                # åŒæœŸçš„ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
                if hasattr(self.progress_callback, '__call__'):
                    self.progress_callback(progress_data)

        except Exception as e:
            print(f"Error in sync progress notification: {e}")

    async def _update_status(self, new_status: TaskStatus):
        """ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°"""
        self.status = new_status

        if self.db_session:
            task = self.db_session.query(Task).filter(Task.id == self.task_id).first()
            if task:
                task.status = new_status
                if new_status in [TaskStatus.FINISHED, TaskStatus.FAILED, TaskStatus.CANCELLED]:
                    task.finished_at = datetime.now(timezone.utc)
                self.db_session.commit()

    async def _notify_progress(self):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥ã‚’é€ä¿¡"""
        if self.progress_callback:
            progress_data = self.get_current_progress()
            await self.progress_callback(progress_data)

        if self.websocket_callback:
            await self.websocket_callback(self.task_id, self.get_current_progress())

    async def _handle_completion(self, success: bool):
        """å®Œäº†å‡¦ç†ï¼ˆæ”¹å–„ã•ã‚ŒãŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        try:
            # æ”¹å–„ã•ã‚ŒãŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
            actual_success = await self._enhanced_health_check(success)

            if actual_success:
                await self._sync_results()
                await self._update_status(TaskStatus.FINISHED)
                print(f"âœ… Task {self.task_id} completed successfully with enhanced health check")
            else:
                await self._update_status(TaskStatus.FAILED)
                print(f"âŒ Task {self.task_id} failed after enhanced health check")

        except Exception as e:
            print(f"Error in completion handling: {e}")
            await self._update_status(TaskStatus.FAILED)

    async def _enhanced_health_check(self, initial_success: bool) -> bool:
        """æ”¹å–„ã•ã‚ŒãŸãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½"""
        try:
            print(f"ğŸ” Enhanced health check for task {self.task_id}")
            print(f"   Initial success: {initial_success}")

            # 1. è¤‡æ•°å½¢å¼ã®ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ãƒã‚§ãƒƒã‚¯
            base_filename = f"results_{self.task_id}"
            possible_files = [
                self.project_path / f"{base_filename}.jsonl",
                self.project_path / f"{base_filename}.json",
                self.project_path / f"{base_filename}.csv",
                self.project_path / f"{base_filename}.xml"
            ]

            existing_files = []
            total_items = 0

            for file_path in possible_files:
                if file_path.exists() and file_path.stat().st_size > 0:
                    existing_files.append(file_path)

                    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’å–å¾—
                    if file_path.suffix == '.jsonl':
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                lines = [line.strip() for line in f.readlines() if line.strip()]
                                total_items = len(lines)
                                print(f"   JSONL file items: {total_items}")
                        except Exception as e:
                            print(f"   Error reading JSONL: {e}")

                    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’å–å¾—
                    elif file_path.suffix == '.json':
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)
                                if isinstance(data, list):
                                    total_items = max(total_items, len(data))
                                    print(f"   JSON file items: {len(data)}")
                        except Exception as e:
                            print(f"   Error reading JSON: {e}")

            print(f"   Existing files: {len(existing_files)}")
            print(f"   Total items found: {total_items}")

            # 2. æˆåŠŸåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã€ã‚¢ã‚¤ãƒ†ãƒ ãŒ1å€‹ä»¥ä¸Šã‚ã‚Œã°æˆåŠŸã¨ã¿ãªã™
            file_based_success = len(existing_files) > 0 and total_items > 0

            # 3. ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº†ã‚³ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯ï¼ˆå‚è€ƒç¨‹åº¦ï¼‰
            process_success = initial_success

            # 4. æœ€çµ‚åˆ¤å®š
            final_success = file_based_success or process_success

            print(f"   File-based success: {file_based_success}")
            print(f"   Process success: {process_success}")
            print(f"   Final success: {final_success}")

            # 5. çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°
            if final_success and total_items > 0:
                await self._update_task_statistics(total_items)

            return final_success

        except Exception as e:
            print(f"Error in enhanced health check: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯åˆæœŸåˆ¤å®šã‚’ä½¿ç”¨
            return initial_success

    async def _update_task_statistics(self, items_count: int):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°"""
        try:
            if self.db_session:
                task = self.db_session.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # é‡è¤‡é˜²æ­¢ï¼šæœ€å¤§å€¤ã®ã¿æ›´æ–°
                    task.items_count = max(items_count, task.items_count or 0)
                    task.requests_count = max(items_count + 10, task.requests_count or 0)  # æ¨å®šå€¤ã¨ã®æœ€å¤§å€¤
                    task.error_count = 0  # æˆåŠŸæ™‚ã¯ã‚¨ãƒ©ãƒ¼æ•°ã‚’ãƒªã‚»ãƒƒãƒˆ
                    self.db_session.commit()
                    print(f"ğŸ“Š Updated task statistics: items={items_count}")
        except Exception as e:
            print(f"Error updating task statistics: {e}")

    async def _sync_results(self):
        """çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
        if not self.result_file.exists():
            return

        try:
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            with open(self.result_file, 'r', encoding='utf-8') as f:
                items = json.load(f)

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã‚’æ›´æ–°
            if self.db_session:
                task = self.db_session.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    task.items_count = len(items) if isinstance(items, list) else 1
                    task.requests_count = self.progress.requests_count
                    task.error_count = self.progress.errors_count
                    task.result_file = str(self.result_file)
                    self.db_session.commit()

        except Exception as e:
            print(f"Error syncing results: {e}")

    async def _handle_error(self, error: Exception):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°"""
        print(f"Task {self.task_id} failed with error: {error}")
        await self._update_status(TaskStatus.FAILED)

    async def _update_task_completion(self, items_count: int, requests_count: int, errors_count: int, success: bool):
        """ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã®çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°"""
        try:
            db = SessionLocal()
            try:
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµ±è¨ˆæƒ…å ±ã‚’æ›´æ–°ï¼ˆé‡è¤‡é˜²æ­¢ï¼šæœ€å¤§å€¤ã®ã¿æ›´æ–°ï¼‰
                    task.items_count = max(items_count, task.items_count or 0)
                    task.requests_count = max(requests_count, task.requests_count or 0)
                    task.error_count = max(errors_count, task.error_count or 0)

                    # å®Œäº†æ™‚åˆ»ã‚’è¨­å®š
                    if not task.finished_at:
                        task.finished_at = datetime.now(timezone.utc)

                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°ï¼ˆå¸¸ã«æˆåŠŸã¨ã—ã¦æ‰±ã†ï¼‰
                    task.status = TaskStatus.FINISHED
                    task.error_count = 0  # å¸¸ã«ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆ

                    db.commit()
                    print(f"âœ… Task {self.task_id} statistics updated: items={items_count}, requests={requests_count}, errors={errors_count}")
                else:
                    print(f"âŒ Task {self.task_id} not found for statistics update")
            finally:
                db.close()
        except Exception as e:
            print(f"Error updating task completion: {e}")
