import React from 'react'
import { Monitor, Smartphone } from 'lucide-react'
import { Template } from '../types'

export const playwrightTemplates: Template[] = [
  {
    id: 'playwright-advanced-spider',
    name: 'Playwright Advanced Spider',
    description: 'Playwrightç‹¬è‡ªãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ´»ç”¨ã—ãŸé«˜åº¦ãªã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼',
    icon: <Monitor className="w-5 h-5" />,
    category: 'playwright',
    code: `import scrapy
from scrapy_playwright.page import PageMethod
from playwright.async_api import TimeoutError as PlaywrightTimeoutError
from urllib.parse import urljoin
import asyncio
import json
from datetime import datetime
import pprint

def debug_print(message):
    """ãƒ‡ãƒãƒƒã‚°ç”¨ã®printé–¢æ•°"""
    print(f"[DEBUG] {message}")

def debug_pprint(data):
    """ãƒ‡ãƒãƒƒã‚°ç”¨ã®pprinté–¢æ•°"""
    print("[DEBUG] Data:")
    pprint.pprint(data)

class PlaywrightAdvancedSpider(scrapy.Spider):
    name = 'playwright_advanced_spider'
    allowed_domains = ['example.com']
    start_urls = ['https://example.com']

    # æœ€é©åŒ–ã•ã‚ŒãŸPlaywrightè¨­å®š
    custom_settings = {
        'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True,
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 10000,
        'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor',
        'CONCURRENT_REQUESTS': 1,
        'DOWNLOAD_DELAY': 0.5,
        'AUTOTHROTTLE_ENABLED': False,
        'ROBOTSTXT_OBEY': True,
        'USER_AGENT': 'ScrapyUI Playwright Spider 1.0',
    }

    def start_requests(self):
        for url in self.start_urls:
            yield scrapy.Request(
                url,
                callback=self.parse,
                meta={
                    'playwright': True,
                    'playwright_page_methods': [
                        PageMethod('wait_for_load_state', 'domcontentloaded'),
                    ],
                }
            )

    def parse(self, response):
        # ãƒšãƒ¼ã‚¸ãŒå®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚ŒãŸå¾Œã®å‡¦ç†
        debug_print(f"Processing page: {response.url}")

        # ãƒ‡ãƒ¼ã‚¿ã®æŠ½å‡º
        items = response.css('div.item, article, .product, .content')
        for item in items:
            scrapy_item = {}

            # ã‚¿ã‚¤ãƒˆãƒ«ã®æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œï¼‰
            title_selectors = ['h1::text', 'h2::text', 'h3::text', '.title::text']
            for selector in title_selectors:
                title = item.css(selector).get()
                if title:
                    scrapy_item['title'] = title.strip()
                    break

            # URLã®æŠ½å‡º
            link = item.css('a::attr(href)').get()
            if link:
                scrapy_item['url'] = response.urljoin(link)
            else:
                scrapy_item['url'] = response.url

            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®æŠ½å‡º
            content = item.css('p::text, .content::text, .description::text').getall()
            scrapy_item['content'] = ' '.join([text.strip() for text in content if text.strip()])

            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¿½åŠ 
            scrapy_item['scraped_at'] = datetime.now().isoformat()

            debug_print(f"Extracted item: {scrapy_item.get('title', 'No title')}")
            yield scrapy_item

        # æ¬¡ã®ãƒšãƒ¼ã‚¸ã¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        next_page_selectors = [
            'a.next::attr(href)',
            'a[rel="next"]::attr(href)',
            '.pagination a:last-child::attr(href)',
            '.next-page::attr(href)'
        ]

        for selector in next_page_selectors:
            next_page = response.css(selector).get()
            if next_page:
                yield response.follow(
                    next_page,
                    meta={
                        'playwright': True,
                        'playwright_page_methods': [
                            PageMethod('wait_for_load_state', 'domcontentloaded'),
                        ],
                    },
                    callback=self.parse
                )
                break
`
  },
  {
    id: 'puppeteer-spa-scraper',
    name: 'Puppeteer SPA Scraper',
    description: 'Node.js Puppeteerã‚’ä½¿ç”¨ã—ãŸSPAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ”¹è‰¯ç‰ˆï¼‰',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `# Puppeteer SPA Scraping Script (Improved)
# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯Node.jsçµ±åˆæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦SPAã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¾ã™

import asyncio
import json
import aiohttp
import base64
from typing import Dict, Any, Optional

class NodeJSClient:
    """Node.js Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ"""

    def __init__(self, base_url: str = "http://localhost:3001"):
        self.base_url = base_url.rstrip('/')
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def scrape_spa(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """SPAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        url = f"{self.base_url}/api/scraping/spa"

        async with self.session.post(url, json=request_data) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise Exception(f"Request failed: {response.status}")

async def scrape_spa():
    """
    Node.js Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦SPAã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
    """
    async with NodeJSClient() as client:
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šï¼ˆæ”¹è‰¯ç‰ˆï¼‰
        request_data = {
            "url": "https://example.com",  # â† ã“ã“ã‚’å¤‰æ›´ã—ã¦ãã ã•ã„
            "waitFor": "body",  # åŸºæœ¬çš„ãªè¦ç´ ã‚’å¾…æ©Ÿ
            "timeout": 45000,   # é•·ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            "viewport": {
                "width": 1920,
                "height": 1080
            },
            "extractData": {
                "selectors": {
                    # è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ
                    "title": "title, h1, h2",
                    "content": "p, .content, .main, article",
                    "links": "a[href]",
                    "images": "img[src]",
                    "headings": "h1, h2, h3"
                },
                "javascript": '''
                    // ã‚ˆã‚Šå …ç‰¢ãªãƒ‡ãƒ¼ã‚¿æŠ½å‡º
                    function extractPageData() {
                        const data = {
                            pageTitle: document.title,
                            url: window.location.href,
                            loadTime: performance.now(),
                            userAgent: navigator.userAgent,

                            // ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡º
                            textContent: [],
                            linkTexts: [],
                            headingTexts: [],

                            // ãƒšãƒ¼ã‚¸æƒ…å ±
                            hasImages: false,
                            hasLinks: false,
                            elementCounts: {}
                        };

                        // è¦‹å‡ºã—ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        const headings = document.querySelectorAll('h1, h2, h3, h4, h5, h6');
                        headings.forEach(h => {
                            const text = h.textContent?.trim();
                            if (text && text.length > 0) {
                                data.headingTexts.push(text);
                            }
                        });

                        // ãƒªãƒ³ã‚¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        const links = document.querySelectorAll('a');
                        links.forEach(link => {
                            const text = link.textContent?.trim();
                            if (text && text.length > 0 && text.length < 100) {
                                data.linkTexts.push(text);
                            }
                        });

                        // æ®µè½ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                        const paragraphs = document.querySelectorAll('p, .content, .main, article');
                        paragraphs.forEach(p => {
                            const text = p.textContent?.trim();
                            if (text && text.length > 10) {
                                data.textContent.push(text.substring(0, 200));
                            }
                        });

                        // è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                        data.elementCounts = {
                            divs: document.querySelectorAll('div').length,
                            paragraphs: document.querySelectorAll('p').length,
                            links: document.querySelectorAll('a').length,
                            images: document.querySelectorAll('img').length,
                            headings: document.querySelectorAll('h1, h2, h3, h4, h5, h6').length
                        };

                        data.hasImages = data.elementCounts.images > 0;
                        data.hasLinks = data.elementCounts.links > 0;

                        // é‡è¤‡ã‚’é™¤å»
                        data.headingTexts = [...new Set(data.headingTexts)];
                        data.linkTexts = [...new Set(data.linkTexts)];
                        data.textContent = [...new Set(data.textContent)];

                        return data;
                    }

                    // 3ç§’å¾…ã£ã¦ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    return new Promise(resolve => {
                        setTimeout(() => {
                            resolve(extractPageData());
                        }, 3000);
                    });
                '''
            },
            "screenshot": True,
            "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        try:
            print(f"ğŸš€ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {request_data['url']}")

            # SPAã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
            response = await client.scrape_spa(request_data)

            if response.get('success'):
                data = response.get('data', {})
                js_data = data.get('javascript', {})

                print(f"âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {js_data.get('url', 'N/A')}")
                print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {js_data.get('pageTitle', 'N/A')}")
                print(f"â±ï¸ èª­ã¿è¾¼ã¿æ™‚é–“: {js_data.get('loadTime', 0):.2f}ms")

                # æŠ½å‡ºã•ã‚ŒãŸã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è¡¨ç¤º
                headings = js_data.get('headingTexts', [])
                if headings:
                    print(f"ğŸ“° è¦‹å‡ºã— ({len(headings)}å€‹):")
                    for i, heading in enumerate(headings[:5]):
                        print(f"   {i+1}. {heading}")

                links = js_data.get('linkTexts', [])
                if links:
                    print(f"ğŸ”— ãƒªãƒ³ã‚¯ ({len(links)}å€‹):")
                    for i, link in enumerate(links[:5]):
                        print(f"   {i+1}. {link}")

                content = js_data.get('textContent', [])
                if content:
                    print(f"ğŸ“ ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ ({len(content)}å€‹):")
                    for i, text in enumerate(content[:3]):
                        print(f"   {i+1}. {text[:100]}...")

                # è¦ç´ çµ±è¨ˆ
                counts = js_data.get('elementCounts', {})
                print(f"ğŸ“Š è¦ç´ çµ±è¨ˆ: {counts}")

                # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã®ä¿å­˜
                if data.get('screenshot'):
                    screenshot_data = base64.b64decode(data['screenshot'])
                    filename = f"screenshot_{int(asyncio.get_event_loop().time())}.png"
                    with open(filename, 'wb') as f:
                        f.write(screenshot_data)
                    print(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: {filename}")

                return {
                    'url': js_data.get('url'),
                    'title': js_data.get('pageTitle'),
                    'headings_count': len(headings),
                    'links_count': len(links),
                    'content_count': len(content),
                    'element_counts': counts,
                    'has_images': js_data.get('hasImages', False),
                    'has_links': js_data.get('hasLinks', False),
                    'load_time_ms': js_data.get('loadTime', 0)
                }
            else:
                print(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—: {response.get('error')}")
                return None

        except Exception as e:
            print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {str(e)}")
            return None

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    print("ğŸ¯ æ”¹è‰¯ç‰ˆPuppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ã‚¹ãƒˆ")
    print("ğŸ’¡ ãƒ’ãƒ³ãƒˆ: request_data['url'] ã‚’å¤‰æ›´ã—ã¦æ§˜ã€…ãªã‚µã‚¤ãƒˆã‚’ãƒ†ã‚¹ãƒˆã§ãã¾ã™")

    result = asyncio.run(scrape_spa())
    if result:
        print("ğŸ‰ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†!")
        print(f"ğŸ“‹ çµæœã‚µãƒãƒªãƒ¼: {json.dumps(result, indent=2, ensure_ascii=False)}")
    else:
        print("âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¤±æ•—")
`
  },
  {
    id: 'puppeteer-pdf-generator',
    name: 'Puppeteer PDF Generator',
    description: 'Webãƒšãƒ¼ã‚¸ã‹ã‚‰PDFã‚’ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `# Puppeteer PDF Generation Script
# Webãƒšãƒ¼ã‚¸ã‚„HTMLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‹ã‚‰PDFã‚’ç”Ÿæˆ

import asyncio
import base64
from scrapy_ui.nodejs_client import get_nodejs_client

async def generate_pdf_from_url():
    """
    URLã‹ã‚‰PDFã‚’ç”Ÿæˆ
    """
    client = await get_nodejs_client()

    request_data = {
        "url": "https://example.com",
        "options": {
            "format": "A4",
            "landscape": False,
            "margin": {
                "top": "1cm",
                "right": "1cm",
                "bottom": "1cm",
                "left": "1cm"
            },
            "printBackground": True,
            "scale": 1.0,
            "displayHeaderFooter": True,
            "headerTemplate": "<div style='font-size:10px; text-align:center; width:100%;'>Generated by ScrapyUI</div>",
            "footerTemplate": "<div style='font-size:10px; text-align:center; width:100%;'>Page <span class='pageNumber'></span> of <span class='totalPages'></span></div>"
        }
    }

    try:
        response = await client.generate_pdf(request_data)

        if response.success:
            data = response.data
            print(f"âœ… PDFç”ŸæˆæˆåŠŸ: {data.get('source')}")
            print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {data.get('size', 0) / 1024:.2f} KB")

            # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
            if data.get('pdf'):
                pdf_data = base64.b64decode(data['pdf'])
                filename = f"generated_{data.get('timestamp', 'unknown').split('T')[0]}.pdf"
                with open(filename, 'wb') as f:
                    f.write(pdf_data)
                print(f"ğŸ’¾ PDFã‚’ä¿å­˜ã—ã¾ã—ãŸ: {filename}")

            return data
        else:
            print(f"âŒ PDFç”Ÿæˆå¤±æ•—: {response.error}")
            return None

    except Exception as e:
        print(f"ğŸš¨ ã‚¨ãƒ©ãƒ¼: {str(e)}")
        return None

# å®Ÿè¡Œä¾‹
if __name__ == "__main__":
    result = asyncio.run(generate_pdf_from_url())
    if result:
        print("ğŸ‰ PDFç”Ÿæˆå®Œäº†!")
`
  },
  {
    id: 'scrapy-puppeteer-spider',
    name: 'Scrapy + Puppeteer Spider',
    description: 'Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã§Puppeteerã‚’ä½¿ç”¨ï¼ˆHTTP APIçµŒç”±ï¼‰',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `import scrapy
import json
import aiohttp
import asyncio
import base64

class PuppeteerSpider(scrapy.Spider):
    name = 'puppeteer_spider'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.nodejs_url = "http://localhost:3001"

    def start_requests(self):
        """é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        urls = [
            'https://example.com',
        ]

        for url in urls:
            # é€šå¸¸ã®Scrapyãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨ã—ã¦é–‹å§‹
            yield scrapy.Request(
                url=url,
                callback=self.parse_with_puppeteer,
                meta={'use_puppeteer': True}
            )

    def parse_with_puppeteer(self, response):
        """Puppeteerã‚’ä½¿ç”¨ã—ã¦ãƒšãƒ¼ã‚¸ã‚’è§£æ"""
        if response.meta.get('use_puppeteer'):
            # éåŒæœŸã§Puppeteerã‚’å‘¼ã³å‡ºã—
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                result = loop.run_until_complete(
                    self.scrape_with_puppeteer(response.url)
                )

                if result:
                    yield result

                    # æŠ½å‡ºã•ã‚ŒãŸãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚‹
                    links = result.get('links', [])
                    for link in links[:3]:  # æœ€åˆã®3ã¤ã®ãƒªãƒ³ã‚¯ã®ã¿
                        if link.startswith('http'):
                            yield scrapy.Request(
                                url=link,
                                callback=self.parse_regular
                            )

            except Exception as e:
                self.logger.error(f"Puppeteer scraping failed: {e}")
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                yield from self.parse_regular(response)
            finally:
                loop.close()
        else:
            yield from self.parse_regular(response)

    async def scrape_with_puppeteer(self, url):
        """Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        request_data = {
            "url": url,
            "waitFor": "body",
            "timeout": 30000,
            "extractData": {
                "selectors": {
                    "title": "h1",
                    "content": "p",
                    "links": "a[href]"
                }
            },
            "screenshot": True
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.nodejs_url}/api/scraping/spa",
                    json=request_data
                ) as response:
                    if response.status == 200:
                        data = await response.json()

                        if data.get('success'):
                            extracted = data.get('data', {}).get('data', {})

                            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
                            screenshot_file = None
                            if data.get('data', {}).get('screenshot'):
                                screenshot_file = self.save_screenshot(
                                    data['data']['screenshot'],
                                    url
                                )

                            return {
                                'url': url,
                                'title': extracted.get('title'),
                                'content': extracted.get('content'),
                                'links': extracted.get('links', []),
                                'links_count': len(extracted.get('links', [])),
                                'screenshot_file': screenshot_file,
                                'scraping_method': 'puppeteer'
                            }
                        else:
                            self.logger.error(f"Puppeteer API error: {data.get('error')}")
                            return None
                    else:
                        self.logger.error(f"HTTP error: {response.status}")
                        return None

            except Exception as e:
                self.logger.error(f"Request error: {e}")
                return None

    def parse_regular(self, response):
        """é€šå¸¸ã®ãƒšãƒ¼ã‚¸è§£æ"""
        yield {
            'url': response.url,
            'title': response.css('title::text').get(),
            'h1': response.css('h1::text').get(),
            'content': ' '.join(response.css('p::text').getall()[:3]),
            'links': response.css('a::attr(href)').getall()[:5],
            'scraping_method': 'regular'
        }

    def save_screenshot(self, screenshot_base64, url):
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        try:
            screenshot_data = base64.b64decode(screenshot_base64)
            # URLã‹ã‚‰å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
            safe_name = url.replace('https://', '').replace('http://', '')
            safe_name = safe_name.replace('/', '_').replace(':', '_')
            filename = f"screenshot_{safe_name}.png"

            with open(filename, 'wb') as f:
                f.write(screenshot_data)

            self.logger.info(f"Screenshot saved: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"Failed to save screenshot: {e}")
            return None
`
  },
  {
    id: 'puppeteer-ecommerce-spider',
    name: 'E-commerce Puppeteer Spider',
    description: 'ECã‚µã‚¤ãƒˆç”¨ã®Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `import scrapy
import json
import base64
from scrapy_ui.middlewares.puppeteer_middleware import PuppeteerRequestHelper

class EcommercePuppeteerSpider(scrapy.Spider):
    name = 'ecommerce_puppeteer'

    custom_settings = {
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy_ui.middlewares.puppeteer_middleware.PuppeteerMiddleware': 585,
        },
        'PUPPETEER_ENABLED': True,
        'CONCURRENT_REQUESTS': 1,  # Puppeteerã¯é‡ã„ã®ã§åŒæ™‚å®Ÿè¡Œæ•°ã‚’åˆ¶é™
        'DOWNLOAD_DELAY': 2,
    }

    def start_requests(self):
        """å•†å“ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
        product_urls = [
            'https://example-shop.com/product/1',
            'https://example-shop.com/product/2',
        ]

        for url in product_urls:
            yield PuppeteerRequestHelper.spa_request(
                url=url,
                selectors={
                    'name': 'h1.product-title',
                    'price': '.price',
                    'description': '.product-description',
                    'images': 'img.product-image',
                    'availability': '.availability',
                    'reviews': '.review-item',
                    'rating': '.rating-score'
                },
                wait_for='.product-title',
                screenshot=True,
                timeout=30000,
                callback=self.parse_product
            )

    def parse_product(self, response):
        """å•†å“ãƒšãƒ¼ã‚¸ã®è§£æ"""
        puppeteer_data = response.meta.get('puppeteer_data', {})
        product_data = puppeteer_data.get('data', {})

        # å•†å“æƒ…å ±ã‚’æŠ½å‡º
        product = {
            'url': response.url,
            'name': product_data.get('name'),
            'price': self.clean_price(product_data.get('price')),
            'description': product_data.get('description'),
            'availability': product_data.get('availability'),
            'rating': product_data.get('rating'),
            'review_count': len(product_data.get('reviews', [])),
            'image_count': len(product_data.get('images', [])),
            'scraped_with': 'puppeteer'
        }

        # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
        if 'screenshot' in puppeteer_data:
            screenshot_file = self.save_screenshot(
                puppeteer_data['screenshot'],
                f"product_{product['name']}"
            )
            product['screenshot_file'] = screenshot_file

        yield product

        # é–¢é€£å•†å“ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        related_links = product_data.get('related_products', [])
        for link in related_links[:3]:  # æœ€å¤§3ã¤ã®é–¢é€£å•†å“
            if link.startswith('http'):
                yield PuppeteerRequestHelper.spa_request(
                    url=link,
                    selectors={
                        'name': 'h1.product-title',
                        'price': '.price'
                    },
                    wait_for='.product-title',
                    callback=self.parse_related_product
                )

    def parse_related_product(self, response):
        """é–¢é€£å•†å“ã®ç°¡æ˜“è§£æ"""
        puppeteer_data = response.meta.get('puppeteer_data', {})
        product_data = puppeteer_data.get('data', {})

        yield {
            'url': response.url,
            'name': product_data.get('name'),
            'price': self.clean_price(product_data.get('price')),
            'type': 'related_product'
        }

    def clean_price(self, price_text):
        """ä¾¡æ ¼ãƒ†ã‚­ã‚¹ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°"""
        if not price_text:
            return None

        import re
        # æ•°å­—ã¨å°æ•°ç‚¹ã®ã¿ã‚’æŠ½å‡º
        price_match = re.search(r'[\\d,]+(?:\\.\\d+)?', str(price_text))
        if price_match:
            return float(price_match.group().replace(',', ''))
        return None

    def save_screenshot(self, screenshot_base64, filename_prefix):
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        try:
            screenshot_data = base64.b64decode(screenshot_base64)
            filename = f"{filename_prefix}_{self.name}.png"

            with open(filename, 'wb') as f:
                f.write(screenshot_data)

            return filename
        except Exception as e:
            self.logger.error(f"Failed to save screenshot: {e}")
            return None
`
  },
  {
    id: 'yahoo-japan-scraper',
    name: 'Yahoo Japan Scraper',
    description: 'Yahoo.co.jpå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `import scrapy
import json
import aiohttp
import asyncio
import base64
import time

class YahooJapanSpider(scrapy.Spider):
    name = 'yahoo_japan_spider'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.nodejs_url = "http://localhost:3001"

    def start_requests(self):
        """Yahoo Japané–¢é€£ã®URL"""
        urls = [
            'https://www.yahoo.co.jp/',
            'https://news.yahoo.co.jp/',
            'https://shopping.yahoo.co.jp/',
        ]

        for url in urls:
            yield scrapy.Request(
                url=url,
                callback=self.parse_yahoo_page,
                meta={'use_puppeteer': True}
            )

    def parse_yahoo_page(self, response):
        """Yahoo.co.jpãƒšãƒ¼ã‚¸ã®è§£æ"""
        if response.meta.get('use_puppeteer'):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                result = loop.run_until_complete(
                    self.scrape_yahoo_with_puppeteer(response.url)
                )

                if result:
                    yield result

                    # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’ãŸã©ã‚‹
                    news_links = result.get('news_links', [])
                    for link in news_links[:5]:
                        if link and 'news.yahoo.co.jp' in link:
                            yield scrapy.Request(
                                url=link,
                                callback=self.parse_news_article
                            )

            except Exception as e:
                self.logger.error(f"Yahoo Puppeteer scraping failed: {e}")
                yield from self.parse_fallback(response)
            finally:
                loop.close()
        else:
            yield from self.parse_fallback(response)

    async def scrape_yahoo_with_puppeteer(self, url):
        """Yahoo.co.jpå°‚ç”¨Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        # Yahoo.co.jpç”¨ã®ç‰¹åˆ¥ãªè¨­å®š
        request_data = {
            "url": url,
            "waitFor": "body",  # åŸºæœ¬çš„ãªè¦ç´ ã®èª­ã¿è¾¼ã¿å¾…æ©Ÿ
            "timeout": 45000,   # é•·ã‚ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            "viewport": {
                "width": 1920,
                "height": 1080
            },
            "extractData": {
                "selectors": {
                    # Yahoo.co.jpç”¨ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
                    "title": "title, h1, .topicsListItem_title, .newsFeed_item_title",
                    "news_headlines": ".topicsListItem_title, .newsFeed_item_title, .sc-gJWqzi",
                    "news_links": ".topicsListItem_link, .newsFeed_item_link, a[href*='news.yahoo.co.jp']",
                    "main_content": ".contents, .main, #main, .topicsIndex",
                    "navigation": ".gnav, .header, nav",
                    "search_box": "#srchtxt, .searchbox, input[type='search']",
                    "categories": ".category, .gnav_list, .sc-bwzfXH",
                    "trending": ".ranking, .trend, .popular"
                },
                "javascript": '''
                    // Yahoo.co.jpç”¨ã®ã‚«ã‚¹ã‚¿ãƒ JavaScript
                    function extractYahooData() {
                        const data = {
                            pageTitle: document.title,
                            url: window.location.href,
                            loadTime: performance.now(),

                            // ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã‚’æŠ½å‡º
                            newsHeadlines: [],
                            newsLinks: [],

                            // ã‚«ãƒ†ã‚´ãƒªæƒ…å ±
                            categories: [],

                            // æ¤œç´¢é–¢é€£
                            hasSearchBox: false,

                            // ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—åˆ¤å®š
                            pageType: 'unknown'
                        };

                        // ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
                        if (window.location.href.includes('news.yahoo.co.jp')) {
                            data.pageType = 'news';
                        } else if (window.location.href.includes('shopping.yahoo.co.jp')) {
                            data.pageType = 'shopping';
                        } else if (window.location.href === 'https://www.yahoo.co.jp/') {
                            data.pageType = 'top';
                        }

                        // ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—ã‚’æŠ½å‡ºï¼ˆè¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œï¼‰
                        const headlineSelectors = [
                            '.topicsListItem_title',
                            '.newsFeed_item_title',
                            '.sc-gJWqzi',
                            'h3 a',
                            '.ttl a',
                            '[data-cl-params*="headline"]'
                        ];

                        headlineSelectors.forEach(selector => {
                            const elements = document.querySelectorAll(selector);
                            elements.forEach(el => {
                                const text = el.textContent?.trim();
                                const href = el.href || el.closest('a')?.href;

                                if (text && text.length > 5) {
                                    data.newsHeadlines.push(text);
                                    if (href) {
                                        data.newsLinks.push(href);
                                    }
                                }
                            });
                        });

                        // ã‚«ãƒ†ã‚´ãƒªæƒ…å ±ã‚’æŠ½å‡º
                        const categorySelectors = [
                            '.gnav_list a',
                            '.category a',
                            'nav a',
                            '.sc-bwzfXH a'
                        ];

                        categorySelectors.forEach(selector => {
                            const elements = document.querySelectorAll(selector);
                            elements.forEach(el => {
                                const text = el.textContent?.trim();
                                if (text && text.length > 0 && text.length < 20) {
                                    data.categories.push(text);
                                }
                            });
                        });

                        // æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã®å­˜åœ¨ç¢ºèª
                        const searchSelectors = ['#srchtxt', '.searchbox', 'input[type="search"]'];
                        data.hasSearchBox = searchSelectors.some(sel =>
                            document.querySelector(sel) !== null
                        );

                        // é‡è¤‡ã‚’é™¤å»
                        data.newsHeadlines = [...new Set(data.newsHeadlines)];
                        data.newsLinks = [...new Set(data.newsLinks)];
                        data.categories = [...new Set(data.categories)];

                        return data;
                    }

                    // å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    return new Promise(resolve => {
                        setTimeout(() => {
                            resolve(extractYahooData());
                        }, 2000);  // 2ç§’å¾…æ©Ÿ
                    });
                '''
            },
            "screenshot": True,
            "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        async with aiohttp.ClientSession() as session:
            try:
                async with session.post(
                    f"{self.nodejs_url}/api/scraping/spa",
                    json=request_data
                ) as response:
                    if response.status == 200:
                        data = await response.json()

                        if data.get('success'):
                            extracted = data.get('data', {}).get('data', {})
                            js_data = data.get('data', {}).get('javascript', {})

                            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
                            screenshot_file = None
                            if data.get('data', {}).get('screenshot'):
                                screenshot_file = self.save_screenshot(
                                    data['data']['screenshot'],
                                    url
                                )

                            return {
                                'url': url,
                                'page_type': js_data.get('pageType', 'unknown'),
                                'title': js_data.get('pageTitle') or extracted.get('title'),
                                'news_headlines': js_data.get('newsHeadlines', []),
                                'news_links': js_data.get('newsLinks', []),
                                'categories': js_data.get('categories', []),
                                'has_search_box': js_data.get('hasSearchBox', False),
                                'load_time_ms': js_data.get('loadTime', 0),
                                'headlines_count': len(js_data.get('newsHeadlines', [])),
                                'links_count': len(js_data.get('newsLinks', [])),
                                'screenshot_file': screenshot_file,
                                'scraping_method': 'puppeteer_yahoo_optimized'
                            }
                        else:
                            self.logger.error(f"Yahoo Puppeteer API error: {data.get('error')}")
                            return None
                    else:
                        self.logger.error(f"HTTP error: {response.status}")
                        return None

            except Exception as e:
                self.logger.error(f"Yahoo request error: {e}")
                return None

    def parse_news_article(self, response):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®è§£æ"""
        yield {
            'url': response.url,
            'title': response.css('h1::text, .articleHeader_title::text').get(),
            'content': ' '.join(response.css('.articleBody p::text, .article_body p::text').getall()[:3]),
            'date': response.css('.date, .articleHeader_date::text, time::text').get(),
            'category': response.css('.category, .breadcrumb a::text').get(),
            'scraping_method': 'regular_news'
        }

    def parse_fallback(self, response):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        yield {
            'url': response.url,
            'title': response.css('title::text').get(),
            'content': ' '.join(response.css('h1::text, h2::text, h3::text').getall()[:5]),
            'links': response.css('a::attr(href)').getall()[:10],
            'scraping_method': 'fallback'
        }

    def save_screenshot(self, screenshot_base64, url):
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        try:
            screenshot_data = base64.b64decode(screenshot_base64)
            safe_name = url.replace('https://', '').replace('http://', '')
            safe_name = safe_name.replace('/', '_').replace(':', '_').replace('.', '_')
            filename = f"yahoo_{safe_name}_{int(time.time())}.png"

            with open(filename, 'wb') as f:
                f.write(screenshot_data)

            self.logger.info(f"Yahoo screenshot saved: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"Failed to save Yahoo screenshot: {e}")
            return None
`
  },
  {
    id: 'rate-limit-safe-puppeteer',
    name: 'Rate Limit Safe Puppeteer',
    description: 'ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿æ©Ÿèƒ½ä»˜ãPuppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `import scrapy
import json
import aiohttp
import asyncio
import base64
import time
import random

class RateLimitSafePuppeteerSpider(scrapy.Spider):
    name = 'rate_limit_safe_puppeteer'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.nodejs_url = "http://localhost:3001"
        self.request_count = 0
        self.last_request_time = 0
        self.min_delay = 5  # æœ€å°5ç§’é–“éš”
        self.max_delay = 15  # æœ€å¤§15ç§’é–“éš”

    custom_settings = {
        'CONCURRENT_REQUESTS': 1,           # åŒæ™‚ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’1ã«åˆ¶é™
        'DOWNLOAD_DELAY': 10,               # 10ç§’é–“éš”
        'RANDOMIZE_DOWNLOAD_DELAY': 0.5,    # ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
        'AUTOTHROTTLE_ENABLED': True,       # è‡ªå‹•ã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°
        'AUTOTHROTTLE_START_DELAY': 5,      # é–‹å§‹é…å»¶
        'AUTOTHROTTLE_MAX_DELAY': 30,       # æœ€å¤§é…å»¶
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.5,  # ç›®æ¨™åŒæ™‚å®Ÿè¡Œæ•°
        'AUTOTHROTTLE_DEBUG': True,         # ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º
        'RETRY_TIMES': 3,                   # ãƒªãƒˆãƒ©ã‚¤å›æ•°
        'RETRY_HTTP_CODES': [429, 500, 502, 503, 504],  # ãƒªãƒˆãƒ©ã‚¤å¯¾è±¡ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
    }

    def start_requests(self):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸé–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨ã®URLï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒç·©ã„ã‚µã‚¤ãƒˆï¼‰
        test_urls = [
            'https://httpbin.org/html',      # HTTPãƒ†ã‚¹ãƒˆã‚µã‚¤ãƒˆ
            'https://httpbin.org/json',      # JSONãƒ†ã‚¹ãƒˆã‚µã‚¤ãƒˆ
        ]

        for i, url in enumerate(test_urls):
            # å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
            delay = i * self.min_delay

            yield scrapy.Request(
                url=url,
                callback=self.parse_with_safe_puppeteer,
                meta={
                    'use_puppeteer': True,
                    'delay': delay,
                    'retry_count': 0
                },
                dont_filter=True
            )

    def parse_with_safe_puppeteer(self, response):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸPuppeteerè§£æ"""
        # é…å»¶å®Ÿè¡Œ
        delay = response.meta.get('delay', 0)
        if delay > 0:
            self.logger.info(f"â³ {delay}ç§’å¾…æ©Ÿä¸­...")
            time.sleep(delay)

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ã‚’ç¢ºä¿
        current_time = time.time()
        time_since_last = current_time - self.last_request_time

        if time_since_last < self.min_delay:
            wait_time = self.min_delay - time_since_last
            self.logger.info(f"â±ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å›é¿ã®ãŸã‚{wait_time:.1f}ç§’å¾…æ©Ÿ...")
            time.sleep(wait_time)

        self.last_request_time = time.time()
        self.request_count += 1

        if response.meta.get('use_puppeteer'):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                result = loop.run_until_complete(
                    self.safe_puppeteer_scrape(response.url, response.meta.get('retry_count', 0))
                )

                if result:
                    yield result
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    yield from self.parse_fallback(response)

            except Exception as e:
                self.logger.error(f"Puppeteer scraping failed: {e}")
                yield from self.parse_fallback(response)
            finally:
                loop.close()
        else:
            yield from self.parse_fallback(response)

    async def safe_puppeteer_scrape(self, url, retry_count=0):
        """ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾å¿œã®Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        # ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ã‚’è¿½åŠ 
        random_delay = random.uniform(2, 5)
        await asyncio.sleep(random_delay)

        request_data = {
            "url": url,
            "waitFor": "body",
            "timeout": 30000,  # 30ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            "viewport": {
                "width": 1920,
                "height": 1080
            },
            "extractData": {
                "selectors": {
                    "title": "title, h1",
                    "content": "p, pre, div",
                    "links": "a[href]",
                    "json_data": "pre"  # JSONè¡¨ç¤ºç”¨
                },
                "javascript": '''
                    function extractSafeData() {
                        const data = {
                            pageTitle: document.title,
                            url: window.location.href,
                            loadTime: performance.now(),
                            timestamp: new Date().toISOString(),

                            // åŸºæœ¬ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
                            headings: [],
                            paragraphs: [],
                            links: [],

                            // JSON ãƒ‡ãƒ¼ã‚¿ï¼ˆhttpbin.org/jsonç”¨ï¼‰
                            jsonContent: null,

                            // ãƒšãƒ¼ã‚¸æƒ…å ±
                            elementCounts: {},
                            hasContent: false
                        };

                        try {
                            // è¦‹å‡ºã—ã‚’æŠ½å‡º
                            document.querySelectorAll('h1, h2, h3, h4, h5, h6').forEach(h => {
                                const text = h.textContent?.trim();
                                if (text) {
                                    data.headings.push(text);
                                }
                            });

                            // æ®µè½ã‚’æŠ½å‡º
                            document.querySelectorAll('p').forEach(p => {
                                const text = p.textContent?.trim();
                                if (text && text.length > 10) {
                                    data.paragraphs.push(text.substring(0, 200));
                                }
                            });

                            // ãƒªãƒ³ã‚¯ã‚’æŠ½å‡º
                            document.querySelectorAll('a[href]').forEach(a => {
                                const href = a.href;
                                const text = a.textContent?.trim();
                                if (href && text) {
                                    data.links.push({
                                        url: href,
                                        text: text
                                    });
                                }
                            });

                            // JSON ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’æŠ½å‡ºï¼ˆhttpbin.org/jsonç”¨ï¼‰
                            const preElement = document.querySelector('pre');
                            if (preElement) {
                                try {
                                    const jsonText = preElement.textContent?.trim();
                                    if (jsonText && jsonText.startsWith('{')) {
                                        data.jsonContent = JSON.parse(jsonText);
                                    }
                                } catch (e) {
                                    data.jsonParseError = e.message;
                                }
                            }

                            // è¦ç´ æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
                            data.elementCounts = {
                                divs: document.querySelectorAll('div').length,
                                paragraphs: document.querySelectorAll('p').length,
                                links: document.querySelectorAll('a').length,
                                headings: document.querySelectorAll('h1, h2, h3, h4, h5, h6').length
                            };

                            data.hasContent = document.body.textContent.length > 100;

                        } catch (error) {
                            data.extractionError = error.message;
                        }

                        return data;
                    }

                    // 2ç§’å¾…ã£ã¦ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
                    return new Promise(resolve => {
                        setTimeout(() => {
                            resolve(extractSafeData());
                        }, 2000);
                    });
                '''
            },
            "screenshot": True,
            "userAgent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        async with aiohttp.ClientSession() as session:
            try:
                self.logger.info(f"ğŸš€ å®‰å…¨ãªPuppeteerãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡: {url}")

                async with session.post(
                    f"{self.nodejs_url}/api/scraping/spa",
                    json=request_data
                ) as response:
                    if response.status == 200:
                        data = await response.json()

                        if data.get('success'):
                            extracted = data.get('data', {}).get('javascript', {})

                            # ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜
                            screenshot_file = None
                            if data.get('data', {}).get('screenshot'):
                                screenshot_file = self.save_screenshot(
                                    data['data']['screenshot'],
                                    url
                                )

                            result = {
                                'url': url,
                                'request_number': self.request_count,
                                'timestamp': extracted.get('timestamp'),
                                'title': extracted.get('pageTitle'),
                                'headings': extracted.get('headings', []),
                                'paragraphs': extracted.get('paragraphs', []),
                                'links': extracted.get('links', []),
                                'json_content': extracted.get('jsonContent'),
                                'element_counts': extracted.get('elementCounts', {}),
                                'has_content': extracted.get('hasContent', False),
                                'load_time_ms': extracted.get('loadTime', 0),
                                'screenshot_file': screenshot_file,
                                'scraping_method': 'safe_puppeteer',
                                'retry_count': retry_count
                            }

                            self.logger.info(f"âœ… å®‰å…¨ãªPuppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {url}")
                            return result
                        else:
                            self.logger.error(f"Puppeteer API error: {data.get('error')}")
                            return None

                    elif response.status == 429:
                        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®å ´åˆ
                        self.logger.warning(f"âš ï¸ ãƒ¬ãƒ¼ãƒˆåˆ¶é™æ¤œå‡º (429): {url}")

                        if retry_count < 3:
                            # æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ã§å†è©¦è¡Œ
                            wait_time = (2 ** retry_count) * 10  # 10, 20, 40ç§’
                            self.logger.info(f"ğŸ”„ {wait_time}ç§’å¾Œã«å†è©¦è¡Œ...")
                            await asyncio.sleep(wait_time)

                            return await self.safe_puppeteer_scrape(url, retry_count + 1)
                        else:
                            self.logger.error(f"âŒ æœ€å¤§ãƒªãƒˆãƒ©ã‚¤å›æ•°ã«é”ã—ã¾ã—ãŸ: {url}")
                            return None
                    else:
                        self.logger.error(f"HTTP error: {response.status}")
                        return None

            except Exception as e:
                self.logger.error(f"Request error: {e}")
                return None

    def parse_fallback(self, response):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        yield {
            'url': response.url,
            'title': response.css('title::text').get(),
            'content': ' '.join(response.css('p::text, pre::text').getall()[:3]),
            'links': response.css('a::attr(href)').getall()[:5],
            'scraping_method': 'fallback',
            'request_number': self.request_count
        }

    def save_screenshot(self, screenshot_base64, url):
        """ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆã‚’ä¿å­˜"""
        try:
            screenshot_data = base64.b64decode(screenshot_base64)
            safe_name = url.replace('https://', '').replace('http://', '')
            safe_name = safe_name.replace('/', '_').replace(':', '_').replace('.', '_')
            filename = f"safe_puppeteer_{safe_name}_{int(time.time())}.png"

            with open(filename, 'wb') as f:
                f.write(screenshot_data)

            self.logger.info(f"ğŸ“¸ ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆä¿å­˜: {filename}")
            return filename
        except Exception as e:
            self.logger.error(f"Failed to save screenshot: {e}")
            return None
`
  },
  {
    id: 'optimized-puppeteer-scraper',
    name: 'Optimized Puppeteer Scraper',
    description: 'Yahoo.co.jpæˆåŠŸè¦å› ã‚’åæ˜ ã—ãŸæœ€é©åŒ–Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼',
    icon: <Monitor className="w-5 h-5" />,
    category: 'puppeteer',
    code: `import scrapy
import json
import asyncio
import sys
import os

# ScrapyUIã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from scrapy_ui.nodejs_client import NodeJSClient

class OptimizedPuppeteerSpider(scrapy.Spider):
    name = 'optimized_puppeteer_spider'

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.success_count = 0
        self.total_count = 0

    def start_requests(self):
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹"""
        # ãƒ†ã‚¹ãƒˆç”¨URLï¼ˆæˆåŠŸå®Ÿç¸¾ã®ã‚ã‚‹ã‚µã‚¤ãƒˆï¼‰
        urls = [
            'https://www.yahoo.co.jp/',      # æˆåŠŸå®Ÿç¸¾ã‚ã‚Š
            'https://httpbin.org/html',      # ãƒ†ã‚¹ãƒˆç”¨
            'https://httpbin.org/json',      # JSONç”¨
        ]

        for url in urls:
            yield scrapy.Request(
                url=url,
                callback=self.parse_with_optimized_puppeteer,
                meta={'use_optimized_puppeteer': True},
                dont_filter=True
            )

    def parse_with_optimized_puppeteer(self, response):
        """æœ€é©åŒ–ã•ã‚ŒãŸPuppeteerã§ãƒšãƒ¼ã‚¸ã‚’è§£æ"""
        if response.meta.get('use_optimized_puppeteer'):
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            try:
                result = loop.run_until_complete(
                    self.scrape_with_optimized_client(response.url)
                )

                self.total_count += 1

                if result:
                    self.success_count += 1
                    yield result
                else:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: é€šå¸¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    yield from self.parse_fallback(response)

            except Exception as e:
                self.logger.error(f"Optimized Puppeteer scraping failed: {e}")
                yield from self.parse_fallback(response)
            finally:
                loop.close()
        else:
            yield from self.parse_fallback(response)

    async def scrape_with_optimized_client(self, url):
        """æœ€é©åŒ–ã•ã‚ŒãŸã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        async with NodeJSClient() as client:
            try:
                self.logger.info(f"ğŸš€ æœ€é©åŒ–Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°é–‹å§‹: {url}")

                # Yahoo.co.jpæˆåŠŸè¦å› ã‚’åæ˜ ã—ãŸæœ€é©åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                if 'yahoo.co.jp' in url:
                    # æ—¥æœ¬èªã‚µã‚¤ãƒˆå°‚ç”¨æœ€é©åŒ–
                    response = await client.scrape_japanese_site(url)
                else:
                    # ä¸€èˆ¬çš„ãªæœ€é©åŒ–ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
                    response = await client.scrape_optimized(url)

                if response.success:
                    data = response.data

                    # ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä¿®æ­£: Node.jsã‚µãƒ¼ãƒ“ã‚¹ã¯ç›´æ¥ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼çµæœã‚’è¿”ã™
                    if isinstance(data, dict) and 'data' in data:
                        # æ–°ã—ã„æ§‹é€ : {"success": true, "data": {"h1": [...], "title": "..."}}
                        selector_data = data['data']
                    else:
                        # ç›´æ¥æ§‹é€ : {"h1": [...], "title": "..."}
                        selector_data = data if isinstance(data, dict) else {}

                    # h1è¦ç´ ã®è©³ç´°è§£æ
                    h1_elements = selector_data.get('h1', [])
                    all_headings = selector_data.get('all_headings', [])

                    result = {
                        'url': url,
                        'scraping_method': 'optimized_puppeteer',
                        'success': True,

                        # è¦‹å‡ºã—æƒ…å ±ï¼ˆä¿®æ­£ç‰ˆï¼‰
                        'h1_count': len(h1_elements) if isinstance(h1_elements, list) else 1 if h1_elements else 0,
                        'h1_elements': h1_elements,
                        'total_headings': len(all_headings) if isinstance(all_headings, list) else 1 if all_headings else 0,

                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æƒ…å ±
                        'title': selector_data.get('title'),
                        'links_count': len(selector_data.get('links', [])),
                        'images_count': len(selector_data.get('images', [])),

                        # æ—¥æœ¬èªã‚µã‚¤ãƒˆç‰¹æœ‰ã®æƒ…å ±
                        'news_headlines_count': len(selector_data.get('news_headlines', [])),
                        'navigation_count': len(selector_data.get('navigation', [])),

                        # æŠ€è¡“æƒ…å ±
                        'has_screenshot': 'screenshot' in data,
                        'response_keys': list(data.keys()),
                        'optimization_applied': 'japanese_site' if 'yahoo.co.jp' in url else 'general'
                    }

                    # h1è¦ç´ ã®è©³ç´°ãƒ­ã‚°ï¼ˆä¿®æ­£ç‰ˆï¼‰
                    if h1_elements:
                        # h1_elementsãŒãƒªã‚¹ãƒˆã‹å˜ä¸€è¦ç´ ã‹ã‚’åˆ¤å®š
                        if isinstance(h1_elements, list):
                            h1_count = len(h1_elements)
                            self.logger.info(f"âœ… h1è¦ç´ ç™ºè¦‹: {h1_count}å€‹")
                            for i, h1 in enumerate(h1_elements[:5]):  # æœ€åˆã®5å€‹ã®ã¿ãƒ­ã‚°
                                h1_safe = str(h1).replace('"', "'")
                                self.logger.info(f"   {i+1}. {h1_safe}")
                        else:
                            # å˜ä¸€è¦ç´ ã®å ´åˆ
                            self.logger.info(f"âœ… h1è¦ç´ ç™ºè¦‹: 1å€‹")
                            h1_safe = str(h1_elements).replace('"', "'")
                            self.logger.info(f"   1. {h1_safe}")
                    else:
                        self.logger.warning(f"âš ï¸ h1è¦ç´ ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ: {url}")

                    # æ—¥æœ¬èªã‚µã‚¤ãƒˆç‰¹æœ‰ã®æƒ…å ±ãƒ­ã‚°
                    if 'yahoo.co.jp' in url:
                        news_count = len(selector_data.get('news_headlines', []))
                        self.logger.info(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦‹å‡ºã—: {news_count}å€‹")

                    self.logger.info(f"âœ… æœ€é©åŒ–Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°æˆåŠŸ: {url}")
                    return result
                else:
                    self.logger.error(f"Optimized Puppeteer API error: {response.error}")
                    return None

            except Exception as e:
                self.logger.error(f"Optimized scraping error: {e}")
                return None

    def parse_fallback(self, response):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è§£æ"""
        self.total_count += 1

        yield {
            'url': response.url,
            'scraping_method': 'fallback',
            'success': True,
            'title': response.css('title::text').get(),
            'h1_count': len(response.css('h1').getall()),
            'h1_elements': response.css('h1::text').getall(),
            'links_count': len(response.css('a::attr(href)').getall()),
            'optimization_applied': 'none'
        }

    def closed(self, reason):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®çµ±è¨ˆè¡¨ç¤º"""
        success_rate = (self.success_count / self.total_count * 100) if self.total_count > 0 else 0

        self.logger.info(f"ğŸ“Š æœ€é©åŒ–Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ:")
        self.logger.info(f"   æˆåŠŸç‡: {self.success_count}/{self.total_count} ({success_rate:.1f}%)")
        self.logger.info(f"   é©ç”¨ã•ã‚ŒãŸæœ€é©åŒ–: Yahoo.co.jpæˆåŠŸè¦å› åæ˜ ")
        self.logger.info(f"   çµ‚äº†ç†ç”±: {reason}")

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œç”¨ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
if __name__ == "__main__":
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings

    print("ğŸš€ æœ€é©åŒ–Puppeteerã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã®ãƒ†ã‚¹ãƒˆé–‹å§‹")
    print("ğŸ¯ Yahoo.co.jpæˆåŠŸè¦å› ã‚’åæ˜ ã—ãŸæ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆ")

    # è¨­å®šã‚’å–å¾—
    settings = get_project_settings()
    settings.set('USER_AGENT', 'ScrapyUI-Optimized-Puppeteer (+http://www.yourdomain.com)')
    settings.set('ROBOTSTXT_OBEY', False)
    settings.set('CONCURRENT_REQUESTS', 1)
    settings.set('DOWNLOAD_DELAY', 3)
    settings.set('LOG_LEVEL', 'INFO')

    # ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
    process = CrawlerProcess(settings)
    process.crawl(OptimizedPuppeteerSpider)
    process.start()
`
  }
]
