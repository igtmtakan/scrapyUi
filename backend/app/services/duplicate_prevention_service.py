"""
é‡è¤‡é˜²æ­¢ã‚µãƒ¼ãƒ“ã‚¹
è¤‡æ•°ã®ãƒ‡ãƒ¼ã‚¿ä¿å­˜çµŒè·¯ã§ã®é‡è¤‡ã‚’é˜²æ­¢ã™ã‚‹
"""
import hashlib
import json
from typing import Dict, Any, List, Optional
from datetime import datetime
from sqlalchemy.orm import Session
from sqlalchemy import func
from ..database import Result, Task

class DuplicatePreventionService:
    """é‡è¤‡é˜²æ­¢ã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, db: Session):
        self.db = db
    
    def generate_content_hash(self, item_data: Dict[str, Any]) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ"""
        try:
            # é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ã¿ã‚’ä½¿ç”¨
            key_fields = ['title', 'product_url', 'ranking_position', 'price', 'rating']
            hash_data = {}
            
            for field in key_fields:
                if field in item_data and item_data[field] is not None:
                    hash_data[field] = str(item_data[field]).strip()
            
            # URLã‹ã‚‰ASINã‚’æŠ½å‡º
            product_url = item_data.get('product_url', '')
            if '/dp/' in product_url:
                asin = product_url.split('/dp/')[1].split('/')[0]
                hash_data['asin'] = asin
            
            # ã‚½ãƒ¼ãƒˆã•ã‚ŒãŸè¾æ›¸ã‹ã‚‰æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
            data_str = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
            
        except Exception as e:
            print(f"âŒ Error generating content hash: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå…¨ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥
            data_str = json.dumps(item_data, sort_keys=True, ensure_ascii=False)
            return hashlib.sha256(data_str.encode('utf-8')).hexdigest()
    
    def check_duplicate_by_hash(self, task_id: str, data_hash: str) -> bool:
        """ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        try:
            existing = self.db.query(Result).filter(
                Result.task_id == task_id,
                Result.data_hash == data_hash
            ).first()
            return existing is not None
        except Exception as e:
            print(f"âŒ Error checking duplicate by hash: {e}")
            return False
    
    def check_duplicate_by_content(self, task_id: str, item_data: Dict[str, Any]) -> bool:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        try:
            # ASINãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
            product_url = item_data.get('product_url', '')
            if '/dp/' in product_url:
                asin = product_url.split('/dp/')[1].split('/')[0]
                existing = self.db.query(Result).filter(
                    Result.task_id == task_id,
                    Result.data['product_url'].astext.like(f'%/dp/{asin}/%')
                ).first()
                if existing:
                    return True
            
            # ã‚¿ã‚¤ãƒˆãƒ« + ãƒ©ãƒ³ã‚­ãƒ³ã‚°ä½ç½®ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
            title = item_data.get('title', '').strip()
            ranking_position = item_data.get('ranking_position')
            
            if title and ranking_position:
                existing = self.db.query(Result).filter(
                    Result.task_id == task_id,
                    Result.data['title'].astext == title,
                    Result.data['ranking_position'].astext == str(ranking_position)
                ).first()
                if existing:
                    return True
            
            return False
            
        except Exception as e:
            print(f"âŒ Error checking duplicate by content: {e}")
            return False
    
    def is_duplicate(self, task_id: str, item_data: Dict[str, Any], data_hash: str = None) -> bool:
        """ç·åˆçš„ãªé‡è¤‡ãƒã‚§ãƒƒã‚¯"""
        try:
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
            if data_hash and self.check_duplicate_by_hash(task_id, data_hash):
                return True
            
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯
            if self.check_duplicate_by_content(task_id, item_data):
                return True
            
            return False
            
        except Exception as e:
            print(f"âŒ Error in duplicate check: {e}")
            return False
    
    def get_duplicate_stats(self, task_id: str) -> Dict[str, int]:
        """é‡è¤‡çµ±è¨ˆã‚’å–å¾—"""
        try:
            total_results = self.db.query(Result).filter(Result.task_id == task_id).count()
            
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
            hash_duplicates = self.db.query(Result.data_hash).filter(
                Result.task_id == task_id,
                Result.data_hash.isnot(None)
            ).group_by(Result.data_hash).having(func.count(Result.data_hash) > 1).count()
            
            return {
                'total_results': total_results,
                'hash_duplicates': hash_duplicates,
                'unique_results': total_results - hash_duplicates
            }
            
        except Exception as e:
            print(f"âŒ Error getting duplicate stats: {e}")
            return {'total_results': 0, 'hash_duplicates': 0, 'unique_results': 0}
    
    def cleanup_duplicates(self, task_id: str, dry_run: bool = True) -> Dict[str, int]:
        """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            stats = {'removed': 0, 'kept': 0}
            
            # ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹ã®é‡è¤‡ã‚’æ¤œç´¢
            duplicate_hashes = self.db.query(Result.data_hash).filter(
                Result.task_id == task_id,
                Result.data_hash.isnot(None)
            ).group_by(Result.data_hash).having(func.count(Result.data_hash) > 1).all()
            
            for (data_hash,) in duplicate_hashes:
                # åŒã˜ãƒãƒƒã‚·ãƒ¥ã®çµæœã‚’å–å¾—ï¼ˆæœ€æ–°ã‚’é™¤ãï¼‰
                duplicates = self.db.query(Result).filter(
                    Result.task_id == task_id,
                    Result.data_hash == data_hash
                ).order_by(Result.created_at.desc()).offset(1).all()
                
                for duplicate in duplicates:
                    if not dry_run:
                        self.db.delete(duplicate)
                    stats['removed'] += 1
                
                stats['kept'] += 1
            
            if not dry_run:
                self.db.commit()
                print(f"âœ… Cleanup completed: {stats['removed']} duplicates removed, {stats['kept']} unique items kept")
            else:
                print(f"ğŸ” Dry run: Would remove {stats['removed']} duplicates, keep {stats['kept']} unique items")
            
            return stats
            
        except Exception as e:
            if not dry_run:
                self.db.rollback()
            print(f"âŒ Error during cleanup: {e}")
            return {'removed': 0, 'kept': 0}
