"""
ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡ŒAPI
ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã‹ã‚‰ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œã¨ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºçµæœã®è¿”å´
"""
import asyncio
import tempfile
import subprocess
import json
import os
import sys
from pathlib import Path
from typing import Dict, List, Any, Optional
from fastapi import APIRouter, HTTPException, Depends, BackgroundTasks
from pydantic import BaseModel
import uuid
from datetime import datetime
import traceback
import re

from ..database import get_db, User, Task
from sqlalchemy.orm import Session
from .auth import get_current_active_user

router = APIRouter()

class ScriptExecutionRequest(BaseModel):
    script_content: str
    spider_name: str
    start_urls: List[str] = ["https://example.com"]
    settings: Dict[str, Any] = {}
    project_id: Optional[str] = None
    spider_id: Optional[str] = None

class ScriptSaveRequest(BaseModel):
    file_name: str
    content: str

class ScriptExecutionResponse(BaseModel):
    execution_id: str
    status: str
    output: List[str]
    errors: List[str]
    extracted_data: List[Dict[str, Any]]
    execution_time: float
    started_at: str
    finished_at: Optional[str] = None

# å®Ÿè¡Œä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’ç®¡ç†
running_executions: Dict[str, Dict[str, Any]] = {}

# å®Ÿè¡Œå±¥æ­´ã‚’ç®¡ç†ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯æ°¸ç¶šåŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½¿ç”¨ï¼‰
execution_history: Dict[str, List[Dict[str, Any]]] = {}

def save_execution_history(user_id: int, execution_data: Dict[str, Any]):
    """å®Ÿè¡Œå±¥æ­´ã‚’ä¿å­˜"""
    user_key = str(user_id)
    if user_key not in execution_history:
        execution_history[user_key] = []

    execution_history[user_key].append(execution_data)

    # æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
    if len(execution_history[user_key]) > 100:
        execution_history[user_key] = execution_history[user_key][-100:]

def get_execution_history(user_id: int, limit: int = 20) -> List[Dict[str, Any]]:
    """å®Ÿè¡Œå±¥æ­´ã‚’å–å¾—"""
    user_key = str(user_id)
    if user_key not in execution_history:
        return []

    # æœ€æ–°ã®ã‚‚ã®ã‹ã‚‰è¿”ã™
    return execution_history[user_key][-limit:][::-1]

@router.post("/test", response_model=ScriptExecutionResponse)
async def test_script_simple(
    request: ScriptExecutionRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    ç°¡å˜ãªãƒ†ã‚¹ãƒˆç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œï¼ˆPlaywrightãªã—ï¼‰
    """
    execution_id = str(uuid.uuid4())
    started_at = datetime.now().isoformat()

    try:
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        test_data = [
            {
                "url": url,
                "title": f"Test Title for {url}",
                "status": "success",
                "timestamp": datetime.now().isoformat()
            }
            for url in request.start_urls
        ]

        finished_at = datetime.now().isoformat()

        return ScriptExecutionResponse(
            execution_id=execution_id,
            status="completed",
            output=[
                f"âœ… Test execution started at {started_at}",
                f"ğŸ“ Spider name: {request.spider_name}",
                f"ğŸ”— Processing {len(request.start_urls)} URLs",
                f"âœ… Test execution completed at {finished_at}"
            ],
            errors=[],
            extracted_data=test_data,
            execution_time=1.0,
            started_at=started_at,
            finished_at=finished_at
        )

    except Exception as e:
        import traceback
        error_details = traceback.format_exc()
        print(f"âŒ Script execution error: {str(e)}")
        print(f"âŒ Error details: {error_details}")

        return ScriptExecutionResponse(
            execution_id=execution_id,
            status="failed",
            output=[],
            errors=[str(e), error_details],
            extracted_data=[],
            execution_time=0.0,
            started_at=started_at,
            finished_at=datetime.now().isoformat()
        )

@router.post("/execute", response_model=ScriptExecutionResponse)
async def execute_script(
    request: ScriptExecutionRequest,
    background_tasks: BackgroundTasks,
    current_user: User = Depends(get_current_active_user),
    db: Session = Depends(get_db)
):
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆCeleryã‚¿ã‚¹ã‚¯ã‚’ä½¿ç”¨ã—ã¦Reactorç«¶åˆå›é¿ï¼‰
    """
    execution_id = str(uuid.uuid4())
    started_at = datetime.now().isoformat()

    # ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
    project_id = request.project_id or "webui-execution"
    spider_id = request.spider_id or request.spider_name

    task = Task(
        id=execution_id,
        project_id=project_id,
        spider_id=spider_id,
        status="PENDING",
        user_id=current_user.id,
        log_level="INFO",
        settings=request.settings or {}
    )
    db.add(task)
    db.commit()

    # å®Ÿè¡ŒçŠ¶æ…‹ã‚’åˆæœŸåŒ–
    execution_state = {
        "status": "pending",
        "output": [],
        "errors": [],
        "extracted_data": [],
        "started_at": started_at,
        "finished_at": None
    }
    running_executions[execution_id] = execution_state

    try:
        print(f"ğŸš€ Starting script execution via Celery: {request.spider_name}")
        print(f"ğŸ“ Start URLs: {request.start_urls}")
        print(f"âš™ï¸ Settings: {request.settings}")

        # Celeryã‚¿ã‚¹ã‚¯ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œï¼ˆReactorç«¶åˆå›é¿ï¼‰
        from ..tasks.script_tasks import run_script_task

        celery_task = run_script_task.delay(
            execution_id=execution_id,
            script_content=request.script_content,
            spider_name=request.spider_name,
            start_urls=request.start_urls,
            settings=request.settings or {},
            user_id=current_user.id
        )

        # Celeryã‚¿ã‚¹ã‚¯IDã‚’è¨˜éŒ²
        task.celery_task_id = celery_task.id
        db.commit()

        print(f"âœ… Celery script task started: {celery_task.id}")

        # å³åº§ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¿”ã™ï¼ˆéåŒæœŸå®Ÿè¡Œï¼‰
        return ScriptExecutionResponse(
            execution_id=execution_id,
            status="pending",
            output=[],
            errors=[],
            extracted_data=[],
            execution_time=0.0,
            started_at=started_at,
            finished_at=None
        )

    except Exception as e:
        execution_state.update({
            "status": "failed",
            "errors": [str(e), traceback.format_exc()],
            "finished_at": datetime.now().isoformat()
        })

        # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’æ›´æ–°
        task.status = "FAILED"
        task.finished_at = datetime.now()
        db.commit()

        return ScriptExecutionResponse(
            execution_id=execution_id,
            status="failed",
            output=[],
            errors=[str(e)],
            extracted_data=[],
            execution_time=0.0,
            started_at=started_at,
            finished_at=execution_state["finished_at"]
        )

@router.get("/execution/{execution_id}", response_model=ScriptExecutionResponse)
async def get_execution_status(
    execution_id: str,
    current_user: User = Depends(get_current_active_user)
):
    """
    å®Ÿè¡ŒçŠ¶æ…‹ã‚’å–å¾—
    """
    if execution_id not in running_executions:
        raise HTTPException(status_code=404, detail="Execution not found")

    state = running_executions[execution_id]

    return ScriptExecutionResponse(
        execution_id=execution_id,
        status=state["status"],
        output=state["output"],
        errors=state["errors"],
        extracted_data=state["extracted_data"],
        execution_time=0.0,  # TODO: å®Ÿéš›ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
        started_at=state["started_at"],
        finished_at=state["finished_at"]
    )

async def execute_scrapy_script(
    script_content: str,
    spider_name: str,
    start_urls: List[str],
    settings: Dict[str, Any],
    use_playwright: bool = False
) -> Dict[str, Any]:
    """
    Scrapyã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
    """
    start_time = datetime.now()
    output_lines = []
    error_lines = []
    extracted_data = []

    try:
        print(f"ğŸ”§ Creating temporary directory for script execution")

        # ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            print(f"ğŸ“ Temp directory: {temp_path}")

            # Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åŸºæœ¬æ§‹é€ ã‚’ä½œæˆ
            project_dir = temp_path / "temp_project"
            project_dir.mkdir()
            print(f"ğŸ“ Project directory: {project_dir}")

            spiders_dir = project_dir / "spiders"
            spiders_dir.mkdir()

            # __init__.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            (project_dir / "__init__.py").write_text("")
            (spiders_dir / "__init__.py").write_text("")

            # items.pyã‚’ä½œæˆ
            items_content = """
import scrapy

class TempProjectItem(scrapy.Item):
    pass
"""
            (project_dir / "items.py").write_text(items_content)

            # settings.pyã‚’ä½œæˆ
            playwright_settings = ""
            if use_playwright:
                playwright_settings = '''
# Playwrightè¨­å®š
DOWNLOAD_HANDLERS = {
    "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
    "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
}
TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"

# Playwrightè¿½åŠ è¨­å®šï¼ˆãƒãƒ³ã‚°ã‚¢ãƒƒãƒ—é˜²æ­¢ï¼‰
PLAYWRIGHT_BROWSER_TYPE = 'chromium'
PLAYWRIGHT_LAUNCH_OPTIONS = {"headless": True, "args": ["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage", "--disable-gpu"]}
PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT = 30000
PLAYWRIGHT_PROCESS_REQUEST_HEADERS = None
'''

            settings_content = f"""
BOT_NAME = 'temp_project'
SPIDER_MODULES = ['temp_project.spiders']
NEWSPIDER_MODULE = 'temp_project.spiders'
ROBOTSTXT_OBEY = False
DOWNLOAD_DELAY = 1
RANDOMIZE_DOWNLOAD_DELAY = 0.5

# Override the default request headers:
DEFAULT_REQUEST_HEADERS = {{
    'Accept-Language': 'ja',
}}

# Feed export encoding
FEED_EXPORT_ENCODING = 'utf-8'

# HTTP Cache settings (for development efficiency)
HTTPCACHE_ENABLED = True
HTTPCACHE_DIR = 'httpcache'
HTTPCACHE_EXPIRATION_SECS = 86400  # 1 day

# Proxy settings (optional - configure as needed)
# DOWNLOADER_MIDDLEWARES = {{
#     'scrapy_proxies.RandomProxy': 350,
# }}

# Proxy settings (optional - configure as needed)
# PROXY_LIST = '/path/to/proxy/list.txt'
# PROXY_MODE = 0  # 0: random, 1: round-robin, 2: only once

{playwright_settings}

# ãƒ­ã‚°è¨­å®š
LOG_LEVEL = 'INFO'

# ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
DOWNLOAD_TIMEOUT = 30
DOWNLOAD_DELAY = 1

# ã‚«ã‚¹ã‚¿ãƒ è¨­å®š
{json.dumps(settings, indent=4) if settings else ''}
"""
            (project_dir / "settings.py").write_text(settings_content)

            # scrapy.cfgã‚’ä½œæˆ
            scrapy_cfg_content = f"""
[settings]
default = temp_project.settings

[deploy]
project = temp_project
"""
            (temp_path / "scrapy.cfg").write_text(scrapy_cfg_content)

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            # start_urlsã‚’å‹•çš„ã«è¨­å®š
            urls_str = ",\n        ".join([f'"{url}"' for url in start_urls])

            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã®start_urlsã¨nameã‚’ç½®æ›
            modified_script = script_content

            # start_urlsã‚’ç½®æ›
            if "start_urls = [" in script_content:
                # æ—¢å­˜ã®start_urlsã‚’ç½®æ›
                pattern = r'start_urls\s*=\s*\[.*?\]'
                replacement = f'start_urls = [\n        {urls_str}\n    ]'
                modified_script = re.sub(pattern, replacement, script_content, flags=re.DOTALL)

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã‚’å¼·åˆ¶çš„ã«è¨­å®š
            if 'name = "' in modified_script:
                # æ—¢å­˜ã®nameã‚’ç½®æ›
                pattern = r'name\s*=\s*["\'][^"\']*["\']'
                replacement = f'name = "{spider_name}"'
                modified_script = re.sub(pattern, replacement, modified_script)
            elif "name = '" in modified_script:
                # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã®å ´åˆ
                pattern = r"name\s*=\s*'[^']*'"
                replacement = f"name = '{spider_name}'"
                modified_script = re.sub(pattern, replacement, modified_script)
            else:
                # nameãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ã€ã‚¯ãƒ©ã‚¹å®šç¾©ã®å¾Œã«è¿½åŠ 
                if "class " in modified_script and "Spider" in modified_script:
                    pattern = r'(class\s+\w+.*?Spider.*?:)'
                    replacement = f'\\1\n    name = "{spider_name}"'
                    modified_script = re.sub(pattern, replacement, modified_script, flags=re.MULTILINE)

            # JavaScripté¢¨ã®booleanã‚’Pythonã«å¤‰æ›
            modified_script = modified_script.replace('"headless": true', '"headless": True')
            modified_script = modified_script.replace('"headless": false', '"headless": False')
            modified_script = modified_script.replace("'headless': true", "'headless': True")
            modified_script = modified_script.replace("'headless': false", "'headless': False")

            # ãƒ‡ãƒãƒƒã‚°ç”¨ã®printé–¢æ•°ã‚’è¿½åŠ ï¼ˆç°¡å˜ãªæ–¹æ³•ï¼‰
            debug_functions = '''
# ãƒ‡ãƒãƒƒã‚°ç”¨é–¢æ•°
import json
import pprint as pp
from datetime import datetime

def debug_print(*args, **kwargs):
    """ãƒ‡ãƒãƒƒã‚°ç”¨printé–¢æ•°"""
    message = ' '.join(str(arg) for arg in args)
    timestamp = datetime.now().isoformat()
    print(f"ğŸ› [DEBUG] {timestamp}: {message}")

def debug_pprint(obj, **kwargs):
    """ãƒ‡ãƒãƒƒã‚°ç”¨pprinté–¢æ•°"""
    timestamp = datetime.now().isoformat()
    formatted = pp.pformat(obj, **kwargs)
    print(f"ğŸ› [PPRINT] {timestamp}:")
    for line in formatted.split('\\n'):
        print(f"ğŸ›   {line}")

'''
            # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…ˆé ­ã«ãƒ‡ãƒãƒƒã‚°é–¢æ•°ã‚’è¿½åŠ 
            modified_script = debug_functions + modified_script

            spider_file = spiders_dir / f"{spider_name}.py"
            spider_file.write_text(modified_script)



            # ãƒ‡ãƒ¼ã‚¿å‡ºåŠ›ç”¨ã®ã‚«ã‚¹ã‚¿ãƒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆ
            pipeline_content = f"""
import json
import os

class DataCollectionPipeline:
    def __init__(self):
        self.items = []
        self.output_file = os.path.join(r'{temp_dir}', 'scraped_data.json')
        print(f"ğŸ”§ Pipeline initialized, output file: {{self.output_file}}")

    def process_item(self, item, spider):
        print(f"ğŸ”§ Pipeline processing item: {{type(item)}}")

        # ã‚¢ã‚¤ãƒ†ãƒ ã‚’è¾æ›¸ã«å¤‰æ›
        if hasattr(item, 'keys'):
            item_dict = dict(item)
        elif isinstance(item, dict):
            item_dict = item
        else:
            item_dict = {{'data': str(item)}}

        print(f"ğŸ”§ Item converted to dict: {{item_dict}}")
        self.items.append(item_dict)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        try:
            with open(self.output_file, 'w', encoding='utf-8') as f:
                json.dump(self.items, f, ensure_ascii=False, indent=2, default=str)
            print(f"ğŸ”§ Saved {{len(self.items)}} items to file")
        except Exception as e:
            print(f"ğŸ”§ Error saving to file: {{e}}")

        return item

    def close_spider(self, spider):
        print(f"ğŸ”§ Pipeline closing, total items: {{len(self.items)}}")
        spider.logger.info(f"Collected {{len(self.items)}} items")
"""
            (project_dir / "pipelines.py").write_text(pipeline_content)

            # settings.pyã«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ 
            with open(project_dir / "settings.py", "a") as f:
                f.write(f"\nITEM_PIPELINES = {{'temp_project.pipelines.DataCollectionPipeline': 300}}\n")

            # Scrapyã‚’å®Ÿè¡Œ
            cmd = [
                sys.executable, "-m", "scrapy", "crawl", spider_name,
                "-s", "LOG_LEVEL=INFO"
            ]

            print(f"ğŸš€ Executing command: {' '.join(cmd)}")
            print(f"ğŸ“ Working directory: {temp_path}")
            print(f"ğŸŒ PYTHONPATH: {temp_path}")

            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=temp_path,
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env={**os.environ, "PYTHONPATH": str(temp_path)}
            )

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œï¼ˆ300ç§’ = 5åˆ†ï¼‰
            try:
                print(f"â³ Waiting for process to complete (timeout: 300s)")
                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=300.0
                )
                print(f"âœ… Process completed with return code: {process.returncode}")
            except asyncio.TimeoutError:
                print(f"â° Process timed out after 300 seconds")
                # ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†
                process.kill()
                await process.wait()
                raise Exception("Script execution timed out after 300 seconds")
            except Exception as e:
                print(f"âŒ Process execution error: {str(e)}")
                raise

            # å‡ºåŠ›ã‚’å‡¦ç†
            print(f"ğŸ“ Processing stdout ({len(stdout) if stdout else 0} bytes)")
            if stdout:
                output_lines = stdout.decode('utf-8', errors='ignore').split('\n')
                output_lines = [line for line in output_lines if line.strip()]
                print(f"ğŸ“ Stdout lines: {len(output_lines)}")

            print(f"ğŸ“ Processing stderr ({len(stderr) if stderr else 0} bytes)")
            if stderr:
                stderr_lines = stderr.decode('utf-8', errors='ignore').split('\n')
                stderr_lines = [line for line in stderr_lines if line.strip()]
                print(f"ğŸ“ Stderr lines: {len(stderr_lines)}")

                # stderrã®å†…å®¹ã‚’é©åˆ‡ã«åˆ†é¡
                for line in stderr_lines:
                    # å®Ÿéš›ã®ã‚¨ãƒ©ãƒ¼ã®ã¿ã‚’ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦åˆ†é¡
                    if any(level in line for level in ['ERROR', 'CRITICAL']) or 'Traceback' in line:
                        error_lines.append(line)
                    # INFOã€DEBUGã€WARNINGã¯å‡ºåŠ›ã¨ã—ã¦åˆ†é¡
                    elif any(level in line for level in ['INFO', 'DEBUG', 'WARNING']):
                        output_lines.append(line)
                    # ãã®ä»–ã®è¡Œã‚‚å‡ºåŠ›ã¨ã—ã¦åˆ†é¡
                    else:
                        output_lines.append(line)

            # æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            data_file = temp_path / "scraped_data.json"
            if data_file.exists():
                try:
                    with open(data_file, 'r', encoding='utf-8') as f:
                        extracted_data = json.load(f)
                except Exception as e:
                    error_lines.append(f"Error reading scraped data: {str(e)}")

            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã¯æ¨™æº–å‡ºåŠ›ã«å«ã¾ã‚Œã¦ã„ã‚‹ã®ã§ã€ç‰¹åˆ¥ãªå‡¦ç†ã¯ä¸è¦
            # ğŸ› ãƒãƒ¼ã‚¯ãŒä»˜ã„ãŸè¡ŒãŒãƒ‡ãƒãƒƒã‚°å‡ºåŠ›ã¨ã—ã¦è­˜åˆ¥ã•ã‚Œã‚‹

            # å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆç®—
            execution_time = (datetime.now() - start_time).total_seconds()

            return {
                "output": output_lines,
                "errors": error_lines,
                "extracted_data": extracted_data,
                "execution_time": execution_time
            }

    except Exception as e:
        execution_time = (datetime.now() - start_time).total_seconds()
        return {
            "output": output_lines,
            "errors": error_lines + [str(e), traceback.format_exc()],
            "extracted_data": extracted_data,
            "execution_time": execution_time
        }

@router.delete("/execution/{execution_id}")
async def cancel_execution(
    execution_id: str,
    current_user: User = Depends(get_current_active_user)
):
    """
    å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
    """
    if execution_id in running_executions:
        running_executions[execution_id]["status"] = "cancelled"
        return {"message": "Execution cancelled"}
    else:
        raise HTTPException(status_code=404, detail="Execution not found")

@router.get("/execution/{execution_id}/export/{format}")
async def export_execution_data(
    execution_id: str,
    format: str,
    current_user: User = Depends(get_current_active_user)
):
    """
    å®Ÿè¡Œçµæœã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ (CSV, JSON, Excel)
    """
    if execution_id not in running_executions:
        raise HTTPException(status_code=404, detail="Execution not found")

    execution_data = running_executions[execution_id]
    extracted_data = execution_data.get("extracted_data", [])

    if not extracted_data:
        raise HTTPException(status_code=400, detail="No data to export")

    if format.lower() == "json":
        from fastapi.responses import JSONResponse
        return JSONResponse(
            content=extracted_data,
            headers={"Content-Disposition": f"attachment; filename=scraped_data_{execution_id[:8]}.json"}
        )

    elif format.lower() == "csv":
        import csv
        import io
        from fastapi.responses import StreamingResponse

        # CSVãƒ‡ãƒ¼ã‚¿ã‚’ç”Ÿæˆ
        output = io.StringIO()
        if extracted_data:
            # å…¨ã¦ã®ã‚­ãƒ¼ã‚’åé›†
            all_keys = set()
            for item in extracted_data:
                if isinstance(item, dict):
                    all_keys.update(item.keys())

            fieldnames = list(all_keys)
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()

            for item in extracted_data:
                if isinstance(item, dict):
                    # ãƒã‚¹ãƒˆã•ã‚ŒãŸã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
                    flattened_item = {}
                    for key, value in item.items():
                        if isinstance(value, (dict, list)):
                            flattened_item[key] = json.dumps(value, ensure_ascii=False)
                        else:
                            flattened_item[key] = value
                    writer.writerow(flattened_item)

        output.seek(0)
        return StreamingResponse(
            io.BytesIO(output.getvalue().encode('utf-8')),
            media_type="text/csv",
            headers={"Content-Disposition": f"attachment; filename=scraped_data_{execution_id[:8]}.csv"}
        )

    elif format.lower() == "excel":
        import pandas as pd
        import io
        from fastapi.responses import StreamingResponse

        # DataFrameã‚’ä½œæˆ
        df = pd.json_normalize(extracted_data)

        # Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Scraped Data', index=False)

        output.seek(0)
        return StreamingResponse(
            output,
            media_type="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
            headers={"Content-Disposition": f"attachment; filename=scraped_data_{execution_id[:8]}.xlsx"}
        )

    elif format.lower() == "xml":
        import xml.etree.ElementTree as ET
        from xml.dom import minidom
        import io
        from fastapi.responses import StreamingResponse

        # XMLãƒ«ãƒ¼ãƒˆè¦ç´ ã‚’ä½œæˆ
        root = ET.Element("scraped_data")
        root.set("execution_id", execution_id)
        root.set("total_items", str(len(extracted_data)))
        root.set("generated_at", datetime.now().isoformat())

        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¦ç´ ã‚’è¿½åŠ 
        metadata = ET.SubElement(root, "metadata")
        ET.SubElement(metadata, "execution_id").text = execution_id
        ET.SubElement(metadata, "total_items").text = str(len(extracted_data))
        ET.SubElement(metadata, "generated_at").text = datetime.now().isoformat()
        ET.SubElement(metadata, "format_version").text = "1.0"

        # ã‚¢ã‚¤ãƒ†ãƒ ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½œæˆ
        items_container = ET.SubElement(root, "items")

        # å„ã‚¢ã‚¤ãƒ†ãƒ ã‚’XMLè¦ç´ ã¨ã—ã¦è¿½åŠ 
        for index, item in enumerate(extracted_data):
            item_element = ET.SubElement(items_container, "item")
            item_element.set("index", str(index + 1))

            # ã‚¢ã‚¤ãƒ†ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’å†å¸°çš„ã«XMLè¦ç´ ã«å¤‰æ›
            def dict_to_xml(parent, data, name="data"):
                if isinstance(data, dict):
                    for key, value in data.items():
                        # ã‚­ãƒ¼åã‚’XMLè¦ç´ åã¨ã—ã¦ä½¿ç”¨ï¼ˆç„¡åŠ¹ãªæ–‡å­—ã‚’ç½®æ›ï¼‰
                        safe_key = str(key).replace(" ", "_").replace("-", "_")
                        safe_key = "".join(c for c in safe_key if c.isalnum() or c == "_")
                        if not safe_key or safe_key[0].isdigit():
                            safe_key = f"field_{safe_key}"

                        child = ET.SubElement(parent, safe_key)
                        dict_to_xml(child, value, safe_key)
                elif isinstance(data, list):
                    for i, item in enumerate(data):
                        child = ET.SubElement(parent, f"{name}_item")
                        child.set("index", str(i))
                        dict_to_xml(child, item, f"{name}_item")
                else:
                    parent.text = str(data) if data is not None else ""

            dict_to_xml(item_element, item)

        # XMLã‚’æ–‡å­—åˆ—ã«å¤‰æ›ï¼ˆæ•´å½¢ä»˜ãï¼‰
        rough_string = ET.tostring(root, encoding='unicode')
        reparsed = minidom.parseString(rough_string)
        pretty_xml = reparsed.toprettyxml(indent="  ")

        # UTF-8 BOMã‚’é™¤å»ã—ã€é©åˆ‡ãªXMLãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¿½åŠ 
        if pretty_xml.startswith('<?xml version="1.0" ?>'):
            pretty_xml = '<?xml version="1.0" encoding="UTF-8"?>' + pretty_xml[22:]

        # XMLã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        xml_comment = f"""
<!--
  ScrapyUI Exported Data
  Generated: {datetime.now().isoformat()}
  Execution ID: {execution_id}
  Total Items: {len(extracted_data)}
  Format: XML v1.0
-->
"""
        # XMLãƒ˜ãƒƒãƒ€ãƒ¼ã®å¾Œã«ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŒ¿å…¥
        if pretty_xml.startswith('<?xml'):
            header_end = pretty_xml.find('?>') + 2
            pretty_xml = pretty_xml[:header_end] + xml_comment + pretty_xml[header_end:]

        return StreamingResponse(
            io.BytesIO(pretty_xml.encode('utf-8')),
            media_type="application/xml",
            headers={"Content-Disposition": f"attachment; filename=scraped_data_{execution_id[:8]}.xml"}
        )

    else:
        raise HTTPException(status_code=400, detail="Unsupported format. Use 'json', 'csv', 'excel', or 'xml'")

@router.post("/test", response_model=ScriptExecutionResponse)
async def test_script(
    request: ScriptExecutionRequest,
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_active_user)
):
    """
    ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆçŸ­æ™‚é–“å®Ÿè¡Œï¼‰
    """
    execution_id = str(uuid.uuid4())
    started_at = datetime.now().isoformat()

    # ã‚¿ã‚¹ã‚¯ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«è¨˜éŒ²
    project_id = request.project_id or "webui-test"
    # spider_idãŒæä¾›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯ã€spider_nameã‚’ãã®ã¾ã¾ä½¿ç”¨
    spider_id = request.spider_id or request.spider_name

    task = Task(
        id=execution_id,
        project_id=project_id,
        spider_id=spider_id,
        status="RUNNING",
        user_id=current_user.id,
        log_level="INFO",
        settings=request.settings or {}
    )
    db.add(task)
    db.commit()

    # å®Ÿè¡ŒçŠ¶æ…‹ã‚’åˆæœŸåŒ–
    running_executions[execution_id] = {
        "status": "running",
        "output": [],
        "errors": [],
        "extracted_data": [],
        "started_at": started_at,
        "finished_at": None
    }

    try:
        print(f"ğŸ§ª Starting test script execution: {request.spider_name}")
        print(f"ğŸ“ Start URLs: {request.start_urls}")
        print(f"âš™ï¸ Settings: {request.settings}")

        # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ†ã‚¹ãƒˆç”¨ã«çŸ­æ™‚é–“ï¼‰
        use_playwright = "scrapy_playwright" in request.script_content or "playwright" in str(request.settings)
        print(f"ğŸ­ Playwright detection: {use_playwright}")

        result = await execute_scrapy_script(
            request.script_content,
            request.spider_name,
            request.start_urls,
            request.settings or {},
            use_playwright=use_playwright
        )

        # å®Ÿè¡ŒçŠ¶æ…‹ã‚’æ›´æ–°
        finished_at = datetime.now().isoformat()
        running_executions[execution_id].update({
            "status": "completed",
            "output": result["output"],
            "errors": result["errors"],
            "extracted_data": result["extracted_data"],
            "finished_at": finished_at
        })

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã‚’æ›´æ–°
        task.status = "FINISHED"
        task.finished_at = datetime.now()
        task.items_count = len(result["extracted_data"])
        db.commit()

        return ScriptExecutionResponse(
            execution_id=execution_id,
            status="completed",
            output=result["output"],
            errors=result["errors"],
            extracted_data=result["extracted_data"],
            execution_time=result["execution_time"],
            started_at=started_at,
            finished_at=finished_at
        )

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†
        error_message = str(e)
        finished_at = datetime.now().isoformat()

        running_executions[execution_id].update({
            "status": "failed",
            "errors": [error_message],
            "finished_at": finished_at
        })

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯ã‚’æ›´æ–°
        task.status = "FAILED"
        task.finished_at = datetime.now()
        task.error_message = error_message
        db.commit()

        raise HTTPException(
            status_code=500,
            detail=f"Script test execution failed: {error_message}"
        )

@router.get("/history")
async def get_user_execution_history(
    limit: int = 20,
    current_user: User = Depends(get_current_active_user)
):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å®Ÿè¡Œå±¥æ­´ã‚’å–å¾—
    """
    history = get_execution_history(current_user.id, limit)
    return {"history": history}

@router.post("/save")
async def save_script(
    request: ScriptSaveRequest,
    current_user: User = Depends(get_current_active_user)
):
    """
    ã‚¨ãƒ‡ã‚£ã‚¿ãƒ¼ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä¿å­˜
    """
    try:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å°‚ç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        user_scripts_dir = Path(f"user_scripts/{current_user.id}")
        user_scripts_dir.mkdir(parents=True, exist_ok=True)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¤œè¨¼
        if not request.file_name.endswith('.py'):
            request.file_name += '.py'

        # å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã«å¤‰æ›
        safe_filename = "".join(c for c in request.file_name if c.isalnum() or c in '._-')
        if not safe_filename:
            safe_filename = "script.py"

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        file_path = user_scripts_dir / safe_filename

        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(request.content)

        return {
            "message": "Script saved successfully",
            "file_name": safe_filename,
            "file_path": str(file_path)
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to save script: {str(e)}"
        )

@router.get("/files")
async def get_user_scripts(
    current_user: User = Depends(get_current_active_user)
):
    """
    ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä¿å­˜æ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¸€è¦§ã‚’å–å¾—
    """
    try:
        user_scripts_dir = Path(f"user_scripts/{current_user.id}")

        if not user_scripts_dir.exists():
            return {"files": []}

        files = []
        for file_path in user_scripts_dir.glob("*.py"):
            try:
                stat = file_path.stat()
                files.append({
                    "name": file_path.name,
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "path": str(file_path.relative_to(user_scripts_dir))
                })
            except Exception:
                continue

        # æ›´æ–°æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
        files.sort(key=lambda x: x["modified"], reverse=True)

        return {"files": files}

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to get script files: {str(e)}"
        )

@router.get("/files/{file_name}")
async def get_script_content(
    file_name: str,
    current_user: User = Depends(get_current_active_user)
):
    """
    ä¿å­˜æ¸ˆã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å†…å®¹ã‚’å–å¾—
    """
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«åã®æ¤œè¨¼
        if '..' in file_name or '/' in file_name:
            raise HTTPException(status_code=400, detail="Invalid file name")

        user_scripts_dir = Path(f"user_scripts/{current_user.id}")
        file_path = user_scripts_dir / file_name

        if not file_path.exists():
            raise HTTPException(status_code=404, detail="File not found")

        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        return {
            "file_name": file_name,
            "content": content
        }

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to read script file: {str(e)}"
        )
