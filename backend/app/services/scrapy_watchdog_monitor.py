#!/usr/bin/env python3
"""
scrapy crawlã‚³ãƒãƒ³ãƒ‰ + watchdogç›£è¦–ã®å®Ÿè£…
backend/app/services/scrapy_watchdog_monitor.py
"""
import asyncio
import subprocess
import json
import sqlite3
import threading
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
import uuid
import os
import sys

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install watchdog")


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, monitor):
        self.monitor = monitor
        self.last_size = 0

    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return

        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self._handle_file_change,
                daemon=True
            ).start()

    def _handle_file_change(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®å‡¦ç†ï¼ˆã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œï¼‰"""
        try:
            # asyncioãƒ«ãƒ¼ãƒ—ã§å®Ÿè¡Œ
            if self.monitor.loop and not self.monitor.loop.is_closed():
                asyncio.run_coroutine_threadsafe(
                    self.monitor._process_new_lines(),
                    self.monitor.loop
                )
        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")


class ScrapyWatchdogMonitor:
    """scrapy crawl + watchdogç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self,
                 task_id: str,
                 project_path: str,
                 spider_name: str,
                 db_path: str = "backend/database/scrapy_ui.db",
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.project_path = Path(project_path)
        self.spider_name = spider_name
        self.db_path = db_path
        self.websocket_callback = websocket_callback

        # ç›£è¦–çŠ¶æ…‹
        self.is_monitoring = False
        self.observer = None
        self.loop = None
        self.processed_lines = 0
        self.last_file_size = 0

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.jsonl_file_path = self.project_path / f"results_{task_id}.jsonl"

        # Scrapyãƒ—ãƒ­ã‚»ã‚¹
        self.scrapy_process = None

    async def execute_spider_with_monitoring(self,
                                           settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """watchdogç›£è¦–ä»˜ãã§scrapy crawlã‚’å®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {self.spider_name}")

            # ç¾åœ¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä¿å­˜
            self.loop = asyncio.get_event_loop()

            # 1. watchdogç›£è¦–ã‚’é–‹å§‹
            await self._start_watchdog_monitoring()

            # 2. scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            scrapy_task = asyncio.create_task(
                self._execute_scrapy_crawl(settings)
            )

            # 3. ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã¾ã§å¾…æ©Ÿ
            scrapy_result = await scrapy_task

            # 4. å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç›£è¦–åœæ­¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç¢ºå®Ÿã«å‡¦ç†ï¼‰
            await asyncio.sleep(2)
            self._stop_watchdog_monitoring()

            # 5. æœ€çµ‚çš„ãªçµæœå‡¦ç†
            await self._process_remaining_lines()

            return {
                'success': scrapy_result['success'],
                'task_id': self.task_id,
                'items_processed': self.processed_lines,
                'scrapy_result': scrapy_result,
                'jsonl_file': str(self.jsonl_file_path)
            }

        except Exception as e:
            self._stop_watchdog_monitoring()
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }

    async def _start_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        if not WATCHDOG_AVAILABLE:
            raise Exception("watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

        self.is_monitoring = True

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
        watch_directory = self.jsonl_file_path.parent

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        event_handler = JSONLWatchdogHandler(self)

        # Observerã‚’ä½œæˆã—ã¦ç›£è¦–é–‹å§‹
        self.observer = Observer()
        self.observer.schedule(event_handler, str(watch_directory), recursive=False)
        self.observer.start()

        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {watch_directory}")
        print(f"ğŸ“„ ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {self.jsonl_file_path}")

    def _stop_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False

        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None

        print(f"ğŸ›‘ watchdogç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")

    async def _execute_scrapy_crawl(self, settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ"""
        try:
            # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
            cmd = [
                sys.executable, "-m", "scrapy", "crawl", self.spider_name,
                "-o", str(self.jsonl_file_path),  # JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                "-s", "FEED_FORMAT=jsonlines",    # JSONLå½¢å¼æŒ‡å®š
                "-s", "LOG_LEVEL=INFO"
            ]

            # è¿½åŠ è¨­å®šãŒã‚ã‚Œã°é©ç”¨
            if settings:
                for key, value in settings.items():
                    cmd.extend(["-s", f"{key}={value}"])

            print(f"ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            print(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.project_path}")

            # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
            env = os.environ.copy()
            env['PYTHONPATH'] = str(self.project_path)

            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            self.scrapy_process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹: PID {self.scrapy_process.pid}")

            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = await self.scrapy_process.communicate()

            # çµæœã‚’è§£æ
            success = self.scrapy_process.returncode == 0

            result = {
                'success': success,
                'return_code': self.scrapy_process.returncode,
                'stdout': stdout.decode('utf-8', errors='ignore'),
                'stderr': stderr.decode('utf-8', errors='ignore')
            }

            if success:
                print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†")
            else:
                print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼ (code: {self.scrapy_process.returncode})")
                print(f"   stderr: {result['stderr'][:200]}...")

            return result

        except Exception as e:
            print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

    async def _process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆwatchdogã‚¤ãƒ™ãƒ³ãƒˆã‹ã‚‰å‘¼ã°ã‚Œã‚‹ï¼‰"""
        try:
            if not self.jsonl_file_path.exists():
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            if current_size <= self.last_file_size:
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                for line in new_lines:
                    await self._process_single_line(line)
                    self.processed_lines += 1

                # WebSocketé€šçŸ¥
                if self.websocket_callback:
                    await self.websocket_callback({
                        'type': 'items_update',
                        'task_id': self.task_id,
                        'new_items': len(new_lines),
                        'total_items': self.processed_lines
                    })

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            self.last_file_size = current_size

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    async def _process_remaining_lines(self):
        """æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†ï¼ˆæœ€çµ‚å‡¦ç†ï¼‰"""
        try:
            if not self.jsonl_file_path.exists():
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿ç›´ã—ã¦æœªå‡¦ç†ã®è¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len([line for line in all_lines if line.strip()])
            remaining_lines = total_lines - self.processed_lines

            if remaining_lines > 0:
                print(f"ğŸ“ æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†: {remaining_lines}ä»¶")

                # æœªå‡¦ç†ã®è¡Œã‚’å‡¦ç†
                for i in range(self.processed_lines, total_lines):
                    if i < len(all_lines):
                        line = all_lines[i].strip()
                        if line:
                            await self._process_single_line(line)
                            self.processed_lines += 1

            print(f"âœ… æœ€çµ‚å‡¦ç†å®Œäº†: ç·å‡¦ç†è¡Œæ•° {self.processed_lines}")

        except Exception as e:
            print(f"âŒ æ®‹ã‚Šè¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    async def _process_single_line(self, json_line: str):
        """å˜ä¸€ã®è¡Œã‚’å‡¦ç†ã—ã¦DBã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            # JSONè§£æ
            item_data = json.loads(json_line)

            # éåŒæœŸã§DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._sync_insert_item, item_data)

        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {json_line[:100]}...")
        except Exception as e:
            print(f"âŒ è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _sync_insert_item(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # scraped_itemsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            item_id = str(uuid.uuid4())
            cursor.execute("""
                INSERT INTO scraped_items
                (id, task_id, project_id, spider_name, data, scraped_at, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                item_id,
                self.task_id,
                str(self.project_path.name),  # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’project_idã¨ã—ã¦ä½¿ç”¨
                self.spider_name,
                json.dumps(item_data, ensure_ascii=False),
                datetime.now().isoformat(),
                datetime.now().isoformat()
            ))

            conn.commit()
            conn.close()

        except Exception as e:
            print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def test_scrapy_watchdog_monitor():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ¯ scrapy crawl + watchdogç›£è¦–ãƒ†ã‚¹ãƒˆ")

    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ WebSocketé€šçŸ¥: {data}")

    # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    monitor = ScrapyWatchdogMonitor(
        task_id="test_task_123",
        project_path="scrapy_projects/test_project",  # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«å¤‰æ›´
        spider_name="test_spider",  # å®Ÿéš›ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã«å¤‰æ›´
        websocket_callback=websocket_callback
    )

    # å®Ÿè¡Œ
    result = await monitor.execute_spider_with_monitoring(
        settings={
            'LOG_LEVEL': 'INFO',
            'ROBOTSTXT_OBEY': False
        }
    )

    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")


if __name__ == "__main__":
    if WATCHDOG_AVAILABLE:
        asyncio.run(test_scrapy_watchdog_monitor())
    else:
        print("âŒ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install watchdog")
