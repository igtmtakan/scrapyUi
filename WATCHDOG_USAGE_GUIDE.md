# ğŸ” watchdogç›£è¦–æ©Ÿèƒ½ ä½¿ç”¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ æ¦‚è¦

ScrapyUIã§ã¯ã€**2ã¤ã®æ–¹æ³•**ã§watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡ŒãŒåˆ©ç”¨ã§ãã¾ã™ï¼š

1. **ScrapyUI API** - WebUIã¾ãŸã¯APIçµŒç”±ã§ã®å®Ÿè¡Œ
2. **scrapy crawlwithwatchdog** - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ

ã©ã¡ã‚‰ã‚‚**JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§ç›£è¦–ã—ã€æ–°ã—ã„ã‚¢ã‚¤ãƒ†ãƒ ã‚’å³åº§ã«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ**ã—ã¾ã™ã€‚

## ğŸš€ æ–¹æ³•1: ScrapyUI API

### WebUIçµŒç”±ã§ã®å®Ÿè¡Œ

1. ãƒ–ãƒ©ã‚¦ã‚¶ã§ `http://localhost:4000` ã«ã‚¢ã‚¯ã‚»ã‚¹
2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ â†’ **å®Ÿè¡Œãƒœã‚¿ãƒ³**ã‚’ã‚¯ãƒªãƒƒã‚¯
3. å†…éƒ¨çš„ã«watchdogç›£è¦–ä»˜ãAPIãŒå‘¼ã°ã‚Œã¾ã™

### APIç›´æ¥å‘¼ã³å‡ºã—

```bash
# ãƒ­ã‚°ã‚¤ãƒ³
curl -X POST "http://localhost:8000/api/auth/login" \
  -H "Content-Type: application/json" \
  -d '{"email": "admin@scrapyui.com", "password": "admin123456"}'

# watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
curl -X POST "http://localhost:8000/api/spiders/{spider_id}/run-with-watchdog?project_id={project_id}" \
  -H "Authorization: Bearer {token}" \
  -H "Content-Type: application/json" \
  -d '{
    "settings": {
      "LOG_LEVEL": "INFO",
      "ROBOTSTXT_OBEY": false,
      "DOWNLOAD_DELAY": 1
    }
  }'
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "task_id": "12345678-1234-1234-1234-123456789abc",
  "celery_task_id": "12345678-1234-1234-1234-123456789abc",
  "status": "started_with_watchdog",
  "monitoring": "jsonl_file_watchdog",
  "spider_name": "example_spider",
  "project_name": "example_project",
  "message": "Spider example_spider started with watchdog monitoring"
}
```

## ğŸ”§ æ–¹æ³•2: scrapy crawlwithwatchdog ã‚³ãƒãƒ³ãƒ‰

### å‰ææ¡ä»¶

```bash
# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install watchdog
```

### åŸºæœ¬çš„ãªä½¿ç”¨æ–¹æ³•

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•
cd scrapy_projects/your_project

# watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
scrapy crawlwithwatchdog spider_name -o results.jsonl --task-id=test_123
```

### ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```bash
scrapy crawlwithwatchdog spider_name [ã‚ªãƒ—ã‚·ãƒ§ãƒ³]

ã‚ªãƒ—ã‚·ãƒ§ãƒ³:
  -o, --output FILE         JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®å‡ºåŠ›å…ˆï¼ˆå¿…é ˆï¼‰
  --task-id TASK_ID        ã‚¿ã‚¹ã‚¯IDï¼ˆçœç•¥æ™‚ã¯è‡ªå‹•ç”Ÿæˆï¼‰
  --db-path DB_PATH        ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: backend/database/scrapy_ui.dbï¼‰
  -h, --help               ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º
```

### ä½¿ç”¨ä¾‹

```bash
# åŸºæœ¬çš„ãªå®Ÿè¡Œ
scrapy crawlwithwatchdog my_spider -o results.jsonl

# ã‚¿ã‚¹ã‚¯IDã‚’æŒ‡å®š
scrapy crawlwithwatchdog my_spider -o results.jsonl --task-id=custom_task_123

# ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’æŒ‡å®š
scrapy crawlwithwatchdog my_spider -o results.jsonl --db-path=/path/to/custom.db

# Scrapyã®æ¨™æº–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‚ä½¿ç”¨å¯èƒ½
scrapy crawlwithwatchdog my_spider -o results.jsonl -s LOG_LEVEL=DEBUG -s DOWNLOAD_DELAY=2
```

## ğŸ“Š å‹•ä½œã®ä»•çµ„ã¿

### 1. å®Ÿè¡Œé–‹å§‹

```bash
ğŸš€ Starting spider with watchdog monitoring
   Spider: example_spider
   Task ID: cmd_example_spider_1703123456
   Output: results.jsonl
   DB Path: backend/database/scrapy_ui.db
   Watchdog Available: Yes
```

### 2. ç›£è¦–é–‹å§‹

```bash
ğŸ” watchdogç›£è¦–é–‹å§‹: /path/to/results.jsonl
ğŸ•·ï¸ Starting Scrapy crawler...
```

### 3. ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†

```bash
ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: 5ä»¶
âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: item_id_1
âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: item_id_2
âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: item_id_3
âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: item_id_4
âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: item_id_5
ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: 5
```

### 4. å®Œäº†

```bash
ğŸ›‘ ç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° 25

ğŸ“Š Final Statistics:
   Total items processed: 25
   Output file: results.jsonl
   Database: backend/database/scrapy_ui.db
âœ… crawlwithwatchdog completed
```

## ğŸ” åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§åˆ©ç”¨å¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰ã‚’ç¢ºèª
curl -X GET "http://localhost:8000/api/spiders/commands/available?project_id={project_id}" \
  -H "Authorization: Bearer {token}"
```

### ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹

```json
{
  "standard_commands": [
    {
      "name": "crawl",
      "description": "Run a spider",
      "usage": "scrapy crawl <spider_name>",
      "watchdog_support": false
    }
  ],
  "custom_commands": [
    {
      "name": "crawlwithwatchdog",
      "description": "Run a spider with watchdog monitoring for real-time DB insertion",
      "usage": "scrapy crawlwithwatchdog <spider_name> -o results.jsonl --task-id=<task_id>",
      "watchdog_support": true,
      "file_path": "/path/to/project/commands/crawlwithwatchdog.py",
      "requirements": []
    }
  ],
  "watchdog_available": true
}
```

## ğŸ› ï¸ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„

```bash
âŒ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚
ğŸ”„ ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–é–‹å§‹: /path/to/results.jsonl
```

**è§£æ±ºæ–¹æ³•:**
```bash
pip install watchdog
```

### å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„

```bash
âŒ Usage error: Output file (-o) is required for watchdog monitoring
```

**è§£æ±ºæ–¹æ³•:**
```bash
scrapy crawlwithwatchdog spider_name -o results.jsonl
```

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šã‚¨ãƒ©ãƒ¼

```bash
âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: no such table: scraped_items
```

**è§£æ±ºæ–¹æ³•:**
1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’ç¢ºèª
2. ScrapyUIã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãŒæ­£ã—ãåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„

```bash
âŒ crawlwithwatchdogã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“
```

**è§£æ±ºæ–¹æ³•:**
1. æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆï¼ˆè‡ªå‹•çš„ã«ã‚³ãƒãƒ³ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¾ã™ï¼‰
2. æ—¢å­˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆã¯ã€æ‰‹å‹•ã§ã‚³ãƒãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 

## ğŸ“ˆ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒ

| æ–¹æ³• | ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ€§ | è¨­å®šã®ç°¡å˜ã• | ç›£è¦–åŠ¹ç‡ | æ¨å¥¨ç”¨é€” |
|------|---------------|-------------|----------|----------|
| **ScrapyUI API** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | WebUIã€è‡ªå‹•åŒ– |
| **crawlwithwatchdog** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ | ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã€é–‹ç™º |

## ğŸ¯ æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•

### é–‹ç™ºãƒ»ãƒ†ã‚¹ãƒˆæ™‚
```bash
# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ç›´æ¥å®Ÿè¡Œ
scrapy crawlwithwatchdog my_spider -o test_results.jsonl --task-id=dev_test
```

### æœ¬ç•ªãƒ»è‡ªå‹•åŒ–æ™‚
```bash
# ScrapyUI APIçµŒç”±ã§å®Ÿè¡Œ
curl -X POST "http://localhost:8000/api/spiders/{spider_id}/run-with-watchdog?project_id={project_id}"
```

### WebUIä½¿ç”¨æ™‚
- ãƒ–ãƒ©ã‚¦ã‚¶ã§ScrapyUI WebUIã«ã‚¢ã‚¯ã‚»ã‚¹
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ â†’ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ â†’ å®Ÿè¡Œãƒœã‚¿ãƒ³

## ğŸ”§ ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«æ§‹é€ 

```sql
CREATE TABLE scraped_items (
    id TEXT PRIMARY KEY,
    task_id TEXT NOT NULL,
    project_id TEXT,
    spider_name TEXT,
    data TEXT,  -- JSONå½¢å¼ã®ã‚¢ã‚¤ãƒ†ãƒ ãƒ‡ãƒ¼ã‚¿
    scraped_at TEXT,
    created_at TEXT
);
```

### ç›£è¦–é–“éš”ã®èª¿æ•´

ã‚³ãƒãƒ³ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«å†…ã® `time.sleep(1)` ã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ã®é–“éš”ã‚’èª¿æ•´ã§ãã¾ã™ã€‚

## ğŸ“š é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [ScrapyUI API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](http://localhost:8000/docs)
- [Scrapyå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.scrapy.org/)
- [watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://python-watchdog.readthedocs.io/)
