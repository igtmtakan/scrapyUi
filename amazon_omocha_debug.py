import scrapy
from scrapy_playwright.page import PageMethod
import json
import re
from urllib.parse import urljoin


class OmochaSpider(scrapy.Spider):
    name = "omocha"
    allowed_domains = ["amazon.co.jp"]
    start_urls = ["https://www.amazon.co.jp/gp/bestsellers/toys/ref=zg_bs_nav_toys_0"]
    
    # ãƒ‡ãƒãƒƒã‚°ç”¨è¨­å®š
    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': 1.0,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        
        # Playwrightå¿…é ˆè¨­å®š
        'DOWNLOAD_HANDLERS': {
            "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
            "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
        },
        
        'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
        'PLAYWRIGHT_LAUNCH_OPTIONS': {
            'headless': True,
            'args': ['--no-sandbox', '--disable-dev-shm-usage']
        },
        'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 60000,
        'PLAYWRIGHT_DEFAULT_TIMEOUT': 30000,
        
        # ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢è¨­å®šã‚’å®Œå…¨ã«é™¤å¤–
        'DOWNLOADER_MIDDLEWARES': {},
        
        'LOG_LEVEL': 'DEBUG',  # ãƒ‡ãƒãƒƒã‚°ãƒ¬ãƒ™ãƒ«ã«å¤‰æ›´
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    def start_requests(self):
        """ãƒ‡ãƒãƒƒã‚°ç”¨é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
        self.logger.info('ğŸš€ Starting debug spider execution...')
        self.logger.info(f'ğŸ“‹ Spider name: {self.name}')
        self.logger.info(f'ğŸŒ Start URLs: {self.start_urls}')
        
        for url in self.start_urls:
            self.logger.info(f'ğŸ”— Creating request for: {url}')
            yield scrapy.Request(
                url=url,
                callback=self.parse_ranking_page,
                meta={
                    'playwright': True,
                    'playwright_include_page': True,
                    'playwright_page_methods': [
                        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿å®Œäº†ã¾ã§å¾…æ©Ÿ
                        PageMethod('wait_for_load_state', 'domcontentloaded'),
                        PageMethod('wait_for_timeout', 5000),
                        
                        # ãƒšãƒ¼ã‚¸æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                        PageMethod('evaluate', '''
                            async () => {
                                console.log('ğŸ”„ Starting scroll to bottom...');
                                
                                const autoScroll = async () => {
                                    await new Promise((resolve) => {
                                        let totalHeight = 0;
                                        const distance = 300;
                                        const timer = setInterval(() => {
                                            const scrollHeight = document.body.scrollHeight;
                                            window.scrollBy(0, distance);
                                            totalHeight += distance;
                                            
                                            console.log(`ğŸ“œ Scrolled: ${totalHeight}px / ${scrollHeight}px`);
                                            
                                            if(totalHeight >= scrollHeight - 1000) {
                                                clearInterval(timer);
                                                console.log('âœ… Scroll completed');
                                                resolve();
                                            }
                                        }, 300);
                                    });
                                };
                                
                                await autoScroll();
                                await new Promise(resolve => setTimeout(resolve, 3000));
                                
                                console.log('ğŸ¯ Ready to extract product links');
                                return 'Scroll completed successfully';
                            }
                        '''),
                        
                        PageMethod('wait_for_timeout', 3000)
                    ]
                }
            )

    async def parse_ranking_page(self, response):
        """ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ©ãƒ³ã‚­ãƒ³ã‚°ãƒšãƒ¼ã‚¸è§£æ"""
        page = response.meta['playwright_page']
        
        try:
            self.logger.info(f'ğŸŒ Processing ranking page: {response.url}')
            self.logger.info(f'ğŸ“„ Response status: {response.status}')
            self.logger.info(f'ğŸ“ Response length: {len(response.text)} characters')
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            page_title = response.css('title::text').get()
            self.logger.info(f'ğŸ“‹ Page title: {page_title}')
            
            # å•†å“ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆæŒ‡å®šã•ã‚ŒãŸã‚¯ãƒ©ã‚¹ï¼‰
            product_links = response.css('a.a-link-normal.aok-block::attr(href)').getall()
            self.logger.info(f'ğŸ”— Found {len(product_links)} product links with selector: a.a-link-normal.aok-block')
            
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚‚è©¦ã™
            alt_links = response.css('a[href*="/dp/"]::attr(href)').getall()
            self.logger.info(f'ğŸ”— Found {len(alt_links)} product links with alternative selector: a[href*="/dp/"]')
            
            # å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
            all_links = response.css('a::attr(href)').getall()
            amazon_links = [link for link in all_links if link and '/dp/' in link]
            self.logger.info(f'ğŸ”— Found {len(amazon_links)} total Amazon product links')
            
            # ä½¿ç”¨ã™ã‚‹ãƒªãƒ³ã‚¯ã‚’æ±ºå®š
            links_to_use = product_links if product_links else (alt_links if alt_links else amazon_links[:5])
            self.logger.info(f'ğŸ“¦ Using {len(links_to_use)} links for processing')
            
            # å„å•†å“ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ã«2å•†å“ï¼‰
            for i, link in enumerate(links_to_use[:2]):
                if link:
                    full_url = urljoin(response.url, link)
                    self.logger.info(f'ğŸ“¦ Processing product {i+1}/2: {full_url}')
                    
                    yield scrapy.Request(
                        url=full_url,
                        callback=self.parse_product,
                        meta={
                            'playwright': True,
                            'playwright_include_page': True,
                            'playwright_page_methods': [
                                PageMethod('wait_for_load_state', 'domcontentloaded'),
                                PageMethod('wait_for_timeout', 3000),
                                PageMethod('wait_for_selector', '#title, #productTitle', timeout=20000),
                                PageMethod('wait_for_timeout', 2000)
                            ]
                        }
                    )
            
            # ãƒ‡ãƒãƒƒã‚°ç”¨ï¼šãƒšãƒ¼ã‚¸ã®HTMLã®ä¸€éƒ¨ã‚’ä¿å­˜
            html_snippet = response.text[:1000]
            self.logger.debug(f'ğŸ“„ HTML snippet: {html_snippet}')
            
        except Exception as e:
            self.logger.error(f'âŒ Error in parse_ranking_page: {str(e)}')
            import traceback
            self.logger.error(f'ğŸ“‹ Traceback: {traceback.format_exc()}')
        finally:
            await page.close()

    async def parse_product(self, response):
        """ãƒ‡ãƒãƒƒã‚°ç”¨å•†å“è©³ç´°ãƒšãƒ¼ã‚¸è§£æ"""
        page = response.meta['playwright_page']
        
        try:
            self.logger.info(f'ğŸ” Parsing product page: {response.url}')
            self.logger.info(f'ğŸ“„ Response status: {response.status}')
            
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’å–å¾—
            page_title = response.css('title::text').get()
            self.logger.info(f'ğŸ“‹ Page title: {page_title}')
            
            # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—ï¼ˆid="title"ï¼‰
            title = None
            title_selectors = [
                '#title span::text',
                '#productTitle::text',
                'h1#title span::text',
                'h1 span::text'
            ]
            
            for selector in title_selectors:
                title_elements = response.css(selector).getall()
                self.logger.debug(f'ğŸ” Selector {selector}: found {len(title_elements)} elements')
                if title_elements:
                    title = ' '.join([t.strip() for t in title_elements if t.strip()])
                    self.logger.info(f'âœ… Title found with selector {selector}: {title[:50]}...')
                    break
            
            if not title:
                title = 'ã‚¿ã‚¤ãƒˆãƒ«ä¸æ˜'
                self.logger.warning('âš ï¸ No title found with any selector')
            
            # è©•ä¾¡å–å¾—
            rating = None
            rating_elements = response.css('span.a-icon-alt::text').getall()
            self.logger.debug(f'ğŸ” Rating elements found: {len(rating_elements)}')
            for rating_text in rating_elements:
                if rating_text and '5ã¤æ˜Ÿã®ã†ã¡' in rating_text:
                    rating_match = re.search(r'([0-9.]+)', rating_text)
                    if rating_match:
                        try:
                            rating = float(rating_match.group(1))
                            self.logger.info(f'âœ… Rating found: {rating}')
                            break
                        except ValueError:
                            continue
            
            if not rating:
                self.logger.warning('âš ï¸ No rating found')
            
            # ç¨è¾¼ä¾¡æ ¼å–å¾—
            price = None
            price_selectors = [
                '.a-price-current .a-offscreen::text',
                '.a-price .a-offscreen::text',
                '#priceblock_dealprice::text',
                '#priceblock_ourprice::text'
            ]
            
            for selector in price_selectors:
                price_elements = response.css(selector).getall()
                self.logger.debug(f'ğŸ” Price selector {selector}: found {len(price_elements)} elements')
                for price_text in price_elements:
                    if price_text:
                        price_clean = price_text.replace('ï¿¥', '').replace(',', '').strip()
                        price_match = re.search(r'([0-9,]+)', price_clean)
                        if price_match:
                            try:
                                price = int(price_match.group(1).replace(',', ''))
                                self.logger.info(f'âœ… Price found: Â¥{price}')
                                break
                            except ValueError:
                                continue
                if price:
                    break
            
            if not price:
                self.logger.warning('âš ï¸ No price found')
            
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°å–å¾—
            review_count = None
            review_selectors = [
                '#acrCustomerReviewText::text',
                'span[data-hook="total-review-count"]::text'
            ]
            
            for selector in review_selectors:
                review_elements = response.css(selector).getall()
                self.logger.debug(f'ğŸ” Review selector {selector}: found {len(review_elements)} elements')
                for review_text in review_elements:
                    if review_text and ('å€‹ã®è©•ä¾¡' in review_text or 'ä»¶ã®ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼' in review_text):
                        review_match = re.search(r'([0-9,]+)', review_text)
                        if review_match:
                            try:
                                review_count = int(review_match.group(1).replace(',', ''))
                                self.logger.info(f'âœ… Review count found: {review_count}')
                                break
                            except ValueError:
                                continue
                if review_count:
                    break
            
            if not review_count:
                self.logger.warning('âš ï¸ No review count found')
            
            # ç”»åƒURLå–å¾—ï¼ˆclass="a-dynamic-image a-stretch-vertical"ï¼‰
            image_url = None
            image_selectors = [
                'img.a-dynamic-image.a-stretch-vertical::attr(src)',
                '#landingImage::attr(src)',
                '.a-dynamic-image::attr(src)'
            ]
            
            for selector in image_selectors:
                image_elements = response.css(selector).getall()
                self.logger.debug(f'ğŸ” Image selector {selector}: found {len(image_elements)} elements')
                for url in image_elements:
                    if url and url.startswith('http'):
                        image_url = url
                        self.logger.info(f'âœ… Image URL found: {image_url[:50]}...')
                        break
                if image_url:
                    break
            
            if not image_url:
                self.logger.warning('âš ï¸ No image URL found')
            
            # ãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ ã‚’ä½œæˆ
            item = {
                'title': title,
                'rating': rating,
                'price': price,
                'review_count': review_count,
                'image_url': image_url,
                'product_url': response.url,
                'scraped_at': scrapy.utils.misc.load_object('datetime.datetime').now().isoformat()
            }
            
            self.logger.info(f'âœ… Scraped item: {title} | Price: Â¥{price} | Rating: {rating}â­ | Reviews: {review_count}')
            yield item
            
        except Exception as e:
            self.logger.error(f'âŒ Error parsing product {response.url}: {str(e)}')
            import traceback
            self.logger.error(f'ğŸ“‹ Traceback: {traceback.format_exc()}')
        finally:
            await page.close()
