#!/usr/bin/env python3
"""
scrapy crawlã‚³ãƒãƒ³ãƒ‰ + watchdogç›£è¦–ã®å®Ÿè£…
backend/app/services/scrapy_watchdog_monitor.py
"""
import asyncio
import subprocess
import json
import sqlite3
import threading
import time
import hashlib
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Callable, List
import uuid
import os
import sys

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False
    print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå¿…è¦ã§ã™: pip install watchdog")


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""

    def __init__(self, monitor):
        self.monitor = monitor
        self.last_size = 0
        self.last_update_time = 0
        self.update_interval = 10.0  # 10ç§’é–“éš”ã«ç·©å’Œ

    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†ï¼ˆé–“éš”åˆ¶é™ä»˜ãï¼‰"""
        if event.is_directory:
            return

        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # æ›´æ–°é–“éš”ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆ10ç§’ä»¥å†…ã®é€£ç¶šæ›´æ–°ã‚’åˆ¶é™ï¼‰
            current_time = time.time()
            if current_time - self.last_update_time < self.update_interval:
                return

            self.last_update_time = current_time

            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self._handle_file_change,
                daemon=True
            ).start()

    def _handle_file_change(self):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã®å‡¦ç†ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ã€DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆãªã—ï¼‰"""
        try:
            print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸï¼ˆ10ç§’é–“éš”åˆ¶é™ï¼‰")
            print(f"ğŸ“Š watchdogç›£è¦–ã§é€²æ—è¡¨ç¤ºã‚’æ›´æ–°ã—ã¾ã™")

            # æ–°ã—ã„è¡Œã‚’é€²æ—è¡¨ç¤ºã®ã¿ï¼ˆDBã‚¤ãƒ³ã‚µãƒ¼ãƒˆãªã—ï¼‰
            if self.monitor.jsonl_file_path.exists():
                current_size = self.monitor.jsonl_file_path.stat().st_size
                print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {self.monitor.last_file_size} â†’ {current_size}")

                # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
                if current_size > self.monitor.last_file_size:
                    with open(self.monitor.jsonl_file_path, 'r', encoding='utf-8') as f:
                        f.seek(self.monitor.last_file_size)
                        new_content = f.read()

                    # æ–°ã—ã„è¡Œã‚’æ¤œå‡ºï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ï¼‰
                    new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
                    print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ï¼‰")

                    if new_lines:
                        # é€²æ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ã¿æ›´æ–°ï¼ˆDBã‚¤ãƒ³ã‚µãƒ¼ãƒˆãªã—ï¼‰
                        self.monitor.processed_lines += len(new_lines)
                        print(f"ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.monitor.processed_lines}ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ï¼‰")

                self.monitor.last_file_size = current_size

                # WebSocketé€šçŸ¥ã‚’é€ä¿¡ï¼ˆé€²æ—è¡¨ç¤ºç”¨ãƒ»é »åº¦åˆ¶é™ä»˜ãï¼‰
                current_time = time.time()
                if (self.monitor.websocket_callback and
                    current_time - self.monitor.last_websocket_time >= self.monitor.websocket_interval):
                    try:
                        import requests
                        response = requests.post(
                            'http://localhost:8000/api/tasks/internal/websocket-notify',
                            json={
                                'type': 'progress_update',
                                'task_id': self.monitor.task_id,
                                'file_lines': self.monitor.processed_lines,
                                'message': 'ãƒ•ã‚¡ã‚¤ãƒ«æ›´æ–°æ¤œå‡ºãƒ»é€²æ—è¡¨ç¤ºæ›´æ–°'
                            },
                            timeout=5
                        )
                        if response.status_code == 200:
                            self.monitor.last_websocket_time = current_time
                            print(f"ğŸ“¡ WebSocketé€²æ—é€šçŸ¥é€ä¿¡å®Œäº†ï¼ˆ15ç§’é–“éš”åˆ¶é™ï¼‰")
                    except Exception as ws_error:
                        print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                else:
                    print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—ï¼ˆ15ç§’é–“éš”åˆ¶é™ï¼‰")

        except Exception as e:
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´å‡¦ç†ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")


class ScrapyWatchdogMonitor:
    """scrapy crawl + watchdogç›£è¦–ã‚¯ãƒ©ã‚¹"""

    def __init__(self,
                 task_id: str,
                 project_path: str,
                 spider_name: str,
                 db_path: str = "backend/database/scrapy_ui.db",
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.project_path = Path(project_path)
        self.spider_name = spider_name
        self.db_path = db_path
        self.websocket_callback = websocket_callback

        # ç›£è¦–çŠ¶æ…‹
        self.is_monitoring = False
        self.observer = None
        self.loop = None

        # WebSocketé€šçŸ¥ã®é »åº¦åˆ¶é™
        self.last_websocket_time = 0
        self.websocket_interval = 15.0  # 15ç§’é–“éš”ã«åˆ¶é™
        self.processed_lines = 0
        self.last_file_size = 0

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.jsonl_file_path = self.project_path / f"results_{task_id}.jsonl"

        # Scrapyãƒ—ãƒ­ã‚»ã‚¹
        self.scrapy_process = None

        # æ ¹æœ¬å¯¾å¿œ: richprogressã¨åŒã˜ã‚¿ã‚¹ã‚¯äº‹å‰ä½œæˆ
        self._ensure_task_exists_like_richprogress()

    def _generate_data_hash_improved(self, item_data: dict) -> str:
        """item_typeã‚’è€ƒæ…®ã—ãŸæ”¹å–„ã•ã‚ŒãŸãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆå…¨ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å¯¾å¿œï¼‰"""
        try:
            # item_typeã«å¿œã˜ã¦é©åˆ‡ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’é¸æŠ
            item_type = item_data.get('item_type', 'unknown')

            if item_type == 'ranking_product':
                # ãƒ©ãƒ³ã‚­ãƒ³ã‚°å•†å“ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'ranking_position': item_data.get('ranking_position'),
                    'page_number': item_data.get('page_number'),
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'source_url': item_data.get('source_url')
                }
            elif item_type == 'ranking_product_detail':
                # å•†å“è©³ç´°ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'product_url': item_data.get('product_url'),
                    'description': item_data.get('description'),
                    'detail_scraped_at': item_data.get('detail_scraped_at')
                }
            elif item_type == 'test_product':
                # ãƒ†ã‚¹ãƒˆå•†å“ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'price': item_data.get('price'),
                    'test_id': item_data.get('test_id')
                }
            elif item_type == 'test_product_detail':
                # ãƒ†ã‚¹ãƒˆå•†å“è©³ç´°ã®å ´åˆ
                hash_data = {
                    'item_type': item_type,
                    'title': item_data.get('title'),
                    'description': item_data.get('description'),
                    'test_id': item_data.get('test_id')
                }
            else:
                # ãã®ä»–ã®å ´åˆã¯å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                hash_data = item_data.copy()

            # è¾æ›¸ã‚’ã‚½ãƒ¼ãƒˆã—ã¦JSONæ–‡å­—åˆ—ã«å¤‰æ›
            hash_string = json.dumps(hash_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(hash_string.encode('utf-8')).hexdigest()
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šãƒ‡ãƒ¼ã‚¿å…¨ä½“ã®ãƒãƒƒã‚·ãƒ¥
            data_str = json.dumps(item_data, sort_keys=True, ensure_ascii=False)
            return hashlib.md5(data_str.encode('utf-8')).hexdigest()

    async def execute_spider_with_monitoring(self,
                                           settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """watchdogç›£è¦–ä»˜ãã§scrapy crawlã‚’å®Ÿè¡Œ"""
        try:
            print(f"ğŸš€ watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {self.spider_name}")

            # ç¾åœ¨ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’ä¿å­˜
            self.loop = asyncio.get_event_loop()

            # 1. watchdogç›£è¦–ã‚’é–‹å§‹
            await self._start_watchdog_monitoring()

            # 2. scrapy crawlã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
            scrapy_task = asyncio.create_task(
                self._execute_scrapy_crawl(settings)
            )

            # 3. ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã¾ã§å¾…æ©Ÿ
            scrapy_result = await scrapy_task

            # 4. å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç›£è¦–åœæ­¢ï¼ˆæœ€å¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ç¢ºå®Ÿã«å‡¦ç†ï¼‰
            await asyncio.sleep(2)
            self._stop_watchdog_monitoring()

            # 5. æœ€çµ‚çš„ãªçµæœå‡¦ç†
            await self._process_remaining_lines()

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            final_success = scrapy_result['success'] or (self.processed_lines > 0)

            return {
                'success': final_success,
                'process_success': scrapy_result.get('process_success', scrapy_result['success']),
                'data_success': self.processed_lines > 0,
                'task_id': self.task_id,
                'items_processed': self.processed_lines,
                'scrapy_result': scrapy_result,
                'jsonl_file': str(self.jsonl_file_path)
            }

        except Exception as e:
            self._stop_watchdog_monitoring()
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }

    async def _start_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        # watchdogç„¡åŠ¹åŒ–ãƒã‚§ãƒƒã‚¯
        if self._is_watchdog_disabled():
            print(f"ğŸ›‘ Watchdog monitoring is disabled for task {self.task_id}")
            return

        if not WATCHDOG_AVAILABLE:
            raise Exception("watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")

        self.is_monitoring = True

        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç›£è¦–
        watch_directory = self.jsonl_file_path.parent

        print(f"ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±:")
        print(f"   - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹: {self.project_path}")
        print(f"   - JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹: {self.jsonl_file_path}")
        print(f"   - ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {watch_directory}")
        print(f"   - ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå­˜åœ¨: {watch_directory.exists()}")

        # ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not watch_directory.exists():
            print(f"âš ï¸ ç›£è¦–å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {watch_directory}")
            print(f"ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™...")
            watch_directory.mkdir(parents=True, exist_ok=True)
            print(f"âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆå®Œäº†: {watch_directory}")

        # ã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ä½œæˆ
        event_handler = JSONLWatchdogHandler(self)

        # Observerã‚’ä½œæˆã—ã¦ç›£è¦–é–‹å§‹
        self.observer = Observer()
        self.observer.schedule(event_handler, str(watch_directory), recursive=False)
        self.observer.start()

        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {watch_directory}")
        print(f"ğŸ“„ ç›£è¦–å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {self.jsonl_file_path}")

    def _is_watchdog_disabled(self):
        """watchdogç›£è¦–ãŒç„¡åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        import os

        # ç’°å¢ƒå¤‰æ•°ãƒã‚§ãƒƒã‚¯
        if os.environ.get('SCRAPY_WATCHDOG_DISABLED') == 'true':
            return True

        if os.environ.get(f'SCRAPY_WATCHDOG_DISABLED_{self.task_id}') == 'true':
            return True

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒã‚§ãƒƒã‚¯
        try:
            project_dir = Path(self.project_path)
            settings_file = project_dir / 'settings.py'

            if settings_file.exists():
                with open(settings_file, 'r', encoding='utf-8') as f:
                    content = f.read()

                # è¨­å®šå€¤ã‚’ãƒã‚§ãƒƒã‚¯
                if 'WATCHDOG_MONITORING_ENABLED = False' in content:
                    return True

                if 'SCRAPY_WATCHDOG_DISABLED = True' in content:
                    return True

        except Exception:
            pass

        return False

    def _stop_watchdog_monitoring(self):
        """watchdogç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False

        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None

        print(f"ğŸ›‘ watchdogç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")

    async def _execute_scrapy_crawl(self, settings: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """scrapy crawlwithwatchdogã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆDBæŒ¿å…¥æ©Ÿèƒ½ä»˜ãï¼‰"""
        try:
            # ã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰ï¼ˆcrawlwithwatchdogã‚’ä½¿ç”¨ï¼‰
            cmd = [
                sys.executable, "-m", "scrapy", "crawlwithwatchdog", self.spider_name,
                "-o", str(self.jsonl_file_path),  # JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                "--task-id", self.task_id,        # ã‚¿ã‚¹ã‚¯IDã‚’æŒ‡å®š
                "-s", "FEED_FORMAT=jsonlines",    # JSONLå½¢å¼æŒ‡å®š
                "-s", "LOG_LEVEL=INFO"
            ]

            # è¿½åŠ è¨­å®šãŒã‚ã‚Œã°é©ç”¨
            if settings:
                for key, value in settings.items():
                    cmd.extend(["-s", f"{key}={value}"])

            print(f"ğŸ“‹ å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            print(f"ğŸ“ å®Ÿè¡Œãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {self.project_path}")

            # ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šï¼ˆPlaywrightå¯¾å¿œå¼·åŒ–ï¼‰
            env = os.environ.copy()
            env['SCRAPY_TASK_ID'] = self.task_id
            env['SCRAPY_PROJECT_PATH'] = str(self.project_path)
            env['PYTHONPATH'] = str(self.project_path)

            # Playwrightç’°å¢ƒå¤‰æ•°ã‚’è¿½åŠ 
            env['PLAYWRIGHT_BROWSERS_PATH'] = '0'  # ã‚·ã‚¹ãƒ†ãƒ ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ä½¿ç”¨
            env['PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD'] = '1'
            env['DISPLAY'] = ':99'  # ä»®æƒ³ãƒ‡ã‚£ã‚¹ãƒ—ãƒ¬ã‚¤ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰

            # Node.jsç’°å¢ƒå¤‰æ•°ï¼ˆPlaywrightã«å¿…è¦ï¼‰
            env['NODE_OPTIONS'] = '--max-old-space-size=4096'

            # ãƒ‡ãƒãƒƒã‚°ç”¨ç’°å¢ƒå¤‰æ•°
            env['DEBUG'] = 'pw:api'  # Playwrightãƒ‡ãƒãƒƒã‚°æœ‰åŠ¹åŒ–

            # ãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            self.scrapy_process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(self.project_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                env=env
            )

            print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹é–‹å§‹: PID {self.scrapy_process.pid}")

            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = await self.scrapy_process.communicate()

            # çµæœã‚’è§£æï¼ˆæ”¹å–„ç‰ˆï¼šãƒ‡ãƒ¼ã‚¿å–å¾—çŠ¶æ³ã‚‚è€ƒæ…®ï¼‰
            process_success = self.scrapy_process.returncode == 0
            data_success = self.processed_lines > 0

            # æœ€çµ‚çš„ãªæˆåŠŸåˆ¤å®šï¼šãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ OR ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ
            success = process_success or data_success

            result = {
                'success': success,
                'process_success': process_success,
                'data_success': data_success,
                'return_code': self.scrapy_process.returncode,
                'processed_lines': self.processed_lines,
                'stdout': stdout.decode('utf-8', errors='ignore'),
                'stderr': stderr.decode('utf-8', errors='ignore')
            }

            # æ¨å¥¨å¯¾å¿œ: richprogressã¨åŒã˜çµæœãƒ•ã‚¡ã‚¤ãƒ«â†’DBä¿å­˜ã«å®Œå…¨ä¾å­˜
            print(f"ğŸ”§ æ¨å¥¨å¯¾å¿œ: richprogressã¨åŒã˜çµæœãƒ•ã‚¡ã‚¤ãƒ«â†’DBä¿å­˜ã‚’å®Ÿè¡Œ")
            self._store_results_to_db_like_richprogress()

            # æ ¹æœ¬å¯¾å¿œ: ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†æ™‚ã®ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            self._update_task_status_on_completion(success, process_success, data_success, result)

            if success:
                if process_success and data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸ + ãƒ‡ãƒ¼ã‚¿å–å¾—: {self.processed_lines}ä»¶ï¼‰")
                elif data_success:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ: {self.processed_lines}ä»¶ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚³ãƒ¼ãƒ‰: {self.scrapy_process.returncode}ï¼‰")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stderr: {result['stderr'][:500]}")
                    print(f"ğŸ” ãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› èª¿æŸ» - stdout: {result['stdout'][-500:]}")
                else:
                    print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ï¼ˆãƒ—ãƒ­ã‚»ã‚¹æˆåŠŸã€ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶ï¼‰")
            else:
                print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•— (code: {self.scrapy_process.returncode}, ãƒ‡ãƒ¼ã‚¿: {self.processed_lines}ä»¶)")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stderr: {result['stderr']}")
                print(f"ğŸ” å®Œå…¨å¤±æ•— - stdout: {result['stdout']}")

                # å¤±æ•—åŸå› ã®è©³ç´°åˆ†æ
                self._analyze_failure_cause(result)

            return result

        except Exception as e:
            print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise

    def _update_task_status_on_completion(self, success: bool, process_success: bool, data_success: bool, result: Dict[str, Any]):
        """ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†æ™‚ã®ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰"""
        try:
            from ..database import SessionLocal, Task, TaskStatus, Result
            from datetime import datetime

            db = SessionLocal()
            try:
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if not task:
                    print(f"âš ï¸ ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.task_id}")
                    return

                # å®Œå…¨çµ±è¨ˆåŒæœŸï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰
                actual_db_items = db.query(Result).filter(Result.task_id == self.task_id).count()

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚‚çµ±è¨ˆã‚’å–å¾—
                file_items_count = 0
                if self.jsonl_file_path and self.jsonl_file_path.exists():
                    try:
                        with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                            file_items_count = len([line for line in f if line.strip()])
                    except Exception as file_error:
                        print(f"âš ï¸ Error reading result file: {file_error}")

                # ã‚ˆã‚Šå¤šã„æ–¹ã‚’å®Ÿéš›ã®ã‚¢ã‚¤ãƒ†ãƒ æ•°ã¨ã™ã‚‹ï¼ˆå®Œå…¨åŒæœŸï¼‰
                final_items_count = max(actual_db_items, file_items_count, result.get('items_count', 0))
                final_requests_count = max(final_items_count + 5, result.get('requests_count', 1))

                # å®Œäº†æ™‚åˆ»ã‚’è¨­å®š
                task.finished_at = datetime.now()

                # æ ¹æœ¬å¯¾å¿œ: æ­£ç¢ºãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šï¼ˆå®Œå…¨çµ±è¨ˆåŒæœŸç‰ˆï¼‰
                if success and data_success and final_items_count > 0:
                    # ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ â†’ FINISHED
                    task.status = TaskStatus.FINISHED
                    task.items_count = final_items_count
                    task.requests_count = final_requests_count
                    task.error_count = 0
                    task.error_message = None
                    print(f"ğŸ”§ ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: {self.task_id[:8]}... â†’ FINISHED")
                    print(f"   Items: {final_items_count} (DB: {actual_db_items}, File: {file_items_count})")
                    print(f"   Requests: {final_requests_count}")

                elif success and process_success and final_items_count == 0:
                    # æ ¹æœ¬å¯¾å¿œå¼·åŒ–: ã‚¢ã‚¤ãƒ†ãƒ æ•°0ä»¶ã¯å¿…ãšFAILEDçŠ¶æ…‹ã«ã™ã‚‹
                    task.status = TaskStatus.FAILED
                    task.items_count = 0
                    task.requests_count = 1
                    task.error_count = (task.error_count or 0) + 1
                    task.error_message = "Process completed but no items were collected - marked as FAILED (lightprogress complete sync fix)"
                    print(f"ğŸ”§ ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: {self.task_id[:8]}... â†’ FAILED (no items collected - complete sync fix)")

                else:
                    # å¤±æ•— â†’ FAILED
                    task.status = TaskStatus.FAILED
                    task.items_count = final_items_count
                    task.requests_count = final_requests_count
                    task.error_count = (task.error_count or 0) + 1

                    # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ç”Ÿæˆ
                    error_details = []
                    if not process_success:
                        error_details.append(f"Process failed (code: {result.get('return_code', 'unknown')})")
                    if not data_success:
                        error_details.append("No data collected")
                    if result.get('stderr'):
                        error_details.append(f"Error: {result['stderr'][:200]}")

                    task.error_message = "; ".join(error_details)
                    print(f"ğŸ”§ ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°: {self.task_id[:8]}... â†’ FAILED ({'; '.join(error_details)})")
                    print(f"   Items: {final_items_count} (DB: {actual_db_items}, File: {file_items_count})")
                    print(f"   Requests: {final_requests_count}")

                task.updated_at = datetime.now()
                db.commit()

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _store_results_to_db_like_richprogress(self):
        """richprogressã¨åŒã˜æ–¹æ³•ã§çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’DBã«ä¿å­˜ï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰"""
        try:
            import json
            from pathlib import Path
            from ..database import SessionLocal, Result as DBResult
            import uuid
            import hashlib
            from datetime import datetime

            print(f"ğŸ“ Starting richprogress-style result storage for task {self.task_id}")

            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆrichprogressã¨åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            possible_paths = []

            # 1. JSONLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæœ€å„ªå…ˆï¼‰
            if self.jsonl_file_path and self.jsonl_file_path.exists():
                possible_paths.append(self.jsonl_file_path)

            # 2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢ï¼ˆæ¨å¥¨å¯¾å¿œ: æ‹¡å¼µãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            project_path = Path(self.project_path)
            patterns = [
                f"results/{self.task_id}.jsonl",
                f"results/{self.task_id}.json",
                f"results/results_{self.task_id}.jsonl",
                f"results_{self.task_id}.jsonl",
                f"results_{self.task_id}.json",
                f"*{self.task_id}*.json",
                f"*{self.task_id}*.jsonl",
                # æ¨å¥¨å¯¾å¿œ: Scrapyãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ¤œç´¢
                "ranking_results.jsonl",
                "items.jsonl",
                "output.jsonl",
                "*.jsonl",
                "*.json"
            ]

            for pattern in patterns:
                files = list(project_path.glob(pattern))
                if files:
                    latest_file = max(files, key=lambda f: f.stat().st_mtime)
                    possible_paths.append(latest_file)
                    break

            # å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹
            result_path = None
            for path in possible_paths:
                if path and path.exists() and path.stat().st_size > 0:
                    result_path = path
                    break

            if not result_path:
                print(f"ğŸ“ No valid result file found for task {self.task_id}")
                print(f"   Searched paths: {[str(p) for p in possible_paths if p]}")
                return

            print(f"ğŸ“ Found result file for task {self.task_id}: {result_path}")

            db = SessionLocal()
            try:
                # æ—¢å­˜ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆé‡è¤‡é˜²æ­¢ï¼‰
                existing_count = db.query(DBResult).filter(DBResult.task_id == self.task_id).count()
                if existing_count > 0:
                    print(f"âš ï¸ Task {self.task_id} already has {existing_count} results in database, skipping to prevent duplicates")
                    return

                with open(result_path, 'r', encoding='utf-8') as f:
                    content = f.read().strip()

                if not content:
                    print(f"ğŸ“ Empty result file for task {self.task_id}")
                    return

                stored_count = 0

                # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦å‡¦ç†ï¼ˆæ¨å¥¨å¯¾å¿œ: ã‚¨ãƒ©ãƒ¼è€æ€§å¼·åŒ–ï¼‰
                if content.count('\n') > 0 or result_path.suffix == '.jsonl':
                    items = content.strip().split('\n')

                    for line_num, line in enumerate(items, 1):
                        line = line.strip()
                        if line:
                            try:
                                item_data = json.loads(line)

                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆrichprogressã¨åŒã˜æ–¹æ³•ï¼‰
                                data_hash = None
                                if isinstance(item_data, dict):
                                    product_url = item_data.get('product_url', '')
                                    ranking_position = item_data.get('ranking_position', '')

                                    if product_url:
                                        data_hash = hashlib.md5(product_url.encode('utf-8')).hexdigest()
                                    elif ranking_position:
                                        data_hash = hashlib.md5(f"pos_{ranking_position}".encode('utf-8')).hexdigest()

                                # æ—¥æ™‚ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†
                                crawl_start_datetime = None
                                item_acquired_datetime = None

                                if isinstance(item_data, dict):
                                    if 'crawl_start_datetime' in item_data:
                                        try:
                                            crawl_start_datetime = datetime.fromisoformat(item_data['crawl_start_datetime'].replace('Z', '+00:00'))
                                        except (ValueError, TypeError):
                                            crawl_start_datetime = datetime.now()

                                    if 'item_acquired_datetime' in item_data:
                                        try:
                                            item_acquired_datetime = datetime.fromisoformat(item_data['item_acquired_datetime'].replace('Z', '+00:00'))
                                        except (ValueError, TypeError):
                                            item_acquired_datetime = datetime.now()

                                # DBã«ä¿å­˜ï¼ˆrichprogressã¨åŒã˜æ–¹æ³•ï¼‰
                                db_result = DBResult(
                                    id=str(uuid.uuid4()),
                                    task_id=self.task_id,
                                    data=item_data,
                                    data_hash=data_hash,
                                    created_at=datetime.now(),
                                    crawl_start_datetime=crawl_start_datetime,
                                    item_acquired_datetime=item_acquired_datetime
                                )
                                db.add(db_result)
                                stored_count += 1

                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ Invalid JSON in result line: {line[:100]}... Error: {e}")
                                continue

                    db.commit()
                    print(f"âœ… Stored {stored_count} items (JSONL format) to DB for task {self.task_id} using richprogress method")

                else:
                    # å˜ä¸€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                    try:
                        data = json.loads(content)

                        if isinstance(data, list):
                            # JSONé…åˆ—ã®å ´åˆ
                            for item in data:
                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
                                data_hash = None
                                if isinstance(item, dict):
                                    product_url = item.get('product_url', '')
                                    if product_url:
                                        data_hash = hashlib.md5(product_url.encode('utf-8')).hexdigest()

                                db_result = DBResult(
                                    id=str(uuid.uuid4()),
                                    task_id=self.task_id,
                                    data=item,
                                    data_hash=data_hash,
                                    created_at=datetime.now(),
                                    item_acquired_datetime=datetime.now()
                                )
                                db.add(db_result)
                                stored_count += 1
                        else:
                            # å˜ä¸€ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®å ´åˆ
                            db_result = DBResult(
                                id=str(uuid.uuid4()),
                                task_id=self.task_id,
                                data=data,
                                created_at=datetime.now(),
                                item_acquired_datetime=datetime.now()
                            )
                            db.add(db_result)
                            stored_count = 1

                        db.commit()
                        print(f"âœ… Stored {stored_count} items (JSON format) to DB for task {self.task_id} using richprogress method")

                    except json.JSONDecodeError as e:
                        print(f"âŒ Unable to parse result file for task {self.task_id}: {e}")

            finally:
                db.close()

        except Exception as e:
            print(f"âŒ Error storing results to DB for task {self.task_id}: {e}")

    def _ensure_task_exists_like_richprogress(self):
        """richprogressã¨åŒã˜æ–¹æ³•ã§ã‚¿ã‚¹ã‚¯ã‚’äº‹å‰ä½œæˆï¼ˆæ ¹æœ¬å¯¾å¿œï¼‰"""
        try:
            from ..database import SessionLocal, Task as DBTask, TaskStatus, Project as DBProject, Spider as DBSpider, User as DBUser
            from datetime import datetime
            import pytz

            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã®å­˜åœ¨ç¢ºèª
                existing_task = db.query(DBTask).filter(DBTask.id == self.task_id).first()

                if existing_task:
                    print(f"âœ… Task {self.task_id} already exists")
                    return self.task_id

                print(f"ğŸ”§ Creating task like richprogress: {self.task_id}")

                # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’æ¤œç´¢
                spider = db.query(DBSpider).filter(DBSpider.name == self.spider_name).first()
                if not spider:
                    print(f"âŒ Spider {self.spider_name} not found in database")
                    return self.task_id

                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
                project = db.query(DBProject).filter(DBProject.id == spider.project_id).first()
                if not project:
                    print(f"âŒ Project for spider {self.spider_name} not found")
                    return self.task_id

                # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’å–å¾—ï¼ˆã‚·ã‚¹ãƒ†ãƒ ãƒ¦ãƒ¼ã‚¶ãƒ¼ï¼‰
                user = db.query(DBUser).first()
                if not user:
                    print(f"âŒ No user found in database")
                    return self.task_id

                # ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆï¼ˆrichprogressã¨åŒã˜æ–¹æ³•ï¼‰
                jst = pytz.timezone('Asia/Tokyo')
                current_time = datetime.now(jst).replace(tzinfo=None)

                new_task = DBTask(
                    id=self.task_id,
                    project_id=project.id,
                    spider_id=spider.id,
                    status=TaskStatus.RUNNING,
                    items_count=0,
                    requests_count=0,
                    error_count=0,
                    created_at=current_time,
                    started_at=current_time,
                    updated_at=current_time,
                    user_id=user.id
                )

                db.add(new_task)
                db.commit()

                print(f"âœ… Created task like richprogress: {self.task_id}")
                return self.task_id

            finally:
                db.close()

        except Exception as e:
            print(f"âŒ Error creating task like richprogress: {e}")
            return self.task_id

    def _analyze_failure_cause(self, result: Dict[str, Any]):
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› ã‚’åˆ†æ"""
        try:
            print(f"ğŸ” === Scrapyãƒ—ãƒ­ã‚»ã‚¹å¤±æ•—åŸå› åˆ†æé–‹å§‹ ===")
            print(f"ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰: {result['return_code']}")

            stderr = result['stderr']
            stdout = result['stdout']

            # ä¸€èˆ¬çš„ãªå¤±æ•—ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ
            failure_patterns = {
                'ImportError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼',
                'ModuleNotFoundError': 'ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'AttributeError': 'å±æ€§ã‚¨ãƒ©ãƒ¼',
                'SyntaxError': 'ã‚·ãƒ³ã‚¿ãƒƒã‚¯ã‚¹ã‚¨ãƒ©ãƒ¼',
                'IndentationError': 'ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼',
                'NameError': 'åå‰ã‚¨ãƒ©ãƒ¼',
                'TypeError': 'å‹ã‚¨ãƒ©ãƒ¼',
                'ValueError': 'å€¤ã‚¨ãƒ©ãƒ¼',
                'ConnectionError': 'æ¥ç¶šã‚¨ãƒ©ãƒ¼',
                'TimeoutError': 'ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼',
                'PermissionError': 'æ¨©é™ã‚¨ãƒ©ãƒ¼',
                'FileNotFoundError': 'ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„',
                'twisted.internet.error': 'Twistedã‚¨ãƒ©ãƒ¼',
                'scrapy.exceptions': 'Scrapyã‚¨ãƒ©ãƒ¼',
                'playwright': 'Playwrightã‚¨ãƒ©ãƒ¼',
                'ERROR': 'ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼',
                'CRITICAL': 'é‡å¤§ãªã‚¨ãƒ©ãƒ¼',
                'Traceback': 'Pythonä¾‹å¤–',
                'Exception': 'ä¾‹å¤–ç™ºç”Ÿ'
            }

            detected_issues = []
            for pattern, description in failure_patterns.items():
                if pattern in stderr or pattern in stdout:
                    detected_issues.append(f"{description} ({pattern})")

            if detected_issues:
                print(f"ğŸ” æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:")
                for issue in detected_issues:
                    print(f"   - {issue}")
            else:
                print(f"ğŸ” æ—¢çŸ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«è©²å½“ã—ãªã„å¤±æ•—")

            # ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰åˆ¥ã®åˆ†æ
            return_code_meanings = {
                1: "ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼",
                2: "ã‚·ã‚§ãƒ«ã®èª¤ç”¨",
                126: "å®Ÿè¡Œæ¨©é™ãªã—",
                127: "ã‚³ãƒãƒ³ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„",
                128: "ç„¡åŠ¹ãªçµ‚äº†å¼•æ•°",
                130: "Ctrl+Cã«ã‚ˆã‚‹ä¸­æ–­",
                137: "SIGKILL (å¼·åˆ¶çµ‚äº†)",
                139: "ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é•å"
            }

            if result['return_code'] in return_code_meanings:
                print(f"ğŸ” ãƒªã‚¿ãƒ¼ãƒ³ã‚³ãƒ¼ãƒ‰ {result['return_code']}: {return_code_meanings[result['return_code']]}")

            print(f"ğŸ” === å¤±æ•—åŸå› åˆ†æå®Œäº† ===")

        except Exception as e:
            print(f"âŒ å¤±æ•—åŸå› åˆ†æã‚¨ãƒ©ãƒ¼: {e}")

    def _process_new_lines_threading(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        print(f"ğŸ§µ _process_new_lines_threadingé–‹å§‹: {threading.current_thread().name}")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                print(f"ğŸ“Š watchdogç›£è¦–ã§é€²æ—è¡¨ç¤ºã‚’æ›´æ–°ã—ã¾ã™")
                print(f"ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {self.last_file_size} â†’ {current_size}")
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ï¼‰")

                # é€²æ—è¡¨ç¤ºã®ã¿ï¼ˆDBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã¯è¡Œã‚ãªã„ï¼‰
                self.processed_lines += len(new_lines)
                print(f"ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.processed_lines}ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ï¼‰")

                # WebSocketé€šçŸ¥ï¼ˆé€²æ—è¡¨ç¤ºã®ã¿ãƒ»é »åº¦åˆ¶é™ä»˜ãï¼‰
                try:
                    current_time = time.time()
                    if (self.websocket_callback and len(new_lines) > 0 and
                        current_time - self.last_websocket_time >= self.websocket_interval):
                        # 15ç§’é–“éš”ã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify_threading({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': len(new_lines),
                            'total_items': self.processed_lines
                        })
                        self.last_websocket_time = current_time
                        print(f"ğŸ“¡ WebSocketé€²æ—é€šçŸ¥é€ä¿¡å®Œäº†ï¼ˆ15ç§’é–“éš”åˆ¶é™ï¼‰")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, new_lines={len(new_lines)}, interval_ok={current_time - self.last_websocket_time >= self.websocket_interval}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lines_threadingå®Œäº†")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰"""
        # asyncãƒ©ãƒƒãƒ‘ãƒ¼ã‚’å‰Šé™¤ã—ã€ç›´æ¥åŒæœŸå‡¦ç†ã‚’å®Ÿè¡Œ
        print(f"ğŸ” _process_new_linesé–‹å§‹ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")
        try:
            if not self.jsonl_file_path.exists():
                print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.jsonl_file_path}")
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: ç¾åœ¨={current_size}, å‰å›={self.last_file_size}")
            if current_size <= self.last_file_size:
                print(f"ğŸ” æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
                return

            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            print(f"ğŸ” æ–°ã—ã„å†…å®¹ã‚’èª­ã¿å–ã‚Šä¸­...")
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()

            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            print(f"ğŸ” æ–°ã—ã„è¡Œæ•°: {len(new_lines)}")

            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶")

                # ç›´æ¥DBæŒ¿å…¥å‡¦ç†ï¼ˆCeleryã‚¿ã‚¹ã‚¯ã‚’ä½¿ã‚ãªã„ï¼‰
                successful_inserts = 0
                print(f"ğŸ” ç›´æ¥DBæŒ¿å…¥å‡¦ç†é–‹å§‹: {len(new_lines)}ä»¶ã®æ–°ã—ã„è¡Œ")

                # ãƒãƒ«ã‚¯DBæŒ¿å…¥å‡¦ç†
                print(f"ğŸ” ãƒãƒ«ã‚¯DBæŒ¿å…¥é–‹å§‹: {len(new_lines)}ä»¶")
                successful_inserts = self._bulk_insert_items(new_lines)
                self.processed_lines += successful_inserts
                print(f"âœ… ãƒãƒ«ã‚¯DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                print(f"âœ… ç›´æ¥DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(new_lines)}ä»¶")

                # WebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ãƒ»é »åº¦åˆ¶é™ä»˜ãï¼‰
                print(f"ğŸ” WebSocketé€šçŸ¥é–‹å§‹...")
                try:
                    current_time = time.time()
                    if (self.websocket_callback and successful_inserts > 0 and
                        current_time - self.last_websocket_time >= self.websocket_interval):
                        print(f"ğŸ” WebSocketé€šçŸ¥å®Ÿè¡Œä¸­...")
                        # 15ç§’é–“éš”ã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
                        self._safe_websocket_notify({
                            'type': 'items_update',
                            'task_id': self.task_id,
                            'new_items': successful_inserts,
                            'total_items': self.processed_lines
                        })
                        self.last_websocket_time = current_time
                        print(f"âœ… WebSocketé€šçŸ¥å®Œäº†ï¼ˆ15ç§’é–“éš”åˆ¶é™ï¼‰")
                    else:
                        print(f"ğŸ” WebSocketé€šçŸ¥ã‚¹ã‚­ãƒƒãƒ—: callback={self.websocket_callback is not None}, inserts={successful_inserts}, interval_ok={current_time - self.last_websocket_time >= self.websocket_interval}")
                except Exception as ws_error:
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {ws_error}")
                    import traceback
                    print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            print(f"ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºæ›´æ–°: {current_size}")
            self.last_file_size = current_size
            print(f"âœ… _process_new_lineså®Œäº†ï¼ˆå®Œå…¨åŒæœŸç‰ˆï¼‰")

        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")

    async def _process_remaining_lines(self):
        """æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†ï¼ˆæœ€çµ‚å‡¦ç†ï¼‰"""
        try:
            if not self.jsonl_file_path.exists():
                return

            # ãƒ•ã‚¡ã‚¤ãƒ«å…¨ä½“ã‚’èª­ã¿ç›´ã—ã¦æœªå‡¦ç†ã®è¡ŒãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                all_lines = f.readlines()

            total_lines = len([line for line in all_lines if line.strip()])
            remaining_lines = total_lines - self.processed_lines

            if remaining_lines > 0:
                print(f"ğŸ“ æ®‹ã‚Šã®è¡Œã‚’å‡¦ç†: {remaining_lines}ä»¶")

                # æœªå‡¦ç†ã®è¡Œã‚’ãƒãƒ«ã‚¯å‡¦ç†
                remaining_lines_data = []
                for i in range(self.processed_lines, total_lines):
                    if i < len(all_lines):
                        line = all_lines[i].strip()
                        if line:
                            remaining_lines_data.append(line)

                # ãƒãƒ«ã‚¯DBæŒ¿å…¥
                if remaining_lines_data:
                    successful_inserts = self._bulk_insert_items(remaining_lines_data)
                    self.processed_lines += successful_inserts
                    print(f"âœ… æ®‹ã‚Šè¡Œãƒãƒ«ã‚¯DBæŒ¿å…¥å®Œäº†: {successful_inserts}/{len(remaining_lines_data)}ä»¶")
                else:
                    successful_inserts = 0

            print(f"âœ… æœ€çµ‚å‡¦ç†å®Œäº†: ç·å‡¦ç†è¡Œæ•° {self.processed_lines}")

        except Exception as e:
            print(f"âŒ æ®‹ã‚Šè¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")

    def _sync_insert_item_threading(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆthreadingç‰ˆãƒ»asyncioå®Œå…¨å›é¿ï¼‰"""
        import threading
        max_retries = 3
        retry_count = 0

        print(f"ğŸ§µ DBæŒ¿å…¥é–‹å§‹: {threading.current_thread().name}")

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                    data_hash = self._generate_data_hash_improved(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()
                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash}")
                            return True  # é‡è¤‡ã¯æˆåŠŸã¨ã¿ãªã™

                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,  # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¿½åŠ 
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1}) - Thread: {threading.current_thread().name}")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆthreadingç‰ˆï¼‰
                    self._update_task_statistics_threading()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _sync_insert_item(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                # SQLAlchemyã‚’ä½¿ç”¨ã—ã¦DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                from ..database import SessionLocal, Result

                db = SessionLocal()
                try:
                    # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                    data_hash = self._generate_data_hash_improved(item_data)

                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()
                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash}")
                            return True  # é‡è¤‡ã¯æˆåŠŸã¨ã¿ãªã™

                    # resultsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
                    result_id = str(uuid.uuid4())
                    db_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,  # ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¿½åŠ 
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(db_result)
                    db.commit()

                    print(f"âœ… DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {result_id[:8]}... (è©¦è¡Œ: {retry_count + 1})")

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆåˆ¥ã®ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ã§ï¼‰
                    self._update_task_statistics_safe()

                    return True  # æˆåŠŸ

                except Exception as e:
                    db.rollback()
                    retry_count += 1
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                    if retry_count >= max_retries:
                        raise
                    else:
                        # çŸ­æ™‚é–“å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤
                        import time
                        time.sleep(0.1 * retry_count)

                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return False

        return False

    def _realtime_insert_item_threading(self, line: str) -> bool:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ï¼ˆthreadingç‰ˆãƒ»1ä»¶ãšã¤å‡¦ç†ï¼‰"""
        if not line.strip():
            return False

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result
                import json
                import uuid
                import hashlib
                from datetime import datetime

                # JSONè§£æ
                try:
                    item_data = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                    return False

                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                data_hash = self._generate_data_hash_improved(item_data)

                db = SessionLocal()
                try:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()

                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                            return False

                    # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                    result_id = str(uuid.uuid4())
                    new_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(new_result)
                    db.commit()

                    print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æˆåŠŸ: {result_id[:8]}...")
                    return True

                except Exception as e:
                    db.rollback()
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æœ€çµ‚å¤±æ•—: {e}")
                    return False
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return False

    def _realtime_insert_item(self, line: str) -> bool:
        """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ï¼ˆé€šå¸¸ç‰ˆãƒ»1ä»¶ãšã¤å‡¦ç†ï¼‰"""
        if not line.strip():
            return False

        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result
                import json
                import uuid
                import hashlib
                from datetime import datetime

                # JSONè§£æ
                try:
                    item_data = json.loads(line)
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
                    return False

                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                data_hash = self._generate_data_hash_improved(item_data)

                db = SessionLocal()
                try:
                    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    if data_hash:
                        existing = db.query(Result).filter(
                            Result.task_id == self.task_id,
                            Result.data_hash == data_hash
                        ).first()

                        if existing:
                            print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                            return False

                    # æ–°ã—ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’ä½œæˆ
                    result_id = str(uuid.uuid4())
                    new_result = Result(
                        id=result_id,
                        task_id=self.task_id,
                        data=item_data,
                        data_hash=data_hash,
                        item_acquired_datetime=datetime.now(),
                        created_at=datetime.now()
                    )

                    db.add(new_result)
                    db.commit()

                    print(f"âœ… ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æˆåŠŸ: {result_id[:8]}...")
                    return True

                except Exception as e:
                    db.rollback()
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥ã‚¨ãƒ©ãƒ¼: {e}")
                    raise
                finally:
                    db.close()

            except Exception as e:
                retry_count += 1
                if retry_count >= max_retries:
                    print(f"âŒ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ DBæŒ¿å…¥æœ€çµ‚å¤±æ•—: {e}")
                    return False
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return False

    def _bulk_insert_items_threading(self, lines: List[str]) -> int:
        """ãƒãƒ«ã‚¯DBæŒ¿å…¥ï¼ˆthreadingç‰ˆï¼‰"""
        if not lines:
            return 0

        max_retries = 3
        retry_count = 0
        batch_size = 100  # ãƒãƒƒãƒã‚µã‚¤ã‚º

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result

                # JSONè§£æã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                items_data = []
                for line in lines:
                    try:
                        item_data = json.loads(line.strip())
                        items_data.append(item_data)
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:50]}...")
                        continue

                if not items_data:
                    print("âŒ æœ‰åŠ¹ãªJSONãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return 0

                successful_inserts = 0

                # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
                for i in range(0, len(items_data), batch_size):
                    batch = items_data[i:i + batch_size]

                    db = SessionLocal()
                    try:
                        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆè»½é‡é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                        bulk_data = []

                        for item_data in batch:
                            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                            data_hash = self._generate_data_hash_improved(item_data)

                            if not data_hash:
                                print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆå¤±æ•—: {item_data}")
                                continue

                            # è»½é‡é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ã‚¿ã‚¹ã‚¯å†…ã®ã¿ï¼‰
                            existing = db.query(Result).filter(
                                Result.task_id == self.task_id,
                                Result.data_hash == data_hash
                            ).first()

                            if existing:
                                print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                                continue

                            result_id = str(uuid.uuid4())
                            bulk_item = {
                                'id': result_id,
                                'task_id': self.task_id,
                                'data': item_data,
                                'data_hash': data_hash,
                                'item_acquired_datetime': datetime.now(),
                                'created_at': datetime.now()
                            }
                            bulk_data.append(bulk_item)

                        # é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰
                        if bulk_data:
                            print(f"ğŸš€ é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ: {len(bulk_data)}ä»¶ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰")

                            for item in bulk_data:
                                try:
                                    db_result = Result(
                                        id=item['id'],
                                        task_id=item['task_id'],
                                        data=item['data'],
                                        data_hash=item['data_hash'],
                                        item_acquired_datetime=item['item_acquired_datetime'],
                                        created_at=item['created_at']
                                    )
                                    db.add(db_result)
                                except Exception as e:
                                    print(f"âŒ ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                                    continue

                            db.commit()
                            print(f"âœ… é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {len(bulk_data)}ä»¶ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰")
                        else:
                            print("âš ï¸ ãƒãƒ«ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰")

                        successful_inserts += len(batch)
                        print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {len(batch)}ä»¶ (ç´¯è¨ˆ: {successful_inserts}/{len(items_data)}) - Thread: {threading.current_thread().name}")

                    except Exception as e:
                        db.rollback()
                        print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i//batch_size + 1}): {e}")
                        # ãƒãƒƒãƒãŒå¤±æ•—ã—ãŸå ´åˆã¯å€‹åˆ¥ã«å‡¦ç†ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        for item_data in batch:
                            try:
                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                                data_hash = self._generate_data_hash_improved(item_data)

                                result_id = str(uuid.uuid4())
                                db_result = Result(
                                    id=result_id,
                                    task_id=self.task_id,
                                    data=item_data,
                                    data_hash=data_hash,
                                    item_acquired_datetime=datetime.now(),
                                    created_at=datetime.now()
                                )
                                db.add(db_result)
                                db.commit()
                                successful_inserts += 1
                            except Exception as individual_error:
                                db.rollback()
                                print(f"âŒ å€‹åˆ¥ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {individual_error}")
                    finally:
                        db.close()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                if successful_inserts > 0:
                    self._update_task_statistics_threading()

                print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {successful_inserts}/{len(items_data)}ä»¶ - Thread: {threading.current_thread().name}")
                return successful_inserts

            except Exception as e:
                retry_count += 1
                print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                if retry_count >= max_retries:
                    print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return 0
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return 0

    def _bulk_insert_items(self, lines: List[str]) -> int:
        """ãƒãƒ«ã‚¯DBæŒ¿å…¥ï¼ˆé€šå¸¸ç‰ˆï¼‰"""
        if not lines:
            return 0

        max_retries = 3
        retry_count = 0
        batch_size = 100  # ãƒãƒƒãƒã‚µã‚¤ã‚º

        while retry_count < max_retries:
            try:
                from ..database import SessionLocal, Result

                # JSONè§£æã¨ãƒ‡ãƒ¼ã‚¿æº–å‚™
                items_data = []
                for line in lines:
                    try:
                        item_data = json.loads(line.strip())
                        items_data.append(item_data)
                    except json.JSONDecodeError as e:
                        print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e} - Line: {line[:50]}...")
                        continue

                if not items_data:
                    print("âŒ æœ‰åŠ¹ãªJSONãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
                    return 0

                successful_inserts = 0

                # ãƒãƒƒãƒã”ã¨ã«å‡¦ç†
                for i in range(0, len(items_data), batch_size):
                    batch = items_data[i:i + batch_size]

                    db = SessionLocal()
                    try:
                        # ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™ï¼ˆè»½é‡é‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰
                        bulk_data = []

                        for item_data in batch:
                            # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                            data_hash = self._generate_data_hash_improved(item_data)

                            if not data_hash:
                                print(f"âš ï¸ ãƒãƒƒã‚·ãƒ¥ç”Ÿæˆå¤±æ•—: {item_data}")
                                continue

                            # è»½é‡é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆåŒä¸€ã‚¿ã‚¹ã‚¯å†…ã®ã¿ï¼‰
                            existing = db.query(Result).filter(
                                Result.task_id == self.task_id,
                                Result.data_hash == data_hash
                            ).first()

                            if existing:
                                print(f"âš ï¸ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚­ãƒƒãƒ—: {data_hash[:8]}...")
                                continue

                            result_id = str(uuid.uuid4())
                            bulk_data.append({
                                'id': result_id,
                                'task_id': self.task_id,
                                'data': item_data,
                                'data_hash': data_hash,
                                'item_acquired_datetime': datetime.now(),
                                'created_at': datetime.now()
                            })

                        # é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰
                        if bulk_data:
                            print(f"ğŸš€ é«˜é€Ÿãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Ÿè¡Œ: {len(bulk_data)}ä»¶ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰")

                            db.bulk_insert_mappings(Result, bulk_data)
                            db.commit()
                        else:
                            print("âš ï¸ ãƒãƒ«ã‚¯ãƒ‡ãƒ¼ã‚¿ãŒç©ºã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰")

                        successful_inserts += len(batch)
                        print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæˆåŠŸ: {len(batch)}ä»¶ (ç´¯è¨ˆ: {successful_inserts}/{len(items_data)})")

                    except Exception as e:
                        db.rollback()
                        print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (ãƒãƒƒãƒ {i//batch_size + 1}): {e}")
                        # ãƒãƒƒãƒãŒå¤±æ•—ã—ãŸå ´åˆã¯å€‹åˆ¥ã«å‡¦ç†ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ãªã—ï¼‰
                        for item_data in batch:
                            try:
                                # ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆï¼ˆæ”¹å–„ç‰ˆï¼šitem_typeè€ƒæ…®ï¼‰
                                data_hash = self._generate_data_hash_improved(item_data)

                                result_id = str(uuid.uuid4())
                                db_result = Result(
                                    id=result_id,
                                    task_id=self.task_id,
                                    data=item_data,
                                    data_hash=data_hash,
                                    item_acquired_datetime=datetime.now(),
                                    created_at=datetime.now()
                                )
                                db.add(db_result)
                                db.commit()
                                successful_inserts += 1
                            except Exception as individual_error:
                                db.rollback()
                                print(f"âŒ å€‹åˆ¥ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {individual_error}")
                    finally:
                        db.close()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                if successful_inserts > 0:
                    self._update_task_statistics_safe()

                print(f"âœ… ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆå®Œäº†: {successful_inserts}/{len(items_data)}ä»¶")
                return successful_inserts

            except Exception as e:
                retry_count += 1
                print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {retry_count}/{max_retries}): {e}")

                if retry_count >= max_retries:
                    print(f"âŒ ãƒãƒ«ã‚¯DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆæœ€çµ‚å¤±æ•—: {e}")
                    return 0
                else:
                    import time
                    time.sleep(0.1 * retry_count)

        return 0

    def _update_task_statistics(self, db):
        """ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°"""
        try:
            from ..database import Task, Result

            # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
            task = db.query(Task).filter(Task.id == self.task_id).first()
            if task:
                # çµæœæ•°ã‚’å–å¾—
                result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®æ­£å¸¸åŒ–ï¼‰
                task.items_count = result_count

                # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã®æ­£å¸¸åŒ–ï¼ˆç•°å¸¸ã«å¤§ãã„å€¤ã‚’é˜²æ­¢ï¼‰
                estimated_normal_requests = result_count + 20  # ã‚¢ã‚¤ãƒ†ãƒ æ•° + åˆæœŸãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
                current_requests = task.requests_count or 0

                if current_requests <= estimated_normal_requests * 2:
                    # ç¾åœ¨å€¤ãŒæ­£å¸¸ç¯„å›²å†…ã®å ´åˆã¯ãã®ã¾ã¾
                    pass
                else:
                    # ç•°å¸¸ã«å¤§ãã„å ´åˆã¯æ¨å®šå€¤ã«ä¿®æ­£
                    task.requests_count = estimated_normal_requests
                    print(f"âš ï¸ Watchdog: Abnormal request count detected for task {self.task_id}, corrected to {estimated_normal_requests}")

                task.updated_at = datetime.now()

                db.commit()
                print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id} - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_threading(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            from ..database import SessionLocal, Task, Result

            print(f"ğŸ§µ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°é–‹å§‹: {threading.current_thread().name}")
            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ï¼ˆé‡è¤‡é˜²æ­¢ï¼šæœ€å¤§å€¤ã®ã¿æ›´æ–°ï¼‰
                    task.items_count = max(result_count, task.items_count or 0)

                    # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã¯æ¨å®šå€¤ã¨ç¾åœ¨å€¤ã®æœ€å¤§å€¤
                    estimated_requests = result_count + 15
                    task.requests_count = max(estimated_requests, task.requests_count or 0)

                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°: {self.task_id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count} - Thread: {threading.current_thread().name}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _update_task_statistics_safe(self):
        """å®‰å…¨ãªã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
        try:
            from ..database import SessionLocal, Task, Result
            from .task_statistics_validator import task_validator

            db = SessionLocal()
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = db.query(Task).filter(Task.id == self.task_id).first()
                if task:
                    # çµæœæ•°ã‚’å–å¾—
                    result_count = db.query(Result).filter(Result.task_id == self.task_id).count()

                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹ã®çµ±è¨ˆã‚‚ç¢ºèª
                    file_items, file_requests = task_validator._get_file_statistics(task)

                    # ã‚ˆã‚Šæ­£ç¢ºãªçµ±è¨ˆã‚’ä½¿ç”¨
                    final_items = max(result_count, file_items, task.items_count or 0)
                    final_requests = max(file_requests, result_count + 15, task.requests_count or 0)

                    # ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°
                    task.items_count = final_items
                    task.requests_count = final_requests

                    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®è‡ªå‹•ä¿®æ­£ï¼ˆæ ¹æœ¬å¯¾å¿œç‰ˆï¼‰
                    from ..database import TaskStatus

                    # æ­£ç¢ºãªã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
                    if final_items > 0:
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—æˆåŠŸ â†’ FINISHED
                        if task.status.name in ['FAILED', 'RUNNING']:
                            task.status = TaskStatus.FINISHED
                            task.error_count = 0
                            print(f"ğŸ”§ Status corrected: {self.task_id[:8]}... {task.status.name} â†’ FINISHED (items: {final_items})")
                    else:
                        # ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•— â†’ FAILED
                        if task.status.name in ['FINISHED', 'RUNNING']:
                            task.status = TaskStatus.FAILED
                            task.error_count = task.error_count + 1 if task.error_count else 1
                            task.error_message = f"No items collected. Requests: {final_requests}, Duration: {(task.finished_at - task.started_at).total_seconds() if task.started_at and task.finished_at else 'unknown'}s"
                            print(f"ğŸ”§ Status corrected: {self.task_id[:8]}... {task.status.name} â†’ FAILED (no items collected)")

                    task.updated_at = datetime.now()

                    db.commit()
                    print(f"ğŸ“Š Enhanced task statistics updated: {self.task_id[:8]}... - Items: {final_items}, Requests: {final_requests}")

            except Exception as e:
                db.rollback()
                print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
            finally:
                db.close()

        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify_threading(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆthreadingç‰ˆï¼‰"""
        import threading
        try:
            if not self.websocket_callback:
                return

            print(f"ğŸ§µ WebSocketé€šçŸ¥é–‹å§‹: {threading.current_thread().name}")

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')} - Thread: {threading.current_thread().name}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")

    def _safe_websocket_notify(self, data: Dict[str, Any]):
        """å®‰å…¨ãªWebSocketé€šçŸ¥ï¼ˆåŒæœŸçš„ï¼‰"""
        try:
            if not self.websocket_callback:
                return

            # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆã§WebSocketé€šçŸ¥ã‚’é€ä¿¡
            import requests

            # ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã®WebSocketé€šçŸ¥ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã—
            response = requests.post(
                'http://localhost:8000/api/tasks/internal/websocket-notify',
                json=data,
                timeout=5
            )

            if response.status_code == 200:
                print(f"ğŸ“¡ WebSocket notification sent: Task {data.get('task_id', 'unknown')[:8]}... - {data.get('type', 'unknown')}")
            else:
                print(f"ğŸ“¡ WebSocket notification failed: {response.status_code}")

        except Exception as e:
            print(f"ğŸ“¡ WebSocketé€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")


# ä½¿ç”¨ä¾‹ã¨ãƒ†ã‚¹ãƒˆ
async def test_scrapy_watchdog_monitor():
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ"""

    print("ğŸ¯ scrapy crawl + watchdogç›£è¦–ãƒ†ã‚¹ãƒˆ")

    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ WebSocketé€šçŸ¥: {data}")

    # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    monitor = ScrapyWatchdogMonitor(
        task_id="test_task_123",
        project_path="scrapy_projects/test_project",  # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«å¤‰æ›´
        spider_name="test_spider",  # å®Ÿéš›ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã«å¤‰æ›´
        websocket_callback=websocket_callback
    )

    # å®Ÿè¡Œ
    result = await monitor.execute_spider_with_monitoring(
        settings={
            'LOG_LEVEL': 'INFO',
            'ROBOTSTXT_OBEY': False
        }
    )

    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")


if __name__ == "__main__":
    if WATCHDOG_AVAILABLE:
        asyncio.run(test_scrapy_watchdog_monitor())
    else:
        print("âŒ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„: pip install watchdog")
