#!/usr/bin/env python3
"""
æ‰‹å‹•ã§watchdogç›£è¦–ã‚’é–‹å§‹ã—ã¦ã‹ã‚‰scrapy crawlã‚’å®Ÿè¡Œã™ã‚‹ä¾‹
"""
import asyncio
import subprocess
import sys
from pathlib import Path

# watchdogç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append('backend/app/services')
from scrapy_watchdog_monitor import ScrapyWatchdogMonitor

async def manual_watchdog_scrapy():
    """æ‰‹å‹•ã§watchdogç›£è¦–ä»˜ãscrapy crawlã‚’å®Ÿè¡Œ"""
    
    print("ğŸ¯ æ‰‹å‹•watchdogç›£è¦– + scrapy crawlå®Ÿè¡Œä¾‹")
    
    # è¨­å®š
    task_id = "manual_test_123"
    project_path = "scrapy_projects/test_project"  # å®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã«å¤‰æ›´
    spider_name = "test_spider"  # å®Ÿéš›ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã«å¤‰æ›´
    
    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥: {data}")
    
    # watchdogç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
    monitor = ScrapyWatchdogMonitor(
        task_id=task_id,
        project_path=project_path,
        spider_name=spider_name,
        websocket_callback=websocket_callback
    )
    
    # watchdogç›£è¦–ä»˜ãã§å®Ÿè¡Œ
    result = await monitor.execute_spider_with_monitoring({
        'LOG_LEVEL': 'INFO',
        'ROBOTSTXT_OBEY': False
    })
    
    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")

if __name__ == "__main__":
    asyncio.run(manual_watchdog_scrapy())
