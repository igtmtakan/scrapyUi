"""
ScrapyRealtimeEngine - Scrapyã‚’ç¶™æ‰¿ã—ãŸãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–ã‚¨ãƒ³ã‚¸ãƒ³

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Scrapyã®æ ¸ã¨ãªã‚‹ã‚¯ãƒ©ã‚¹ã‚’ç¶™æ‰¿ã—ã€ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã—ã¦
ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚Scrapyè‡ªä½“ã¯æ”¹å¤‰ã›ãšã€ç¶™æ‰¿ã®ã¿ã§å®Ÿè£…ã€‚

æ©Ÿèƒ½:
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é€²æ—ç›£è¦–
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ç›£è¦–
- WebSocketé€šçŸ¥çµ±åˆ
- è©³ç´°çµ±è¨ˆæƒ…å ±ã®åé›†
- ã‚¨ãƒ©ãƒ¼ãƒ»ä¾‹å¤–ã®å³åº§æ¤œå‡º
"""

import time
from typing import Dict, Any, Optional, Callable
from datetime import datetime, timezone
import asyncio
import threading
import json

from scrapy.crawler import CrawlerProcess, Crawler
from scrapy.core.engine import ExecutionEngine
from scrapy.core.downloader import Downloader
from scrapy.core.scraper import Scraper
from scrapy.http import Request, Response
from scrapy.item import Item
from scrapy.utils.log import configure_logging
from scrapy.statscollectors import StatsCollector
from scrapy.spiders import Spider


class RealtimeStatsCollector(StatsCollector):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆåé›†ã‚¯ãƒ©ã‚¹"""

    def __init__(self, crawler, progress_callback: Optional[Callable] = None):
        super().__init__(crawler)
        self.progress_callback = progress_callback
        self.start_time = datetime.now(timezone.utc)
        self.last_update = self.start_time

    def inc_value(self, key, count=1, start=0):
        """çµ±è¨ˆå€¤ã®å¢—åŠ ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
        super().inc_value(key, count, start)

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
        if self.progress_callback:
            current_stats = self.get_stats()
            self._notify_progress(current_stats)

    def set_value(self, key, value):
        """çµ±è¨ˆå€¤ã®è¨­å®šã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
        super().set_value(key, value)

        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
        if self.progress_callback:
            current_stats = self.get_stats()
            self._notify_progress(current_stats)

    def _notify_progress(self, stats: Dict[str, Any]):
        """é€²æ—é€šçŸ¥ã‚’é€ä¿¡"""
        try:
            # pending itemæ•°ã‚’è¨ˆç®—
            pending_items = self._calculate_pending_items(stats)

            progress_data = {
                'timestamp': datetime.now(timezone.utc).isoformat(),
                'items_count': stats.get('item_scraped_count', 0),
                'requests_count': stats.get('downloader/request_count', 0),
                'responses_count': stats.get('downloader/response_count', 0),
                'errors_count': stats.get('downloader/exception_count', 0) + stats.get('spider_exceptions', 0),
                'bytes_downloaded': stats.get('downloader/response_bytes', 0),
                'pending_items': pending_items,  # pending itemæ•°ã‚’è¿½åŠ 
                'elapsed_time': (datetime.now(timezone.utc) - self.start_time).total_seconds(),
                'items_per_minute': self._calculate_items_per_minute(stats),
                'requests_per_minute': self._calculate_requests_per_minute(stats),
                'progress_percentage': self._calculate_progress_percentage(stats, pending_items)
            }

            # éåŒæœŸã§ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å®Ÿè¡Œ
            if hasattr(self.progress_callback, '__call__'):
                self.progress_callback(progress_data)

        except Exception as e:
            print(f"Error in progress notification: {e}")

    def _calculate_items_per_minute(self, stats: Dict[str, Any]) -> float:
        """ã‚¢ã‚¤ãƒ†ãƒ /åˆ†ã‚’è¨ˆç®—"""
        items = stats.get('item_scraped_count', 0)
        elapsed = (datetime.now(timezone.utc) - self.start_time).total_seconds() / 60
        return items / elapsed if elapsed > 0 else 0

    def _calculate_requests_per_minute(self, stats: Dict[str, Any]) -> float:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ã‚’è¨ˆç®—"""
        requests = stats.get('downloader/request_count', 0)
        elapsed = (datetime.now(timezone.utc) - self.start_time).total_seconds() / 60
        return requests / elapsed if elapsed > 0 else 0

    def _calculate_pending_items(self, stats: Dict[str, Any]) -> int:
        """pending itemæ•°ã‚’è¨ˆç®—"""
        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°ã‚’å¼•ã„ã¦pending itemæ•°ã‚’æ¨å®š
        requests = stats.get('downloader/request_count', 0)
        responses = stats.get('downloader/response_count', 0)
        pending = max(0, requests - responses)
        return pending

    def _calculate_progress_percentage(self, stats: Dict[str, Any], pending_items: int) -> float:
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        items_scraped = stats.get('item_scraped_count', 0)
        total_estimated = items_scraped + pending_items

        if total_estimated > 0:
            return (items_scraped / total_estimated) * 100.0
        return 0.0


class RealtimeDownloader(Downloader):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, crawler, progress_callback: Optional[Callable] = None):
        super().__init__(crawler)
        self.progress_callback = progress_callback
        self.download_count = 0

    def download(self, request, spider):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
        self.download_count += 1

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹é€šçŸ¥
        if self.progress_callback:
            self._notify_download_start(request)

        # å…ƒã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å‡¦ç†ã‚’å®Ÿè¡Œ
        deferred = super().download(request, spider)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ 
        deferred.addCallback(self._on_download_complete, request)
        deferred.addErrback(self._on_download_error, request)

        return deferred

    def _notify_download_start(self, request: Request):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰é–‹å§‹é€šçŸ¥"""
        try:
            download_data = {
                'type': 'download_start',
                'url': request.url,
                'method': request.method,
                'download_count': self.download_count,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            if hasattr(self.progress_callback, '__call__'):
                self.progress_callback(download_data)

        except Exception as e:
            print(f"Error in download start notification: {e}")

    def _on_download_complete(self, response: Response, request: Request):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†æ™‚ã®å‡¦ç†"""
        try:
            if self.progress_callback:
                download_data = {
                    'type': 'download_complete',
                    'url': response.url,
                    'status': response.status,
                    'size': len(response.body),
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }

                if hasattr(self.progress_callback, '__call__'):
                    self.progress_callback(download_data)

        except Exception as e:
            print(f"Error in download complete notification: {e}")

        return response

    def _on_download_error(self, failure, request: Request):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼æ™‚ã®å‡¦ç†"""
        try:
            if self.progress_callback:
                error_data = {
                    'type': 'download_error',
                    'url': request.url,
                    'error': str(failure.value),
                    'timestamp': datetime.now(timezone.utc).isoformat()
                }

                if hasattr(self.progress_callback, '__call__'):
                    self.progress_callback(error_data)

        except Exception as e:
            print(f"Error in download error notification: {e}")

        return failure


class RealtimeScraper(Scraper):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, crawler, progress_callback: Optional[Callable] = None):
        super().__init__(crawler)
        self.progress_callback = progress_callback
        self.item_count = 0

    def _itemproc_finished(self, output, item, response, spider):
        """ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†å®Œäº†æ™‚ã®å‡¦ç†ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰"""
        self.item_count += 1

        # ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†é€šçŸ¥
        if self.progress_callback:
            self._notify_item_processed(item, response)

        # å…ƒã®å‡¦ç†ã‚’å®Ÿè¡Œ
        return super()._itemproc_finished(output, item, response, spider)

    def _notify_item_processed(self, item: Item, response: Response):
        """ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†é€šçŸ¥"""
        try:
            item_data = {
                'type': 'item_processed',
                'item_count': self.item_count,
                'url': response.url,
                'item_fields': len(dict(item)) if hasattr(item, '__iter__') else 0,
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            if hasattr(self.progress_callback, '__call__'):
                self.progress_callback(item_data)

        except Exception as e:
            print(f"Error in item processed notification: {e}")


class RealtimeSpider(Spider):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–æ©Ÿèƒ½ä»˜ãSpiderã‚¯ãƒ©ã‚¹"""

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.pending_items = []  # pending itemsã®ãƒªã‚¹ãƒˆ
        self.processed_items = 0  # å‡¦ç†æ¸ˆã¿itemæ•°
        self.total_items_target = kwargs.get('target_items', 100)  # ç›®æ¨™ã‚¢ã‚¤ãƒ†ãƒ æ•°

    def add_pending_item(self, item_data):
        """pending itemã‚’è¿½åŠ """
        self.pending_items.append({
            'data': item_data,
            'timestamp': datetime.now(timezone.utc),
            'status': 'pending'
        })

    def process_pending_item(self, index=0):
        """pending itemã‚’å‡¦ç†"""
        if index < len(self.pending_items):
            item = self.pending_items[index]
            item['status'] = 'processed'
            item['processed_timestamp'] = datetime.now(timezone.utc)
            self.processed_items += 1
            return item
        return None

    def get_pending_count(self):
        """pending itemæ•°ã‚’å–å¾—"""
        return len([item for item in self.pending_items if item['status'] == 'pending'])

    def get_progress_percentage(self):
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        if self.total_items_target > 0:
            return (self.processed_items / self.total_items_target) * 100.0
        return 0.0

    def get_progress_stats(self):
        """é€²æ—çµ±è¨ˆã‚’å–å¾—"""
        return {
            'pending_items': self.get_pending_count(),
            'processed_items': self.processed_items,
            'total_target': self.total_items_target,
            'progress_percentage': self.get_progress_percentage(),
            'pending_ratio': self.get_pending_count() / max(1, self.total_items_target)
        }


class RealtimeExecutionEngine(ExecutionEngine):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³ã‚¯ãƒ©ã‚¹"""

    def __init__(self, crawler, spider_closed_callback, progress_callback: Optional[Callable] = None):
        super().__init__(crawler, spider_closed_callback)
        self.progress_callback = progress_callback
        self.pending_requests = set()  # pending requestsã®ã‚»ãƒƒãƒˆ
        self.pending_items_queue = []  # pending itemsã®ã‚­ãƒ¥ãƒ¼

    def crawl(self, request, spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        # pending requestsã«è¿½åŠ 
        self.pending_requests.add(id(request))

        # å…ƒã®å‡¦ç†ã‚’å®Ÿè¡Œ
        result = super().crawl(request, spider)

        # é€²æ—é€šçŸ¥
        if self.progress_callback:
            self._notify_pending_update()

        return result

    def _notify_pending_update(self):
        """pendingçŠ¶æ…‹ã®æ›´æ–°é€šçŸ¥"""
        try:
            pending_data = {
                'type': 'pending_update',
                'pending_requests': len(self.pending_requests),
                'pending_items': len(self.pending_items_queue),
                'timestamp': datetime.now(timezone.utc).isoformat()
            }

            if hasattr(self.progress_callback, '__call__'):
                self.progress_callback(pending_data)

        except Exception as e:
            print(f"Error in pending update notification: {e}")

    def _handle_downloader_output(self, response, request, spider):
        """ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼å‡ºåŠ›å‡¦ç†ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰"""
        # pending requestsã‹ã‚‰å‰Šé™¤
        self.pending_requests.discard(id(request))

        # å…ƒã®å‡¦ç†ã‚’å®Ÿè¡Œ
        result = super()._handle_downloader_output(response, request, spider)

        # é€²æ—é€šçŸ¥
        if self.progress_callback:
            self._notify_pending_update()

        return result

    def _get_downloader(self, crawler):
        """ã‚«ã‚¹ã‚¿ãƒ ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’è¿”ã™"""
        return RealtimeDownloader(crawler, self.progress_callback)

    def _get_scraper(self, crawler):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’è¿”ã™"""
        return RealtimeScraper(crawler, self.progress_callback)


class RealtimeCrawler(Crawler):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚¯ãƒ©ã‚¹"""

    def __init__(self, spidercls, settings=None, progress_callback: Optional[Callable] = None):
        super().__init__(spidercls, settings)
        self.progress_callback = progress_callback

    def _create_engine(self):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’ä½œæˆ"""
        return RealtimeExecutionEngine(self, self._spider_closed, self.progress_callback)

    def _create_stats(self):
        """ã‚«ã‚¹ã‚¿ãƒ çµ±è¨ˆåé›†å™¨ã‚’ä½œæˆ"""
        return RealtimeStatsCollector(self, self.progress_callback)


class RealtimeCrawlerProcess(CrawlerProcess):
    """ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹ã‚¯ãƒ©ã‚¹"""

    def __init__(self, settings=None, install_root_handler=True, progress_callback: Optional[Callable] = None):
        super().__init__(settings, install_root_handler)
        self.progress_callback = progress_callback

    def _create_crawler(self, spidercls):
        """ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’ä½œæˆ"""
        if isinstance(spidercls, str):
            spidercls = self.spider_loader.load(spidercls)
        return RealtimeCrawler(spidercls, self.settings, self.progress_callback)


class ScrapyRealtimeEngine:
    """
    Scrapyãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å®Ÿè¡Œã‚¨ãƒ³ã‚¸ãƒ³

    Scrapyã‚’ç¶™æ‰¿ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€²æ—ç›£è¦–æ©Ÿèƒ½ã‚’è¿½åŠ ã—ãŸãƒ¡ã‚¤ãƒ³ã‚¯ãƒ©ã‚¹
    """

    def __init__(self, progress_callback: Optional[Callable] = None, websocket_callback: Optional[Callable] = None):
        self.progress_callback = progress_callback
        self.websocket_callback = websocket_callback
        self.process = None
        self.stats = {}

    def run_spider(self, spider_name: str, project_path: str, settings: Dict[str, Any] = None) -> Dict[str, Any]:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ä»˜ãã§å®Ÿè¡Œ"""
        try:
            import os
            import sys
            import importlib.util

            # çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
            project_path = os.path.abspath(project_path)

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ã‚’ç¢ºèªã—ã¦é©åˆ‡ãªãƒ‘ã‚¹ã‚’è¨­å®š
            # admin_aiueo/admin_aiueo/spiders ã®æ§‹é€ ã«å¯¾å¿œ
            project_name = os.path.basename(project_path)
            inner_project_path = os.path.join(project_path, project_name)
            spiders_path = os.path.join(inner_project_path, 'spiders')

            # spidersãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ç›´æ¥ã®spidersãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèª
            if not os.path.exists(spiders_path):
                spiders_path = os.path.join(project_path, 'spiders')

            print(f"ğŸ” Project path: {project_path}")
            print(f"ğŸ” Inner project path: {inner_project_path}")
            print(f"ğŸ” Spiders path: {spiders_path}")
            print(f"ğŸ” Spiders directory exists: {os.path.exists(spiders_path)}")

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
            if project_path not in sys.path:
                sys.path.insert(0, project_path)
            if inner_project_path not in sys.path:
                sys.path.insert(0, inner_project_path)
            if spiders_path not in sys.path:
                sys.path.insert(0, spiders_path)

            # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ï¼ˆå†…éƒ¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ï¼‰
            original_cwd = os.getcwd()
            work_dir = inner_project_path if os.path.exists(inner_project_path) else project_path
            os.chdir(work_dir)
            print(f"ğŸ” Working directory: {work_dir}")

            try:
                # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                spider_file = os.path.join(spiders_path, f'{spider_name}.py')
                print(f"ğŸ” Spider file: {spider_file}")
                print(f"ğŸ” Spider file exists: {os.path.exists(spider_file)}")

                # è¨­å®šã‚’æº–å‚™
                custom_settings = {
                    'LOG_LEVEL': 'INFO',
                    'LOGSTATS_INTERVAL': 1,  # 1ç§’é–“éš”ã§çµ±è¨ˆå‡ºåŠ›
                    'ROBOTSTXT_OBEY': False,
                    'BOT_NAME': 'scrapybot',
                    'SPIDER_MODULES': ['spiders'],
                    'NEWSPIDER_MODULE': 'spiders',
                    'USER_AGENT': 'ScrapyUI (+http://www.scrapyui.com)',
                    # Playwrightè¨­å®šï¼ˆæœ‰åŠ¹åŒ–ï¼‰
                    'DOWNLOAD_HANDLERS': {
                        "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                        "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",
                    },
                    # Reactorè¨­å®šã¯åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§å‡¦ç†ã™ã‚‹ãŸã‚å‰Šé™¤
                    'PLAYWRIGHT_BROWSER_TYPE': 'chromium',
                    'PLAYWRIGHT_LAUNCH_OPTIONS': {
                        'headless': True,
                        'args': ['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
                    },
                    'PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT': 30000,
                    'PLAYWRIGHT_PROCESS_REQUEST_HEADERS': None,
                    # HTTPè¨­å®š
                    'DOWNLOAD_DELAY': 2,
                    'RANDOMIZE_DOWNLOAD_DELAY': True,
                    'AUTOTHROTTLE_ENABLED': True,
                    'AUTOTHROTTLE_START_DELAY': 1,
                    'AUTOTHROTTLE_MAX_DELAY': 10,
                    'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
                    # çµæœå‡ºåŠ›è¨­å®š
                    'FEEDS': {
                        f'results/{spider_name}_results.json': {
                            'format': 'json',
                            'encoding': 'utf8',
                            'store_empty': False,
                            'indent': 2
                        }
                    },
                    # ã‚¢ã‚¤ãƒ†ãƒ åˆ¶é™è¨­å®šã‚’å‰Šé™¤ï¼ˆè‡ªç„¶ãªçµ‚äº†ã‚’å¾…ã¤ï¼‰
                    'DOWNLOAD_DELAY': 2,
                    'CONCURRENT_REQUESTS': 1
                }

                # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ãƒãƒ¼ã‚¸
                if settings:
                    custom_settings.update(settings)

                # ãƒ­ã‚°è¨­å®š
                configure_logging({'LOG_LEVEL': 'DEBUG'})

                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ—ãƒ­ã‚»ã‚¹ã‚’ä½œæˆ
                self.process = RealtimeCrawlerProcess(
                    settings=custom_settings,
                    progress_callback=self._on_progress_update
                )

                # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’åˆæœŸåŒ–
                print(f"ğŸ” Available spiders: {list(self.process.spider_loader.list())}")

                # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
                try:
                    self.process.crawl(spider_name)
                    print(f"âœ… Spider {spider_name} added to crawler")
                    self.process.start()
                    print(f"âœ… Crawler process started")
                except Exception as crawl_error:
                    print(f"âŒ Error during crawl: {crawl_error}")
                    raise

                # çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
                final_stats = self.stats_collector.get_stats() if hasattr(self, 'stats_collector') else {}
                items_count = final_stats.get('item_scraped_count', 0)
                requests_count = final_stats.get('downloader/request_count', 0)
                errors_count = final_stats.get('spider_exceptions', 0)

                print(f"ğŸ“Š Final statistics: items={items_count}, requests={requests_count}, errors={errors_count}")

                return {
                    'success': True,
                    'stats': self.stats,
                    'items_count': items_count,
                    'requests_count': requests_count,
                    'errors_count': errors_count
                }

            finally:
                # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¾©å…ƒ
                os.chdir(original_cwd)

        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

    def _on_progress_update(self, progress_data: Dict[str, Any]):
        """é€²æ—æ›´æ–°æ™‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            # çµ±è¨ˆã‚’æ›´æ–°
            self.stats.update(progress_data)

            # å¤–éƒ¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’å‘¼ã³å‡ºã—
            if self.progress_callback:
                self.progress_callback(progress_data)

            # WebSocketé€šçŸ¥
            if self.websocket_callback:
                self.websocket_callback(progress_data)

            # ãƒ‡ãƒãƒƒã‚°å‡ºåŠ›
            if progress_data.get('type') == 'item_processed':
                print(f"ğŸ“¦ Item {progress_data.get('item_count', 0)} processed from {progress_data.get('url', 'unknown')}")
            elif progress_data.get('type') == 'download_complete':
                print(f"â¬‡ï¸ Downloaded {progress_data.get('url', 'unknown')} ({progress_data.get('size', 0)} bytes)")

        except Exception as e:
            print(f"Error in progress update: {e}")
