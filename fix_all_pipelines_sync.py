#!/usr/bin/env python3
"""
å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’å¼·åˆ¶çš„ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ
"""
import sqlite3
import uuid
from pathlib import Path
from datetime import datetime

def fix_all_pipelines_sync():
    """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’å¼·åˆ¶åŒæœŸ"""
    
    print("ğŸ”„ å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyå¼·åˆ¶åŒæœŸé–‹å§‹\n")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db_path = Path("backend/database/scrapy_ui.db")
    
    if not db_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return False
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        cursor.execute("SELECT id, name, path, user_id, db_save_enabled FROM projects")
        projects = cursor.fetchall()
        
        print(f"ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {len(projects)}ä»¶")
        
        success_count = 0
        error_count = 0
        
        for project_id, project_name, project_path, user_id, db_save_enabled in projects:
            print(f"\nğŸ”„ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåŒæœŸä¸­: {project_name}")
            print(f"   ID: {project_id}")
            print(f"   ãƒ‘ã‚¹: {project_path}")
            print(f"   DBä¿å­˜è¨­å®š: {'æœ‰åŠ¹' if db_save_enabled else 'ç„¡åŠ¹'}")
            
            try:
                success = force_sync_single_project(cursor, project_id, project_name, project_path, user_id, db_save_enabled)
                if success:
                    success_count += 1
                    print(f"   âœ… åŒæœŸæˆåŠŸ")
                else:
                    error_count += 1
                    print(f"   âŒ åŒæœŸå¤±æ•—")
            except Exception as e:
                error_count += 1
                print(f"   âŒ åŒæœŸã‚¨ãƒ©ãƒ¼: {e}")
        
        conn.commit()
        
        print(f"\nğŸ‰ åŒæœŸå®Œäº†!")
        print(f"   æˆåŠŸ: {success_count}ä»¶")
        print(f"   å¤±æ•—: {error_count}ä»¶")
        print(f"   åˆè¨ˆ: {len(projects)}ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    finally:
        if 'conn' in locals():
            conn.close()

def force_sync_single_project(cursor, project_id: str, project_name: str, project_path: str, user_id: str, db_save_enabled: bool):
    """å˜ä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’å¼·åˆ¶åŒæœŸ"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰pipelines.pyã‚’æ¢ã™
    scrapy_projects_dir = Path("scrapy_projects")
    
    possible_paths = [
        scrapy_projects_dir / project_path / project_path / "pipelines.py",
        scrapy_projects_dir / project_path / "pipelines.py",
    ]
    
    actual_pipelines_file = None
    for path in possible_paths:
        if path.exists():
            actual_pipelines_file = path
            print(f"     ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {path}")
            break
    
    if not actual_pipelines_file:
        print(f"     âŒ pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return False
    
    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’èª­ã¿å–ã‚Š
        with open(actual_pipelines_file, 'r', encoding='utf-8') as f:
            pipelines_content = f.read()
        
        print(f"     ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(pipelines_content)}æ–‡å­—")
        
        # å†…å®¹ã‚’æ¤œè¨¼
        has_scrapy_ui_pipeline = 'ScrapyUIDatabasePipeline' in pipelines_content
        expected_has_pipeline = db_save_enabled
        
        print(f"     ğŸ” ScrapyUIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³: {'ã‚ã‚Š' if has_scrapy_ui_pipeline else 'ãªã—'}")
        print(f"     ğŸ” æœŸå¾…å€¤: {'ã‚ã‚Š' if expected_has_pipeline else 'ãªã—'}")
        
        if has_scrapy_ui_pipeline == expected_has_pipeline:
            print(f"     âœ… å†…å®¹ãŒè¨­å®šã¨ä¸€è‡´")
        else:
            print(f"     âš ï¸ å†…å®¹ãŒè¨­å®šã¨ä¸ä¸€è‡´")
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ—¢å­˜ã®pipelines.pyã‚’å‰Šé™¤
        cursor.execute("""
            DELETE FROM project_files 
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))
        
        deleted_count = cursor.rowcount
        if deleted_count > 0:
            print(f"     ğŸ—‘ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_count}ä»¶")
        
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        file_id = str(uuid.uuid4())
        cursor.execute("""
            INSERT INTO project_files 
            (id, name, path, content, file_type, project_id, user_id, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            file_id,
            "pipelines.py",
            "pipelines.py",
            pipelines_content,
            "python",
            project_id,
            user_id,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))
        
        print(f"     â• æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ ")
        
        # æ¤œè¨¼
        cursor.execute("""
            SELECT content FROM project_files 
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))
        
        result = cursor.fetchone()
        if result:
            db_content = result[0]
            db_has_scrapy_ui = 'ScrapyUIDatabasePipeline' in db_content
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œè¨¼: ScrapyUIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³={'ã‚ã‚Š' if db_has_scrapy_ui else 'ãªã—'}")
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µã‚¤ã‚º: {len(db_content)}æ–‡å­—")
            
            return db_has_scrapy_ui == expected_has_pipeline
        else:
            print(f"     âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œè¨¼å¤±æ•—")
            return False
        
    except Exception as e:
        print(f"     âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def verify_sync_results():
    """åŒæœŸçµæœã‚’æ¤œè¨¼"""
    
    print("\nğŸ” åŒæœŸçµæœæ¤œè¨¼")
    
    db_path = Path("backend/database/scrapy_ui.db")
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆ
        cursor.execute("""
            SELECT 
                p.name as project_name,
                p.db_save_enabled,
                CASE WHEN pf.content LIKE '%ScrapyUIDatabasePipeline%' THEN 1 ELSE 0 END as has_scrapy_ui,
                length(pf.content) as content_length
            FROM projects p
            LEFT JOIN project_files pf ON p.id = pf.project_id AND pf.name = 'pipelines.py'
            ORDER BY p.name
        """)
        
        results = cursor.fetchall()
        
        print(f"\nğŸ“Š pipelines.pyåŒæœŸçµæœ:")
        print(f"{'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå':<30} {'DBä¿å­˜':<8} {'ScrapyUI':<10} {'ã‚µã‚¤ã‚º':<8} {'çŠ¶æ…‹':<6}")
        print("-" * 70)
        
        correct_count = 0
        total_count = 0
        
        for project_name, db_save_enabled, has_scrapy_ui, content_length in results:
            db_setting = "æœ‰åŠ¹" if db_save_enabled else "ç„¡åŠ¹"
            scrapy_ui_status = "ã‚ã‚Š" if has_scrapy_ui else "ãªã—"
            size_str = f"{content_length}æ–‡å­—" if content_length else "ãªã—"
            
            # æ­£ã—ã„çŠ¶æ…‹ã‹ãƒã‚§ãƒƒã‚¯
            is_correct = (db_save_enabled and has_scrapy_ui) or (not db_save_enabled and not has_scrapy_ui)
            status = "âœ…" if is_correct else "âŒ"
            
            if is_correct:
                correct_count += 1
            total_count += 1
            
            print(f"{project_name:<30} {db_setting:<8} {scrapy_ui_status:<10} {size_str:<8} {status:<6}")
        
        print("-" * 70)
        print(f"æ­£ã—ã„çŠ¶æ…‹: {correct_count}/{total_count}ä»¶ ({correct_count/total_count*100:.1f}%)")
        
        conn.close()
        
        return correct_count == total_count
        
    except Exception as e:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyå¼·åˆ¶åŒæœŸãƒ„ãƒ¼ãƒ«")
    
    # å¼·åˆ¶åŒæœŸå®Ÿè¡Œ
    success = fix_all_pipelines_sync()
    
    if success:
        # çµæœæ¤œè¨¼
        all_correct = verify_sync_results()
        
        print("\nğŸ‰ å¼·åˆ¶åŒæœŸå®Œäº†ï¼")
        
        if all_correct:
            print("\nâœ… å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒæ­£ã—ã„çŠ¶æ…‹ã§ã™")
        else:
            print("\nâš ï¸ ä¸€éƒ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        
        print("\nğŸ”§ å®Ÿè¡Œå†…å®¹:")
        print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰pipelines.pyã‚’èª­ã¿å–ã‚Š")
        print("  âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤")
        print("  âœ… æ­£ã—ã„å†…å®¹ã§ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–°è¦ä½œæˆ")
        print("  âœ… DBä¿å­˜è¨­å®šã¨ã®ä¸€è‡´ã‚’æ¤œè¨¼")
        
        print("\nğŸŒ WebUIç¢ºèª:")
        print("  1. http://localhost:4000 ã«ã‚¢ã‚¯ã‚»ã‚¹")
        print("  2. ä»»æ„ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ")
        print("  3. pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã")
        print("  4. DBä¿å­˜è¨­å®šã«å¿œã˜ãŸæ­£ã—ã„å†…å®¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª")
        
        print("\nğŸ“ æœŸå¾…ã•ã‚Œã‚‹çµæœ:")
        print("  - DBä¿å­˜æœ‰åŠ¹: ScrapyUIDatabasePipelineã€ScrapyUIJSONPipelineãŒå«ã¾ã‚Œã‚‹")
        print("  - DBä¿å­˜ç„¡åŠ¹: åŸºæœ¬çš„ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®ã¿ãŒå«ã¾ã‚Œã‚‹")
    else:
        print("\nâŒ å¼·åˆ¶åŒæœŸå¤±æ•—")

if __name__ == "__main__":
    main()
