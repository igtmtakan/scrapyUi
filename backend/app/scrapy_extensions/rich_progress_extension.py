"""
ScrapyUI Rich Progress Extension

Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«richãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ãŸç¾ã—ã„é€²æ—ãƒãƒ¼ã‚’è¿½åŠ ã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½
"""

import os
import sys
import json
import time
from pathlib import Path
from typing import Optional, Dict, Any
from scrapy import signals
from scrapy.crawler import Crawler
from scrapy.spiders import Spider
from scrapy.http import Request, Response
from scrapy.exceptions import NotConfigured

try:
    from rich.progress import Progress, TaskID, BarColumn, TextColumn, TimeRemainingColumn, SpinnerColumn
    from rich.console import Console
    from rich.live import Live
    from rich.table import Table
    from rich.panel import Panel
    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False


class RichProgressExtension:
    """
    Scrapyã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ç”¨Riché€²æ—ãƒãƒ¼æ‹¡å¼µæ©Ÿèƒ½
    
    Features:
    - ç¾ã—ã„é€²æ—ãƒãƒ¼è¡¨ç¤º
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ çµ±è¨ˆæƒ…å ±
    - ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªè¡¨ç¤ºå½¢å¼
    - WebSocketçµŒç”±ã§ã®é€²æ—é€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    
    def __init__(self, crawler: Crawler):
        if not RICH_AVAILABLE:
            raise NotConfigured("Rich library is not installed. Run: pip install rich")

        self.crawler = crawler
        self.settings = crawler.settings

        # Riché€²æ—ãƒãƒ¼è¨­å®š
        self.console = Console()
        self.progress = None
        self.live = None
        self.task_id: Optional[TaskID] = None

        # çµ±è¨ˆæƒ…å ±
        self.stats = {
            'requests_count': 0,
            'responses_count': 0,
            'items_count': 0,
            'errors_count': 0,
            'start_time': None,
            'finish_time': None,
            'total_urls': 0
        }

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        self.stats_file = None
        self.task_id_str = None

        # è¨­å®š
        self.enabled = self.settings.getbool('RICH_PROGRESS_ENABLED', True)
        self.show_stats = self.settings.getbool('RICH_PROGRESS_SHOW_STATS', True)
        self.update_interval = self.settings.getfloat('RICH_PROGRESS_UPDATE_INTERVAL', 0.1)
        self.websocket_enabled = self.settings.getbool('RICH_PROGRESS_WEBSOCKET', False)

        if not self.enabled:
            raise NotConfigured("Rich progress bar is disabled")
    
    @classmethod
    def from_crawler(cls, crawler: Crawler):
        """Crawlerã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
        extension = cls(crawler)
        
        # ã‚·ã‚°ãƒŠãƒ«ã‚’æ¥ç¶š
        crawler.signals.connect(extension.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(extension.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(extension.request_scheduled, signal=signals.request_scheduled)
        crawler.signals.connect(extension.response_received, signal=signals.response_received)
        crawler.signals.connect(extension.item_scraped, signal=signals.item_scraped)
        crawler.signals.connect(extension.spider_error, signal=signals.spider_error)
        
        return extension
    
    def spider_opened(self, spider: Spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã®å‡¦ç†"""
        self.stats['start_time'] = time.time()

        # ã‚¿ã‚¹ã‚¯IDã‚’å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã¾ãŸã¯crawlerã‹ã‚‰ï¼‰
        self.task_id_str = (
            os.environ.get('SCRAPY_TASK_ID') or
            getattr(self.crawler, 'task_id', None) or
            f"task_{int(time.time())}"
        )

        # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
        project_dir = Path.cwd()
        self.stats_file = project_dir / f"stats_{self.task_id_str}.json"

        # start_urlsã®æ•°ã‚’å–å¾—
        if hasattr(spider, 'start_urls'):
            self.stats['total_urls'] = len(spider.start_urls)

        # åˆæœŸçµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        self._save_stats()

        # Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–
        self._initialize_progress(spider)

        spider.logger.info(f"ğŸ¨ Riché€²æ—ãƒãƒ¼é–‹å§‹: {spider.name}")
        spider.logger.info(f"ğŸ“Š çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«: {self.stats_file}")
    
    def spider_closed(self, spider: Spider, reason: str):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã®å‡¦ç†"""
        # çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        self.stats['finish_time'] = time.time()

        # Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸ
        self._sync_with_scrapy_stats()

        # æœ€çµ‚çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
        self._save_stats()

        # å®Œäº†é€šçŸ¥ï¼ˆwatchdogã¨å…±å­˜ï¼‰
        if reason == 'finished' and hasattr(spider, 'task_id'):
            spider.logger.info(f"ğŸ¯ Spider completed successfully with Rich progress tracking for task {spider.task_id}")

            # watchdogãŒæ—¢ã«DBä¿å­˜ã‚’è¡Œã£ã¦ã„ã‚‹ãŸã‚ã€Rich progressã§ã¯è¿½åŠ ã®DBä¿å­˜ã¯è¡Œã‚ãªã„
            spider.logger.info(f"ğŸ“Š Rich progress tracking completed - watchdog handles DB operations")

        if self.live:
            self.live.stop()

        if self.progress:
            self.progress.stop()

        # æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º
        self._show_final_stats(spider, reason)

        spider.logger.info(f"ğŸ¨ Riché€²æ—ãƒãƒ¼çµ‚äº†: {spider.name} (ç†ç”±: {reason})")
        spider.logger.info(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {self.stats_file}")
    
    def request_scheduled(self, request: Request, spider: Spider):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé€ä¿¡æ™‚ã®å‡¦ç†"""
        self.stats['requests_count'] += 1
        self._update_progress()
        self._save_stats()

    def response_received(self, response: Response, request: Request, spider: Spider):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹å—ä¿¡æ™‚ã®å‡¦ç†"""
        self.stats['responses_count'] += 1
        self._update_progress()
        self._save_stats()

    def item_scraped(self, item: Dict[str, Any], response: Response, spider: Spider):
        """ã‚¢ã‚¤ãƒ†ãƒ å–å¾—æ™‚ã®å‡¦ç†"""
        self.stats['items_count'] += 1
        self._update_progress()
        self._save_stats()

    def spider_error(self, failure, response: Response, spider: Spider):
        """ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã®å‡¦ç†"""
        self.stats['errors_count'] += 1
        self._update_progress()
        self._save_stats()
    
    def _initialize_progress(self, spider: Spider):
        """Riché€²æ—ãƒãƒ¼ã‚’åˆæœŸåŒ–"""
        # ã‚«ã‚¹ã‚¿ãƒ é€²æ—ãƒãƒ¼ã‚«ãƒ©ãƒ 
        columns = [
            SpinnerColumn(),
            TextColumn("[bold blue]{task.description}"),
            BarColumn(bar_width=40),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            TextColumn("â€¢"),
            TextColumn("[bold green]{task.completed}/{task.total}"),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
        ]
        
        self.progress = Progress(*columns, console=self.console)
        
        # ã‚¿ã‚¹ã‚¯ã‚’è¿½åŠ 
        total = max(self.stats['total_urls'], 1)  # æœ€ä½1ã«è¨­å®š
        self.task_id = self.progress.add_task(
            f"ğŸ•·ï¸ {spider.name}",
            total=total
        )
        
        if self.show_stats:
            # ãƒ©ã‚¤ãƒ–è¡¨ç¤ºã§ãƒ†ãƒ¼ãƒ–ãƒ«ã¨é€²æ—ãƒãƒ¼ã‚’çµ„ã¿åˆã‚ã›
            self.live = Live(self._create_layout(), console=self.console, refresh_per_second=10)
            self.live.start()
        else:
            self.progress.start()
    
    def _create_layout(self):
        """è¡¨ç¤ºãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’ä½œæˆ"""
        # çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ«
        stats_table = Table(title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°çµ±è¨ˆ", show_header=True, header_style="bold magenta")
        stats_table.add_column("é …ç›®", style="cyan", width=15)
        stats_table.add_column("å€¤", style="green", width=10)
        
        # çµŒéæ™‚é–“ã‚’è¨ˆç®—
        elapsed_time = 0
        if self.stats['start_time']:
            import time
            elapsed_time = time.time() - self.stats['start_time']
        
        # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
        stats_table.add_row("ğŸ“¤ ãƒªã‚¯ã‚¨ã‚¹ãƒˆ", str(self.stats['requests_count']))
        stats_table.add_row("ğŸ“¥ ãƒ¬ã‚¹ãƒãƒ³ã‚¹", str(self.stats['responses_count']))
        stats_table.add_row("ğŸ“¦ ã‚¢ã‚¤ãƒ†ãƒ ", str(self.stats['items_count']))
        stats_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼", str(self.stats['errors_count']))
        stats_table.add_row("â±ï¸ çµŒéæ™‚é–“", f"{elapsed_time:.1f}ç§’")
        
        # é€Ÿåº¦è¨ˆç®—
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            stats_table.add_row("ğŸš€ å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’çµ„ã¿åˆã‚ã›
        layout = Table.grid()
        layout.add_row(Panel(self.progress, title="ğŸ¯ é€²æ—çŠ¶æ³", border_style="blue"))
        layout.add_row(Panel(stats_table, title="ğŸ“ˆ è©³ç´°çµ±è¨ˆ", border_style="green"))
        
        return layout
    
    def _update_progress(self):
        """é€²æ—ãƒãƒ¼ã‚’æ›´æ–°"""
        if not self.progress or not self.task_id:
            return
        
        # é€²æ—ã‚’æ›´æ–°ï¼ˆã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ï¼‰
        completed = self.stats['items_count']
        
        # å‹•çš„ã«ç·æ•°ã‚’èª¿æ•´ï¼ˆãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ãŒåˆæœŸäºˆæƒ³ã‚’è¶…ãˆãŸå ´åˆï¼‰
        if self.stats['requests_count'] > self.stats['total_urls']:
            total = max(self.stats['requests_count'], self.stats['total_urls'])
            self.progress.update(self.task_id, total=total)
        
        self.progress.update(self.task_id, completed=completed)
        
        # WebSocketé€šçŸ¥ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        if self.websocket_enabled:
            self._send_websocket_update()
    
    def _send_websocket_update(self):
        """WebSocketçµŒç”±ã§é€²æ—ã‚’é€šçŸ¥"""
        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                import time
                elapsed_time = time.time() - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time

            # WebSocketé€šçŸ¥ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆ
            progress_data = {
                'taskId': getattr(self.crawler, 'task_id', 'unknown'),
                'status': 'running',
                'itemsScraped': self.stats['items_count'],
                'requestsCount': self.stats['requests_count'],
                'errorCount': self.stats['errors_count'],
                'elapsedTime': int(elapsed_time),
                'progressPercentage': self._calculate_progress_percentage(),
                'itemsPerSecond': round(items_per_second, 2),
                'requestsPerSecond': round(requests_per_second, 2),
                'totalPages': self.stats['total_urls'],
                'currentPage': self.stats['responses_count']
            }

            # ScrapyUIã®WebSocketãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã«é€ä¿¡
            try:
                import asyncio
                from ..api.websocket_progress import broadcast_rich_progress_update

                # éåŒæœŸã§WebSocketé€ä¿¡
                loop = asyncio.get_event_loop()
                if loop.is_running():
                    asyncio.create_task(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
                else:
                    loop.run_until_complete(
                        broadcast_rich_progress_update(
                            progress_data['taskId'],
                            progress_data
                        )
                    )
            except Exception as ws_error:
                # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
                pass

        except Exception as e:
            # WebSocketé€ä¿¡ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass
    
    def _calculate_progress_percentage(self) -> float:
        """é€²æ—ç‡ã‚’è¨ˆç®—"""
        if self.stats['total_urls'] == 0:
            return 0.0
        
        # ã‚¢ã‚¤ãƒ†ãƒ æ•°ãƒ™ãƒ¼ã‚¹ã§è¨ˆç®—
        return min(100.0, (self.stats['items_count'] / self.stats['total_urls']) * 100)
    
    def _show_final_stats(self, spider: Spider, reason: str):
        """æœ€çµ‚çµ±è¨ˆã‚’è¡¨ç¤º"""
        import time
        
        elapsed_time = 0
        if self.stats['start_time']:
            elapsed_time = time.time() - self.stats['start_time']
        
        # æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ
        final_table = Table(title=f"ğŸ {spider.name} å®Œäº†ãƒ¬ãƒãƒ¼ãƒˆ", show_header=True, header_style="bold yellow")
        final_table.add_column("é …ç›®", style="cyan", width=20)
        final_table.add_column("å€¤", style="green", width=15)
        
        final_table.add_row("ğŸ“¤ ç·ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°", str(self.stats['requests_count']))
        final_table.add_row("ğŸ“¥ ç·ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ•°", str(self.stats['responses_count']))
        final_table.add_row("ğŸ“¦ ç·ã‚¢ã‚¤ãƒ†ãƒ æ•°", str(self.stats['items_count']))
        final_table.add_row("âŒ ã‚¨ãƒ©ãƒ¼æ•°", str(self.stats['errors_count']))
        final_table.add_row("â±ï¸ ç·å®Ÿè¡Œæ™‚é–“", f"{elapsed_time:.2f}ç§’")
        final_table.add_row("ğŸ çµ‚äº†ç†ç”±", reason)
        
        if elapsed_time > 0:
            items_per_sec = self.stats['items_count'] / elapsed_time
            final_table.add_row("ğŸš€ å¹³å‡å‡¦ç†é€Ÿåº¦", f"{items_per_sec:.2f} items/sec")
        
        self.console.print(Panel(final_table, title="ğŸ“Š ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†", border_style="yellow"))

    def _save_stats(self):
        """çµ±è¨ˆæƒ…å ±ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        if not self.stats_file:
            return

        try:
            # çµŒéæ™‚é–“ã‚’è¨ˆç®—
            elapsed_time = 0
            if self.stats['start_time']:
                current_time = self.stats['finish_time'] or time.time()
                elapsed_time = current_time - self.stats['start_time']

            # é€Ÿåº¦è¨ˆç®—
            items_per_second = 0
            requests_per_second = 0
            items_per_minute = 0
            if elapsed_time > 0:
                items_per_second = self.stats['items_count'] / elapsed_time
                requests_per_second = self.stats['requests_count'] / elapsed_time
                items_per_minute = items_per_second * 60

            # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡è¨ˆç®—
            total_responses = self.stats['responses_count']
            success_rate = 0
            error_rate = 0
            if total_responses > 0:
                success_rate = ((total_responses - self.stats['errors_count']) / total_responses) * 100
                error_rate = (self.stats['errors_count'] / total_responses) * 100

            # Scrapyæ¨™æº–å½¢å¼ã®çµ±è¨ˆæƒ…å ±ã‚’ä½œæˆ
            scrapy_stats = {
                # åŸºæœ¬çµ±è¨ˆ
                'item_scraped_count': self.stats['items_count'],
                'downloader/request_count': self.stats['requests_count'],
                'response_received_count': self.stats['responses_count'],
                'spider_exceptions': self.stats['errors_count'],

                # æ™‚é–“æƒ…å ±
                'elapsed_time_seconds': elapsed_time,
                'start_time': self.stats['start_time'],
                'finish_time': self.stats['finish_time'],

                # é€Ÿåº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹
                'items_per_second': items_per_second,
                'requests_per_second': requests_per_second,
                'items_per_minute': items_per_minute,

                # æˆåŠŸç‡ãƒ»ã‚¨ãƒ©ãƒ¼ç‡
                'success_rate': success_rate,
                'error_rate': error_rate,

                # HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'downloader/response_status_count/200': max(0, self.stats['responses_count'] - self.stats['errors_count']),
                'downloader/response_status_count/404': 0,
                'downloader/response_status_count/500': self.stats['errors_count'],

                # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«çµ±è¨ˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
                'log_count/DEBUG': 0,
                'log_count/INFO': self.stats['items_count'],
                'log_count/WARNING': 0,
                'log_count/ERROR': self.stats['errors_count'],
                'log_count/CRITICAL': 0,

                # Rich progressæ‹¡å¼µçµ±è¨ˆ
                'rich_progress_enabled': True,
                'rich_progress_version': '1.0.0'
            }

            # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            with open(self.stats_file, 'w', encoding='utf-8') as f:
                json.dump(scrapy_stats, f, indent=2, ensure_ascii=False)

        except Exception as e:
            # çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆé€²æ—ãƒãƒ¼è¡¨ç¤ºã«å½±éŸ¿ã—ãªã„ã‚ˆã†ã«ï¼‰
            pass

    def _sync_with_scrapy_stats(self):
        """Scrapyã®çµ±è¨ˆæƒ…å ±ã¨åŒæœŸ"""
        try:
            if hasattr(self.crawler, 'stats'):
                scrapy_stats = self.crawler.stats

                # Scrapyã®çµ±è¨ˆæƒ…å ±ã‹ã‚‰å€¤ã‚’å–å¾—
                self.stats['items_count'] = scrapy_stats.get_value('item_scraped_count', self.stats['items_count'])
                self.stats['requests_count'] = scrapy_stats.get_value('downloader/request_count', self.stats['requests_count'])
                self.stats['responses_count'] = scrapy_stats.get_value('response_received_count', self.stats['responses_count'])
                self.stats['errors_count'] = scrapy_stats.get_value('spider_exceptions', self.stats['errors_count'])

        except Exception as e:
            # åŒæœŸã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
            pass




# è¨­å®šä¾‹ã‚’ã‚³ãƒ¡ãƒ³ãƒˆã§è¨˜è¼‰
"""
# settings.pyã«è¿½åŠ ã™ã‚‹è¨­å®šä¾‹

# Riché€²æ—ãƒãƒ¼ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_ENABLED = True

# è©³ç´°çµ±è¨ˆã‚’è¡¨ç¤º
RICH_PROGRESS_SHOW_STATS = True

# æ›´æ–°é–“éš”ï¼ˆç§’ï¼‰
RICH_PROGRESS_UPDATE_INTERVAL = 0.1

# WebSocketé€šçŸ¥ã‚’æœ‰åŠ¹åŒ–
RICH_PROGRESS_WEBSOCKET = True

# æ‹¡å¼µæ©Ÿèƒ½ã‚’ç™»éŒ²
EXTENSIONS = {
    'app.scrapy_extensions.rich_progress_extension.RichProgressExtension': 500,
}
"""
