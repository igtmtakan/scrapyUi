#!/usr/bin/env python3
"""
é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import sys
import os
import json
import hashlib
from datetime import datetime

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from app.database import SessionLocal, Result, Task

def calculate_data_hash(data):
    """ãƒ‡ãƒ¼ã‚¿ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
    # ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€æ„æ€§ã‚’åˆ¤æ–­ã™ã‚‹ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŠ½å‡º
    unique_fields = {
        'title': data.get('title', ''),
        'price': data.get('price', ''),
        'product_url': data.get('product_url', ''),
        'rating': data.get('rating', ''),
        'reviews': data.get('reviews', '')
    }
    
    # JSONæ–‡å­—åˆ—ã«ã—ã¦ãƒãƒƒã‚·ãƒ¥åŒ–
    json_str = json.dumps(unique_fields, sort_keys=True, ensure_ascii=False)
    return hashlib.md5(json_str.encode('utf-8')).hexdigest()

def cleanup_duplicates():
    """é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    db = SessionLocal()
    
    try:
        print("ğŸ” é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’é–‹å§‹...")
        
        # å…¨ã¦ã®çµæœã‚’å–å¾—
        all_results = db.query(Result).order_by(Result.created_at.asc()).all()
        print(f"ğŸ“Š ç·çµæœæ•°: {len(all_results)}ä»¶")
        
        # ãƒãƒƒã‚·ãƒ¥å€¤ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        hash_groups = {}
        for result in all_results:
            data_hash = calculate_data_hash(result.data)
            if data_hash not in hash_groups:
                hash_groups[data_hash] = []
            hash_groups[data_hash].append(result)
        
        # é‡è¤‡ã‚’æ¤œå‡º
        duplicates_found = 0
        duplicates_removed = 0
        
        for data_hash, results in hash_groups.items():
            if len(results) > 1:
                duplicates_found += len(results) - 1
                print(f"ğŸ” é‡è¤‡æ¤œå‡º: {len(results)}ä»¶ã®åŒä¸€ãƒ‡ãƒ¼ã‚¿")
                
                # æœ€åˆã®çµæœã‚’ä¿æŒã€æ®‹ã‚Šã‚’å‰Šé™¤
                keep_result = results[0]
                remove_results = results[1:]
                
                for remove_result in remove_results:
                    print(f"   âŒ å‰Šé™¤: {remove_result.id[:8]}... - {remove_result.data.get('title', 'N/A')[:30]}...")
                    db.delete(remove_result)
                    duplicates_removed += 1
                
                print(f"   âœ… ä¿æŒ: {keep_result.id[:8]}... - {keep_result.data.get('title', 'N/A')[:30]}...")
        
        # å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
        db.commit()
        
        # çµ±è¨ˆã‚’æ›´æ–°
        remaining_results = db.query(Result).count()
        
        print(f"\nğŸ“Š ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—çµæœ:")
        print(f"   é‡è¤‡æ¤œå‡ºæ•°: {duplicates_found}ä»¶")
        print(f"   å‰Šé™¤æ•°: {duplicates_removed}ä»¶")
        print(f"   æ®‹å­˜æ•°: {remaining_results}ä»¶")
        
        # ã‚¿ã‚¹ã‚¯ã®çµ±è¨ˆã‚’æ›´æ–°
        print(f"\nğŸ”„ ã‚¿ã‚¹ã‚¯çµ±è¨ˆã‚’æ›´æ–°ä¸­...")
        tasks = db.query(Task).all()
        for task in tasks:
            result_count = db.query(Result).filter(Result.task_id == task.id).count()
            task.items_count = result_count
            print(f"   ğŸ“Š {task.id[:8]}... - ã‚¢ã‚¤ãƒ†ãƒ æ•°: {result_count}")
        
        db.commit()
        print(f"âœ… ã‚¿ã‚¹ã‚¯çµ±è¨ˆæ›´æ–°å®Œäº†")
        
        print(f"\nğŸ‰ é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        
    except Exception as e:
        db.rollback()
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        print(f"âŒ ã‚¨ãƒ©ãƒ¼è©³ç´°: {traceback.format_exc()}")
    finally:
        db.close()

if __name__ == "__main__":
    cleanup_duplicates()
