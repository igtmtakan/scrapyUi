#!/usr/bin/env python3
"""
ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã®DBåŒæœŸçŠ¶æ³ã‚’ç¢ºèª
"""
import sqlite3
import requests
import json
from pathlib import Path
from datetime import datetime

# APIãƒ™ãƒ¼ã‚¹URL
BASE_URL = "http://localhost:8000"

def check_huhuhuh_sync():
    """ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®DBåŒæœŸçŠ¶æ³ã‚’ç¢ºèª"""

    print("ğŸ” ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®DBåŒæœŸçŠ¶æ³ç¢ºèªé–‹å§‹\n")

    # ãƒ­ã‚°ã‚¤ãƒ³
    login_data = {'email': 'admin@scrapyui.com', 'password': 'admin123456'}
    response = requests.post(f'{BASE_URL}/api/auth/login', json=login_data)
    token = response.json()['access_token']
    headers = {'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'}

    print('ğŸ” ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸ')

    # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œç´¢
    print('\nğŸ“‹ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œç´¢')
    project_info = find_huhuhuh_project()

    if not project_info:
        print('âŒ ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        return False

    project_id, project_name, project_path, user_id, db_save_enabled = project_info

    print(f'âœ… ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç™ºè¦‹:')
    print(f'   ID: {project_id}')
    print(f'   åå‰: {project_name}')
    print(f'   ãƒ‘ã‚¹: {project_path}')
    print(f'   DBä¿å­˜è¨­å®š: {"æœ‰åŠ¹" if db_save_enabled else "ç„¡åŠ¹"}')

    # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å†…å®¹ã‚’ç¢ºèª
    print(f'\nğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å†…å®¹ç¢ºèª')
    filesystem_content = check_filesystem_content(project_path)

    # 3. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ã‚’ç¢ºèª
    print(f'\nğŸ’¾ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ç¢ºèª')
    database_content = check_database_content(project_id)

    # 4. WebUI APIã®å†…å®¹ã‚’ç¢ºèª
    print(f'\nğŸŒ WebUI APIã®å†…å®¹ç¢ºèª')
    webui_content = check_webui_content(project_id, headers)

    # 5. åŒæœŸçŠ¶æ³ã‚’åˆ†æ
    print(f'\nğŸ“Š åŒæœŸçŠ¶æ³åˆ†æ')
    analyze_sync_status(filesystem_content, database_content, webui_content, db_save_enabled)

    # 6. å¿…è¦ã«å¿œã˜ã¦åŒæœŸã‚’å®Ÿè¡Œ
    if filesystem_content and (not database_content or filesystem_content != database_content):
        print(f'\nğŸ”„ åŒæœŸãŒå¿…è¦ã§ã™ã€‚æ‰‹å‹•åŒæœŸã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ')
        print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ : {len(filesystem_content) if filesystem_content else 0}æ–‡å­—')
        print(f'   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {len(database_content) if database_content else 0}æ–‡å­—')

        # è‡ªå‹•ã§åŒæœŸå®Ÿè¡Œ
        print(f'\nğŸ”„ è‡ªå‹•åŒæœŸå®Ÿè¡Œä¸­...')
        sync_success = manual_sync_huhuhuh(project_id, project_path, user_id, db_save_enabled)

        if sync_success:
            print(f'âœ… åŒæœŸå®Œäº†')

            # åŒæœŸå¾Œã®ç¢ºèª
            print(f'\nâœ… åŒæœŸå¾Œã®ç¢ºèª')
            webui_content_after = check_webui_content(project_id, headers)

            if webui_content_after and len(webui_content_after) > len(webui_content or ''):
                print(f'ğŸ‰ WebUIã§æ­£ã—ã„å†…å®¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸï¼')
            else:
                print(f'âš ï¸ WebUIã®å†…å®¹ã«å¤‰åŒ–ãŒã‚ã‚Šã¾ã›ã‚“')
        else:
            print(f'âŒ åŒæœŸå¤±æ•—')

    return True

def find_huhuhuh_project():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œç´¢"""

    db_path = Path("backend/database/scrapy_ui.db")

    if not db_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return None

    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()

        # ã€Œhuhuhuã€ã‚’å«ã‚€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¤œç´¢
        cursor.execute("""
            SELECT id, name, path, user_id, db_save_enabled
            FROM projects
            WHERE name LIKE '%huhuhu%' OR path LIKE '%huhuhu%'
            ORDER BY created_at DESC
        """)

        results = cursor.fetchall()

        if results:
            print(f'ğŸ“Š ã€Œhuhuhuã€é–¢é€£ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {len(results)}ä»¶')
            for i, (pid, name, path, uid, db_save) in enumerate(results):
                print(f'   {i+1}. {name} (ãƒ‘ã‚¹: {path}, DBä¿å­˜: {"æœ‰åŠ¹" if db_save else "ç„¡åŠ¹"})')

            # æœ€åˆã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¿”ã™
            return results[0]
        else:
            print(f'âŒ ã€Œhuhuhuã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')

            # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’è¡¨ç¤º
            cursor.execute("SELECT name, path FROM projects ORDER BY created_at DESC LIMIT 10")
            all_projects = cursor.fetchall()
            print(f'\nğŸ“‹ æœ€è¿‘ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§:')
            for name, path in all_projects:
                print(f'   - {name} (ãƒ‘ã‚¹: {path})')

            return None

        conn.close()

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def check_filesystem_content(project_path):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®å†…å®¹ã‚’ç¢ºèª"""

    scrapy_projects_dir = Path("scrapy_projects")

    # å¯èƒ½ãªãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¢ºèª
    possible_paths = [
        scrapy_projects_dir / project_path / project_path / "pipelines.py",
        scrapy_projects_dir / project_path / "pipelines.py",
    ]

    pipelines_file = None
    for path in possible_paths:
        if path.exists():
            pipelines_file = path
            break

    if pipelines_file:
        print(f'ğŸ“„ pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {pipelines_file}')

        try:
            with open(pipelines_file, 'r', encoding='utf-8') as f:
                content = f.read()

            print(f'ğŸ“Š ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹åˆ†æ:')
            print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content)}æ–‡å­—')

            # é‡è¦ãªè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
            has_scrapy_ui_pipeline = 'ScrapyUIDatabasePipeline' in content
            has_scrapy_ui_json_pipeline = 'ScrapyUIJSONPipeline' in content
            has_import_statement = 'from app.templates.database_pipeline import' in content

            print(f'   ScrapyUIDatabasePipeline: {"ã‚ã‚Š" if has_scrapy_ui_pipeline else "ãªã—"}')
            print(f'   ScrapyUIJSONPipeline: {"ã‚ã‚Š" if has_scrapy_ui_json_pipeline else "ãªã—"}')
            print(f'   ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡: {"ã‚ã‚Š" if has_import_statement else "ãªã—"}')

            return content

        except Exception as e:
            print(f'âŒ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}')
            return None
    else:
        print(f'âŒ pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        print(f'   ç¢ºèªã—ãŸãƒ‘ã‚¹:')
        for path in possible_paths:
            print(f'     - {path}')
        return None

def check_database_content(project_id):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®å†…å®¹ã‚’ç¢ºèª"""

    db_path = Path("backend/database/scrapy_ui.db")

    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        cursor.execute("""
            SELECT name, path, content, length(content) as content_length
            FROM project_files
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))

        result = cursor.fetchone()

        if result:
            name, path, content, content_length = result
            print(f'ğŸ“„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…pipelines.pyç™ºè¦‹:')
            print(f'   åå‰: {name}')
            print(f'   ãƒ‘ã‚¹: {path}')
            print(f'   ã‚µã‚¤ã‚º: {content_length}æ–‡å­—')

            # å†…å®¹ã‚’ãƒã‚§ãƒƒã‚¯
            has_scrapy_ui_pipeline = 'ScrapyUIDatabasePipeline' in content
            has_scrapy_ui_json_pipeline = 'ScrapyUIJSONPipeline' in content

            print(f'   ScrapyUIDatabasePipeline: {"ã‚ã‚Š" if has_scrapy_ui_pipeline else "ãªã—"}')
            print(f'   ScrapyUIJSONPipeline: {"ã‚ã‚Š" if has_scrapy_ui_json_pipeline else "ãªã—"}')

            return content
        else:
            print(f'âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«pipelines.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            return None

        conn.close()

    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def check_webui_content(project_id, headers):
    """WebUI APIã®å†…å®¹ã‚’ç¢ºèª"""

    # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’å–å¾—
    response = requests.get(f'{BASE_URL}/api/projects/{project_id}/files/pipelines.py', headers=headers)

    if response.status_code == 200:
        file_data = response.json()
        content = file_data.get('content', '')

        print(f'ğŸ“Š WebUIå†…å®¹åˆ†æ:')
        print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {len(content)}æ–‡å­—')

        # é‡è¦ãªè¦ç´ ã‚’ãƒã‚§ãƒƒã‚¯
        has_scrapy_ui_pipeline = 'ScrapyUIDatabasePipeline' in content
        has_scrapy_ui_json_pipeline = 'ScrapyUIJSONPipeline' in content

        print(f'   ScrapyUIDatabasePipeline: {"ã‚ã‚Š" if has_scrapy_ui_pipeline else "ãªã—"}')
        print(f'   ScrapyUIJSONPipeline: {"ã‚ã‚Š" if has_scrapy_ui_json_pipeline else "ãªã—"}')

        return content
    else:
        print(f'âŒ WebUIãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾—å¤±æ•—: {response.status_code}')
        return None

def analyze_sync_status(filesystem_content, database_content, webui_content, db_save_enabled):
    """åŒæœŸçŠ¶æ³ã‚’åˆ†æ"""

    print(f'ğŸ“Š åŒæœŸçŠ¶æ³åˆ†æçµæœ:')

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
    fs_has_scrapy_ui = 'ScrapyUIDatabasePipeline' in (filesystem_content or '')
    fs_size = len(filesystem_content) if filesystem_content else 0
    print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ : {fs_size}æ–‡å­—, ScrapyUI={"ã‚ã‚Š" if fs_has_scrapy_ui else "ãªã—"}')

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    db_has_scrapy_ui = 'ScrapyUIDatabasePipeline' in (database_content or '')
    db_size = len(database_content) if database_content else 0
    print(f'   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {db_size}æ–‡å­—, ScrapyUI={"ã‚ã‚Š" if db_has_scrapy_ui else "ãªã—"}')

    # WebUI
    webui_has_scrapy_ui = 'ScrapyUIDatabasePipeline' in (webui_content or '')
    webui_size = len(webui_content) if webui_content else 0
    print(f'   WebUI: {webui_size}æ–‡å­—, ScrapyUI={"ã‚ã‚Š" if webui_has_scrapy_ui else "ãªã—"}')

    # æœŸå¾…å€¤
    expected_has_scrapy_ui = db_save_enabled
    print(f'   æœŸå¾…å€¤: ScrapyUI={"ã‚ã‚Š" if expected_has_scrapy_ui else "ãªã—"} (DBä¿å­˜{"æœ‰åŠ¹" if db_save_enabled else "ç„¡åŠ¹"})')

    # åŒæœŸçŠ¶æ³
    fs_correct = fs_has_scrapy_ui == expected_has_scrapy_ui
    db_correct = db_has_scrapy_ui == expected_has_scrapy_ui
    webui_correct = webui_has_scrapy_ui == expected_has_scrapy_ui

    print(f'\nğŸ“‹ æ­£ç¢ºæ€§ãƒã‚§ãƒƒã‚¯:')
    print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ : {"âœ… æ­£ã—ã„" if fs_correct else "âŒ ä¸æ­£"}')
    print(f'   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {"âœ… æ­£ã—ã„" if db_correct else "âŒ ä¸æ­£"}')
    print(f'   WebUI: {"âœ… æ­£ã—ã„" if webui_correct else "âŒ ä¸æ­£"}')

    # åŒæœŸãƒã‚§ãƒƒã‚¯
    fs_db_sync = filesystem_content == database_content if filesystem_content and database_content else False
    db_webui_sync = database_content == webui_content if database_content and webui_content else False

    print(f'\nğŸ”„ åŒæœŸãƒã‚§ãƒƒã‚¯:')
    print(f'   ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ  â†” ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹: {"âœ… åŒæœŸ" if fs_db_sync else "âŒ éåŒæœŸ"}')
    print(f'   ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ â†” WebUI: {"âœ… åŒæœŸ" if db_webui_sync else "âŒ éåŒæœŸ"}')

def manual_sync_huhuhuh(project_id, project_path, user_id, db_save_enabled):
    """ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ‰‹å‹•åŒæœŸ"""

    print(f'ğŸ”„ æ‰‹å‹•åŒæœŸé–‹å§‹: {project_path}')

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰pipelines.pyã‚’èª­ã¿å–ã‚Š
    scrapy_projects_dir = Path("scrapy_projects")
    possible_paths = [
        scrapy_projects_dir / project_path / project_path / "pipelines.py",
        scrapy_projects_dir / project_path / "pipelines.py",
    ]

    pipelines_file = None
    for path in possible_paths:
        if path.exists():
            pipelines_file = path
            break

    if not pipelines_file:
        print(f'âŒ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«pipelines.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
        return False

    try:
        with open(pipelines_file, 'r', encoding='utf-8') as f:
            content = f.read()

        print(f'ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿å–ã‚ŠæˆåŠŸ: {len(content)}æ–‡å­—')

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ç›´æ¥æ›´æ–°
        db_path = Path("backend/database/scrapy_ui.db")
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()

        # æ—¢å­˜ã®pipelines.pyã‚’å‰Šé™¤
        cursor.execute("""
            DELETE FROM project_files
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))

        deleted_count = cursor.rowcount
        print(f'ğŸ—‘ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_count}ä»¶')

        # æ–°è¦ä½œæˆ
        import uuid
        cursor.execute("""
            INSERT INTO project_files
            (id, name, path, content, file_type, project_id, user_id, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            str(uuid.uuid4()),
            "pipelines.py",
            "pipelines.py",
            content,
            "python",
            project_id,
            user_id,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))
        print(f'âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ–°è¦ä½œæˆæˆåŠŸ')

        conn.commit()
        conn.close()

        return True

    except Exception as e:
        print(f'âŒ æ‰‹å‹•åŒæœŸã‚¨ãƒ©ãƒ¼: {e}')
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®DBåŒæœŸçŠ¶æ³ç¢ºèªãƒ„ãƒ¼ãƒ«")

    success = check_huhuhuh_sync()

    if success:
        print("\nğŸ‰ ç¢ºèªå®Œäº†ï¼")
        print("\nğŸŒ WebUIç¢ºèª:")
        print("  1. http://localhost:4000 ã«ã‚¢ã‚¯ã‚»ã‚¹")
        print("  2. ã€Œhuhuhuhã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ")
        print("  3. pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã")
        print("  4. æ­£ã—ã„å†…å®¹ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª")
    else:
        print("\nâŒ ç¢ºèªå¤±æ•—")

if __name__ == "__main__":
    main()
