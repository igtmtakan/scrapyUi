from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List
import uuid
import os

from ..database import get_db, Project as DBProject, Spider as DBSpider, User as DBUser, UserRole
from ..models.schemas import Project, ProjectCreate, ProjectUpdate, ProjectWithSpiders, ProjectWithUser, Spider, SpiderCreate
from ..services.scrapy_service import ScrapyPlaywrightService
from ..api.auth import get_current_active_user

# ãƒ­ã‚®ãƒ³ã‚°ã¨ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from ..utils.logging_config import get_logger, log_with_context, log_exception
from ..utils.error_handler import (
    ScrapyUIException,
    ProjectException,
    ResourceNotFoundException,
    AuthorizationException,
    ErrorCode,
    handle_exception
)

# ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–
logger = get_logger(__name__)

router = APIRouter(
    responses={
        404: {"description": "Not found"},
        400: {"description": "Bad request"},
        500: {"description": "Internal server error"}
    }
)


def sync_project_files_to_database(db, project_id: str, project_path: str, user_id: str):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆæ™‚ã«å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
    from ..database import ProjectFile
    from pathlib import Path
    from datetime import datetime

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹
    project_dir = Path(f"./scrapy_projects/{project_path}")

    if not project_dir.exists():
        logger.warning(f"Project directory not found: {project_dir}")
        return

    # åŒæœŸå¯¾è±¡ã®ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆScrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®æ¨™æº–æ§‹é€ ï¼‰
    file_patterns = [
        # ãƒ«ãƒ¼ãƒˆãƒ¬ãƒ™ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ«
        ("scrapy.cfg", "config"),

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«
        (f"{project_path}/__init__.py", "python"),
        (f"{project_path}/settings.py", "python"),
        (f"{project_path}/items.py", "python"),
        (f"{project_path}/pipelines.py", "python"),
        (f"{project_path}/middlewares.py", "python"),

        # spidersãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ãƒ•ã‚¡ã‚¤ãƒ«
        (f"{project_path}/spiders/__init__.py", "python"),
    ]

    # å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å‹•çš„ã«æ¤œç´¢ã—ã¦è¿½åŠ 
    try:
        import glob
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®ã™ã¹ã¦ã®.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        py_files = glob.glob(str(project_dir / "**" / "*.py"), recursive=True)
        for py_file in py_files:
            relative_path = Path(py_file).relative_to(project_dir)
            file_pattern = (str(relative_path), "python")
            if file_pattern not in file_patterns:
                file_patterns.append(file_pattern)
                logger.info(f"Added dynamically found file: {relative_path}")

        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ¤œç´¢
        config_files = glob.glob(str(project_dir / "**" / "*.cfg"), recursive=True)
        for config_file in config_files:
            relative_path = Path(config_file).relative_to(project_dir)
            file_pattern = (str(relative_path), "config")
            if file_pattern not in file_patterns:
                file_patterns.append(file_pattern)
                logger.info(f"Added dynamically found config file: {relative_path}")
    except Exception as e:
        logger.warning(f"Failed to dynamically search for files: {str(e)}")

    synced_files = []

    for file_path, file_type in file_patterns:
        full_path = project_dir / file_path

        if full_path.exists():
            try:
                with open(full_path, 'r', encoding='utf-8') as f:
                    content = f.read()

                # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’å–å¾—
                file_name = full_path.name

                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆpathã§åˆ¤å®šï¼‰
                existing_file = db.query(ProjectFile).filter(
                    ProjectFile.project_id == project_id,
                    ProjectFile.path == file_path
                ).first()

                if existing_file:
                    # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                    existing_file.content = content
                    existing_file.updated_at = datetime.now()
                    logger.info(f"Updated existing file in database: {file_path}")
                else:
                    # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                    db_file = ProjectFile(
                        id=str(uuid.uuid4()),
                        name=file_name,
                        path=file_path,
                        content=content,
                        file_type=file_type,
                        project_id=project_id,
                        user_id=user_id
                    )
                    db.add(db_file)
                    logger.info(f"Added new file to database: {file_path}")

                # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å€‹åˆ¥ã«ã‚³ãƒŸãƒƒãƒˆ
                try:
                    db.commit()
                    synced_files.append(file_path)
                except Exception as commit_error:
                    db.rollback()
                    logger.error(f"Failed to commit file {file_path}: {str(commit_error)}")

            except Exception as e:
                logger.error(f"Failed to sync file {file_path}: {str(e)}")

    # æœ€çµ‚çµæœã‚’ãƒ­ã‚°å‡ºåŠ›
    logger.info(f"Successfully synced {len(synced_files)} files to database: {synced_files}")


def sync_spider_file_to_database(db, project_id: str, project_path: str, spider_name: str, spider_code: str, user_id: str):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
    from ..database import ProjectFile
    from datetime import datetime

    try:
        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        spider_file_path = f"{project_path}/spiders/{spider_name}.py"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        existing_file = db.query(ProjectFile).filter(
            ProjectFile.project_id == project_id,
            ProjectFile.path == spider_file_path
        ).first()

        if existing_file:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            existing_file.content = spider_code
            existing_file.updated_at = datetime.now()
            logger.info(f"Updated spider file in database: {spider_file_path}")
        else:
            # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            db_file = ProjectFile(
                id=str(uuid.uuid4()),
                name=f"{spider_name}.py",
                path=spider_file_path,
                content=spider_code,
                file_type="python",
                project_id=project_id,
                user_id=user_id
            )
            db.add(db_file)
            logger.info(f"Added spider file to database: {spider_file_path}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚³ãƒŸãƒƒãƒˆ
        db.commit()
        logger.info(f"Successfully synced spider file to database: {spider_name}")

    except Exception as e:
        db.rollback()
        logger.error(f"Failed to sync spider file to database: {str(e)}")
        raise

@router.get(
    "/",
    response_model=List[ProjectWithUser],
    summary="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—",
    description="ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆ"
)
async def get_projects(
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """
    ## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¸€è¦§å–å¾—

    ç™»éŒ²ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    # ç®¡ç†è€…ã¯å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã€ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿
    if current_user.role == UserRole.ADMIN or current_user.role == "admin" or current_user.role == "ADMIN":
        projects = db.query(DBProject).join(DBUser).all()
    else:
        projects = db.query(DBProject).filter(DBProject.user_id == current_user.id).join(DBUser).all()

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä½œæˆ
    result = []
    for project in projects:
        project_dict = {
            "id": project.id,
            "name": project.name,
            "description": project.description,
            "path": project.path,
            "scrapy_version": project.scrapy_version,
            "settings": project.settings,
            "created_at": project.created_at,
            "updated_at": project.updated_at,
            "user_id": project.user_id,
            "username": project.user.username if project.user else None,
            "is_active": project.is_active  # is_activeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’è¿½åŠ 
        }
        result.append(project_dict)

    return result

@router.get("/{project_id}", response_model=ProjectWithSpiders)
async def get_project(
    project_id: str,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """ç‰¹å®šã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±å«ã‚€ï¼‰"""
    project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ç®¡ç†è€…ä»¥å¤–ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
    is_admin = (current_user.role == UserRole.ADMIN or
                current_user.role == "admin" or
                current_user.role == "ADMIN")
    if not is_admin and project.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚‚å«ã‚ã¦è¿”ã™
    spiders = db.query(DBSpider).filter(DBSpider.project_id == project_id).all()

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼æƒ…å ±ã‚’é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    formatted_spiders = []
    for spider in spiders:
        spider_dict = {
            "id": spider.id,
            "name": spider.name,
            "description": spider.description or "",
            "code": spider.code or "# Empty spider code",  # ç©ºã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚³ãƒ¡ãƒ³ãƒˆã‚’è¨­å®š
            "template": spider.template,
            "framework": spider.framework or "scrapy",
            "start_urls": spider.start_urls or [],
            "settings": spider.settings or {},
            "project_id": spider.project_id,
            "created_at": spider.created_at,
            "updated_at": spider.updated_at
        }
        formatted_spiders.append(spider_dict)

    project_dict = {
        "id": project.id,
        "name": project.name,
        "description": project.description,
        "path": project.path,
        "scrapy_version": project.scrapy_version,
        "settings": project.settings or {},
        "created_at": project.created_at,
        "updated_at": project.updated_at,
        "spiders": formatted_spiders
    }

    return project_dict

@router.post("/", response_model=Project, status_code=status.HTTP_201_CREATED)
async def create_project(
    project: ProjectCreate,
    db: Session = Depends(get_db),
    current_user: DBUser = Depends(get_current_active_user)
):
    """æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ"""

    try:
        log_with_context(
            logger, "INFO",
            f"Creating new project: {project.name}",
            extra_data={"project_name": project.name, "description": project.description}
        )

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼åˆ¥ï¼‰
        existing_project = db.query(DBProject).filter(
            DBProject.name == project.name,
            DBProject.user_id == current_user.id
        ).first()
        if existing_project:
            raise HTTPException(
                status_code=status.HTTP_409_CONFLICT,
                detail=f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå '{project.name}' ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚åˆ¥ã®åå‰ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚"
            )

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼åãƒ™ãƒ¼ã‚¹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’ç”Ÿæˆ
        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: <username>_<projectname>
        username = current_user.username.lower().replace(' ', '_').replace('-', '_')
        project_name_clean = project.name.lower().replace(' ', '_').replace('-', '_')
        project_path = f"{username}_{project_name_clean}"

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆå¿µã®ãŸã‚ï¼‰
        existing_path = db.query(DBProject).filter(DBProject.path == project_path).first()
        if existing_path:
            # åŒã˜ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§åŒã˜ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®å ´åˆã¯æ—¢ã«ä¸Šã§ãƒã‚§ãƒƒã‚¯æ¸ˆã¿
            # ç•°ãªã‚‹ãƒ¦ãƒ¼ã‚¶ãƒ¼ã§åŒã˜username_projectnameã«ãªã‚‹å ´åˆã®ã¿ã“ã“ã«åˆ°é”
            project_path = f"{project_path}_{str(uuid.uuid4())[:8]}"

        # Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆï¼ˆscrapy startproject project_name ã¨åŒã˜å‹•ä½œï¼‰
        try:
            # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å®Ÿéš›ã®Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not os.getenv("TESTING", False):
                scrapy_service = ScrapyPlaywrightService()
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåï¼‰ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ï¼ˆè¨­å®šåï¼‰ã‚’æ­£ã—ãæŒ‡å®š
                scrapy_service.create_project(project_path, project_path)
                logger.info(f"Scrapy project created successfully: {project_path}")
            else:
                # ãƒ†ã‚¹ãƒˆç’°å¢ƒã§ã¯å˜ç´”ã«ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
                project_dir = f"./scrapy_projects/{project_path}"
                os.makedirs(project_dir, exist_ok=True)
                logger.info(f"Test project directory created: {project_dir}")
        except Exception as e:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã«å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã¯ä¿å­˜ã™ã‚‹ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
            log_exception(
                logger, f"Warning: Failed to create Scrapy project: {str(e)}",
                extra_data={"project_name": project.name, "project_path": project_path}
            )

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        db_project = DBProject(
            id=str(uuid.uuid4()),
            name=project.name,
            description=project.description,
            path=project_path,
            scrapy_version=project.scrapy_version or "2.11.0",
            settings=project.settings or {},
            user_id=current_user.id  # ç¾åœ¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’è¨­å®š
        )

        db.add(db_project)
        db.commit()
        db.refresh(db_project)

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸï¼ˆå…¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
        try:
            if not os.getenv("TESTING", False):
                # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆå®Œäº†ã‚’ç¢ºå®Ÿã«ã™ã‚‹ï¼‰
                import time
                time.sleep(0.5)

                sync_project_files_to_database(db, db_project.id, project_path, current_user.id)
                logger.info(f"All project files synced to database for project: {project_path}")

                # åŒæœŸå¾Œã®ç¢ºèª
                from ..database import ProjectFile
                synced_count = db.query(ProjectFile).filter(ProjectFile.project_id == db_project.id).count()
                logger.info(f"Total files synced to database: {synced_count}")
        except Exception as e:
            logger.warning(f"Failed to save project files to database: {str(e)}")
            # ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä½œæˆã¯æˆåŠŸã¨ã™ã‚‹ï¼‰

        log_with_context(
            logger, "INFO",
            f"Project created successfully: {project.name}",
            project_id=db_project.id,
            extra_data={"project_path": project_path}
        )

        return db_project

    except HTTPException:
        # HTTPExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
        raise
    except ProjectException:
        # æ—¢ã«ProjectExceptionã®å ´åˆã¯å†ç™ºç”Ÿ
        raise
    except Exception as e:
        error_msg = f"Failed to create project: {str(e)}"
        log_exception(
            logger, error_msg,
            extra_data={"project_name": project.name}
        )
        raise ProjectException(
            message=error_msg,
            error_code=ErrorCode.PROJECT_CREATION_FAILED,
            details={"original_error": str(e)}
        )

@router.put("/{project_id}", response_model=Project)
async def update_project(
    project_id: str,
    project_update: ProjectUpdate,
    db: Session = Depends(get_db)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’æ›´æ–°"""
    db_project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not db_project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®é©ç”¨
    update_data = project_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_project, field, value)

    db.commit()
    db.refresh(db_project)

    return db_project

@router.delete("/{project_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_project(project_id: str, db: Session = Depends(get_db)):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤"""
    db_project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not db_project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«é–¢é€£ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤
    try:
        from ..database import Schedule as DBSchedule
        related_schedules = db.query(DBSchedule).filter(DBSchedule.project_id == project_id).all()

        if related_schedules:
            print(f"ğŸ—‘ï¸ Deleting {len(related_schedules)} schedules related to project {db_project.name}")
            for schedule in related_schedules:
                print(f"  - Deleting schedule: {schedule.name} (ID: {schedule.id})")
                db.delete(schedule)

        # å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ï¼‰
        db.commit()
        print(f"âœ… Successfully deleted {len(related_schedules)} related schedules")

    except Exception as e:
        print(f"âš ï¸ Error deleting related schedules: {str(e)}")
        db.rollback()
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå‰Šé™¤ã¯ç¶šè¡Œ

    # Scrapyãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    try:
        scrapy_service = ScrapyPlaywrightService()
        scrapy_service.delete_project(db_project.path)
    except Exception as e:
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã¯å‰Šé™¤ã™ã‚‹
        pass

    db.delete(db_project)
    db.commit()

    return None


@router.post("/{project_id}/spiders/", response_model=dict, status_code=status.HTTP_201_CREATED)
async def create_project_spider(
    project_id: str,
    spider_data: dict,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
    project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    spider_name = spider_data.get("name")
    if not spider_name:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider name is required"
        )

    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == project_id,
        DBSpider.name == spider_name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider with this name already exists in the project"
        )

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    db_spider = DBSpider(
        id=str(uuid.uuid4()),
        name=spider_name,
        code=spider_data.get("script", spider_data.get("code", "")),
        template=spider_data.get("template"),
        settings=spider_data.get("settings", {}),
        project_id=project_id,
        user_id=current_user.id
    )

    db.add(db_spider)
    db.commit()
    db.refresh(db_spider)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    try:
        if not os.getenv("TESTING", False):
            scrapy_service = ScrapyPlaywrightService()
            spider_code = spider_data.get("script", spider_data.get("code", ""))
            scrapy_service.save_spider_code(project.path, spider_name, spider_code)

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ
            try:
                sync_spider_file_to_database(db, project.id, project.path, spider_name, spider_code, current_user.id)
                logger.info(f"Spider file synced to database: {spider_name}")

                # åŒæœŸç¢ºèª
                from ..database import ProjectFile
                spider_file_path = f"{project.path}/spiders/{spider_name}.py"
                synced_file = db.query(ProjectFile).filter(
                    ProjectFile.project_id == project.id,
                    ProjectFile.path == spider_file_path
                ).first()
                if synced_file:
                    logger.info(f"Spider file sync confirmed: {spider_file_path}")
                else:
                    logger.warning(f"Spider file sync verification failed: {spider_file_path}")
            except Exception as sync_error:
                logger.error(f"Failed to sync spider file to database: {sync_error}")
                # åŒæœŸå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä½œæˆã¯æˆåŠŸã¨ã™ã‚‹ï¼‰
    except Exception as e:
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        db.delete(db_spider)
        db.commit()
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save spider file: {str(e)}"
        )

    return {
        "id": db_spider.id,
        "name": db_spider.name,
        "project_id": db_spider.project_id,
        "start_urls": spider_data.get("start_urls", []),
        "created_at": db_spider.created_at.isoformat() if db_spider.created_at else None
    }
