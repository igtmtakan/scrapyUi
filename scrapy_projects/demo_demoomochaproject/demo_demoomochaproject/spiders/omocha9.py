import scrapy
from scrapy import Request
import re
from rich.console import Console
from rich.progress import Progress, BarColumn, TextColumn, TimeRemainingColumn


class Omocha9Spider(scrapy.Spider):
    name = "omocha9"
    allowed_domains = ['amazon.co.jp']
    start_urls = [
        'https://www.amazon.co.jp/gp/bestsellers/toys/ref=zg_bs_nav_toys_0'
    ]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # Richçµ±åˆï¼ˆCLIã§ã®ã¿æœ‰åŠ¹ï¼‰
        try:
            self.console = Console()
            self.progress = None
            self.task_id = None
            self.rich_enabled = True
        except:
            self.console = None
            self.progress = None
            self.task_id = None
            self.rich_enabled = False

        self.items_count = 0
        self.requests_count = 0
        self.target_items = 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆç›®æ¨™

    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'ROBOTSTXT_OBEY': False,
        'COOKIES_ENABLED': True,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'HTTPCACHE_ENABLED': False,
        # 'HTTPCACHE_DIR': 'httpcache',
        # 'HTTPCACHE_EXPIRATION_SECS': 86400,
        'FEED_EXPORT_ENCODING': 'utf-8',
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Cache-Control': 'no-cache',
        }
    }

    def start_requests(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹æ™‚ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’åˆæœŸåŒ–"""
        # CLOSESPIDER_ITEMCOUNTã®è¨­å®šã‚’å–å¾—
        item_limit = self.settings.getint('CLOSESPIDER_ITEMCOUNT', 10)
        self.target_items = item_limit

        # Richãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’åˆæœŸåŒ–ï¼ˆCLIã§ã®ã¿ï¼‰
        if self.rich_enabled and self.console:
            try:
                self.progress = Progress(
                    TextColumn("[bold blue]ðŸ•·ï¸ Scraping Amazon Toys"),
                    BarColumn(bar_width=40),
                    TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
                    TextColumn("â€¢"),
                    TextColumn("[green]Items: {task.fields[items]}/{task.total}"),
                    TextColumn("â€¢"),
                    TextColumn("[blue]Requests: {task.fields[requests]}"),
                    TimeRemainingColumn(),
                    console=self.console
                )

                self.task_id = self.progress.add_task(
                    "scraping",
                    total=self.target_items,
                    items=0,
                    requests=0
                )

                self.progress.start()
                self.console.print(f"\nðŸš€ [bold green]Starting omocha9 spider - Target: {self.target_items} items[/bold green]\n")
            except Exception as e:
                self.logger.warning(f"Rich progress initialization failed: {e}")
                self.rich_enabled = False

        # é€šå¸¸ã®start_requestsã‚’å®Ÿè¡Œ
        for url in self.start_urls:
            yield scrapy.Request(url, callback=self.parse)

    def update_progress(self):
        """ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’æ›´æ–°"""
        if self.rich_enabled and self.progress and self.task_id is not None:
            try:
                self.progress.update(
                    self.task_id,
                    completed=self.items_count,
                    items=self.items_count,
                    requests=self.requests_count
                )
            except Exception as e:
                self.logger.warning(f"Rich progress update failed: {e}")
                self.rich_enabled = False

    def parse(self, response):
        """ãŠã‚‚ã¡ã‚ƒãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®è§£æž"""
        self.logger.info(f'Processing page: {response.url}')

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æ›´æ–°
        self.requests_count += 1
        self.update_progress()

        # å•†å“ãƒªãƒ³ã‚¯ã‚’å–å¾— (class="a-link-normal aok-block")
        product_links = response.css('a.a-link-normal.aok-block::attr(href)').getall()

        self.logger.info(f'Found {len(product_links)} product links')

        # å„å•†å“ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†
        for link in product_links:
            if link:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                full_url = response.urljoin(link)
                yield Request(
                    url=full_url,
                    callback=self.parse_product,
                    meta={'page_url': response.url}
                )

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆå†å¸°çš„ã«å·¡ã‚‹ï¼‰
        next_page_links = response.css('a[aria-label="æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•"]::attr(href)').getall()
        if not next_page_links:
            # åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚‚è©¦ã™
            next_page_links = response.css('a.s-pagination-next::attr(href)').getall()
        if not next_page_links:
            # ã•ã‚‰ã«åˆ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿
            next_page_links = response.css('li.a-last a::attr(href)').getall()

        for next_link in next_page_links:
            if next_link:
                next_url = response.urljoin(next_link)
                self.logger.info(f'Following next page: {next_url}')
                yield Request(
                    url=next_url,
                    callback=self.parse,
                    meta={'page_url': response.url}
                )

    def parse_product(self, response):
        """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã®è§£æž"""
        self.logger.info(f'Processing product: {response.url}')

        # å•†å“ã‚¿ã‚¤ãƒˆãƒ« (id="title")
        title = response.css('#title span::text').get()
        if not title:
            title = response.css('#title::text').get()
        if title:
            title = title.strip()

        # è©•ä¾¡ã‚’å–å¾—
        rating = None
        rating_text = response.css('span.a-icon-alt::text').get()
        if rating_text:
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            if rating_match:
                rating = rating_match.group(1)

        # ç¨Žè¾¼ä¾¡æ ¼ã‚’å–å¾—
        price = None
        price_selectors = [
            '.a-price-whole::text',
            '.a-price .a-offscreen::text',
            '.a-price-current .a-offscreen::text',
            '.a-price-range .a-offscreen::text'
        ]
        for selector in price_selectors:
            price_text = response.css(selector).get()
            if price_text:
                price = price_text.strip()
                break

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã‚’å–å¾—
        review_count = None
        review_selectors = [
            '#acrCustomerReviewText::text',
            'span[data-hook="total-review-count"]::text',
            '.a-size-base.a-link-normal::text'
        ]
        for selector in review_selectors:
            review_text = response.css(selector).get()
            if review_text and ('ãƒ¬ãƒ“ãƒ¥ãƒ¼' in review_text or 'review' in review_text.lower()):
                review_count = review_text.strip()
                break

        # ç”»åƒãƒ‘ã‚¹ (class="a-dynamic-image a-stretch-vertical")
        image_url = response.css('img.a-dynamic-image.a-stretch-vertical::attr(src)').get()
        if not image_url:
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿
            image_url = response.css('#landingImage::attr(src)').get()

        # ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ›´æ–°
        self.items_count += 1
        self.requests_count += 1
        self.update_progress()

        # ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        item_data = {
            'title': title,
            'rating': rating,
            'price': price,
            'review_count': review_count,
            'image_url': image_url,
            'product_url': response.url,
            'source_page': response.meta.get('page_url', ''),
            'scraped_at': response.headers.get('Date', '').decode('utf-8') if response.headers.get('Date') else ''
        }

        # Richè¡¨ç¤ºã§ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã‚’å‡ºåŠ›ï¼ˆCLIã§ã®ã¿ï¼‰
        if self.rich_enabled and self.console:
            try:
                self.console.print(f"âœ… [green]Item {self.items_count}:[/green] {title[:50]}...")
            except:
                pass

        yield item_data

        # ç›®æ¨™é”æˆæ™‚ã«ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼ã‚’åœæ­¢ï¼ˆCLIã§ã®ã¿ï¼‰
        if self.rich_enabled and self.items_count >= self.target_items and self.progress:
            try:
                self.progress.stop()
                if self.console:
                    self.console.print(f"\nðŸŽ¯ [bold green]Target achieved! Collected {self.items_count} items[/bold green]\n")
            except:
                pass