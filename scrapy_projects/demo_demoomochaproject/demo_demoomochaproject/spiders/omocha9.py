import scrapy
from scrapy import Request
import re


class Omocha9Spider(scrapy.Spider):
    name = "omocha9"
    allowed_domains = ['amazon.co.jp']
    start_urls = [
        'https://www.amazon.co.jp/gp/bestsellers/toys/ref=zg_bs_nav_toys_0'
    ]

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.items_count = 0
        self.requests_count = 0
        self.pages_count = 0

    custom_settings = {
        'DOWNLOAD_DELAY': 3,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        'CONCURRENT_REQUESTS': 1,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 1,
        'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'ROBOTSTXT_OBEY': False,
        'COOKIES_ENABLED': True,
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 10,
        'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0,
        'HTTPCACHE_ENABLED': True,
        'HTTPCACHE_DIR': 'httpcache',
        'HTTPCACHE_EXPIRATION_SECS': 86400,
        'FEED_EXPORT_ENCODING': 'utf-8',
        # å®‰å…¨ã®ãŸã‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ¶é™ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
        # 'CLOSESPIDER_PAGECOUNT': 20,  # ãƒšãƒ¼ã‚¸åˆ¶é™ã‚’ç„¡åŠ¹åŒ–
        'CLOSESPIDER_TIMEOUT': 3600,  # 60åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
        'DEFAULT_REQUEST_HEADERS': {
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ja,en-US;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Cache-Control': 'no-cache',
        }
    }

    def start_requests(self):
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–‹å§‹"""
        self.logger.info("ğŸš€ Starting omocha9 spider - Unlimited mode (no item limit)")

        # é€šå¸¸ã®start_requestsã‚’å®Ÿè¡Œ
        for url in self.start_urls:
            yield scrapy.Request(url, callback=self.parse)



    def parse(self, response):
        """ãŠã‚‚ã¡ã‚ƒãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®è§£æ"""
        self.logger.info(f'Processing page: {response.url}')

        # ãƒšãƒ¼ã‚¸æ•°ã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ã‚’æ›´æ–°
        self.pages_count += 1
        self.requests_count += 1
        self.logger.info(f'ğŸ“Š Progress: Pages: {self.pages_count}, Requests: {self.requests_count}, Items: {self.items_count}')

        # PlaywrightãŒä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if response.meta.get('playwright'):
            self.logger.info(f'ğŸ“œ Processing Playwright-rendered page with scroll')
            # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®ãƒšãƒ¼ã‚¸å‡¦ç†
            yield from self.parse_after_scroll(response)
        else:
            # é€šå¸¸ã®HTTPãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã€Playwrightã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å‡¦ç†ã‚’è¡Œã†
            self.logger.info(f'ğŸ”„ Requesting page with scroll using Playwright...')
            yield Request(
                url=response.url,
                callback=self.parse,
                dont_filter=True,  # é‡è¤‡ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ç„¡åŠ¹åŒ–
                meta={
                    'playwright': True,
                    'playwright_page_methods': [
                        {'method': 'wait_for_load_state', 'args': ['domcontentloaded']},
                        {'method': 'evaluate', 'args': ['''
                            () => {
                                // æ®µéšçš„ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦é…å»¶èª­ã¿è¾¼ã¿ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                                return new Promise((resolve) => {
                                    let totalHeight = 0;
                                    let distance = 100;
                                    let timer = setInterval(() => {
                                        let scrollHeight = document.body.scrollHeight;
                                        window.scrollBy(0, distance);
                                        totalHeight += distance;

                                        if(totalHeight >= scrollHeight){
                                            clearInterval(timer);
                                            // æœ€ä¸‹éƒ¨ã¾ã§ç¢ºå®Ÿã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                                            window.scrollTo(0, document.body.scrollHeight);
                                            setTimeout(() => {
                                                resolve({
                                                    finalHeight: document.body.scrollHeight,
                                                    scrollCompleted: true
                                                });
                                            }, 1000);
                                        }
                                    }, 100);
                                });
                            }
                        ''']},
                        {'method': 'wait_for_timeout', 'args': [3000]}  # è¿½åŠ ã®å¾…æ©Ÿæ™‚é–“
                    ],
                    'page_url': response.url,
                    'page_number': self.pages_count
                }
            )

    def parse_after_scroll(self, response):
        """ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®ãƒšãƒ¼ã‚¸è§£æ"""
        page_number = response.meta.get('page_number', self.pages_count)
        self.logger.info(f'ğŸ“„ Processing scrolled page {page_number}: {response.url}')

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã®å•†å“ãƒªãƒ³ã‚¯ã‚’å–å¾— (è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦è¡Œ)
        product_links = []

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼1: å¾“æ¥ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        links1 = response.css('a.a-link-normal.aok-block::attr(href)').getall()
        product_links.extend(links1)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼2: ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ã®æ–°ã—ã„æ§‹é€ 
        links2 = response.css('div[data-component-type="s-search-result"] h3 a::attr(href)').getall()
        product_links.extend(links2)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼3: ä¸€èˆ¬çš„ãªå•†å“ãƒªãƒ³ã‚¯
        links3 = response.css('a[href*="/dp/"]::attr(href)').getall()
        product_links.extend(links3)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼4: ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ç”¨
        links4 = response.css('div.zg-item-immersion a::attr(href)').getall()
        product_links.extend(links4)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼5: æ–°ã—ã„Amazonãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼æ§‹é€ 
        links5 = response.css('div.zg-grid-general-faceout a::attr(href)').getall()
        product_links.extend(links5)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼6: ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚¢ã‚¤ãƒ†ãƒ 
        links6 = response.css('span.zg-item a::attr(href)').getall()
        product_links.extend(links6)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼7: å•†å“ã‚«ãƒ¼ãƒ‰
        links7 = response.css('div[data-asin] h3 a::attr(href)').getall()
        product_links.extend(links7)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼8: ã‚ˆã‚Šåºƒç¯„å›²ãªASINå•†å“
        links8 = response.css('div[data-asin] a::attr(href)').getall()
        product_links.extend(links8)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼9: ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ã‚¢ã‚¤ãƒ†ãƒ 
        links9 = response.css('div.zg-item a::attr(href)').getall()
        product_links.extend(links9)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼10: å…¨ã¦ã®å•†å“ãƒªãƒ³ã‚¯ï¼ˆã‚ˆã‚ŠåŒ…æ‹¬çš„ï¼‰
        links10 = response.css('a[href*="amazon.co.jp"][href*="/dp/"]::attr(href)').getall()
        product_links.extend(links10)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼11: ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œã«è¡¨ç¤ºã•ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹è¿½åŠ å•†å“
        links11 = response.css('div.s-result-item a[href*="/dp/"]::attr(href)').getall()
        product_links.extend(links11)

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼12: é…å»¶èª­ã¿è¾¼ã¿ã•ã‚ŒãŸå•†å“
        links12 = response.css('div[data-component-type] a[href*="/dp/"]::attr(href)').getall()
        product_links.extend(links12)

        # é‡è¤‡ã‚’å‰Šé™¤
        product_links = list(set(product_links))

        # å•†å“ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_links = []
        for link in product_links:
            if link and '/dp/' in link:
                filtered_links.append(link)

        product_links = filtered_links  # åˆ¶é™ã‚’æ’¤å»ƒã—ã¦å…¨ã¦ã®å•†å“ãƒªãƒ³ã‚¯ã‚’å‡¦ç†

        self.logger.info(f'ğŸ” Found {len(product_links)} unique product links on scrolled page {page_number}')
        self.logger.info(f'ğŸ“ Sample links: {product_links[:3]}')

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼åˆ¥ã®çµæœã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œï¼‰
        self.logger.debug(f'Scroll Selector results - Links1: {len(links1)}, Links2: {len(links2)}, Links3: {len(links3)}, Links4: {len(links4)}, Links5: {len(links5)}, Links6: {len(links6)}, Links7: {len(links7)}, Links8: {len(links8)}, Links9: {len(links9)}, Links10: {len(links10)}, Links11: {len(links11)}, Links12: {len(links12)}')

        # ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«åŠ¹æœã®ç¢ºèª
        total_before_scroll = len(links1) + len(links2) + len(links3) + len(links4) + len(links5) + len(links6) + len(links7) + len(links8) + len(links9) + len(links10)
        total_after_scroll = len(links11) + len(links12)
        self.logger.info(f'ğŸ“Š Scroll impact: Before scroll selectors: {total_before_scroll}, After scroll selectors: {total_after_scroll}, Total unique: {len(product_links)}')

        # ãƒ‡ãƒãƒƒã‚°: ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ç¢ºèª
        if len(product_links) == 0:
            self.logger.warning("No product links found. Debugging page content...")
            # ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«ã‚’ç¢ºèª
            page_title = response.css('title::text').get()
            self.logger.debug(f'Page title: {page_title}')

            # å…¨ã¦ã®ãƒªãƒ³ã‚¯ã‚’ç¢ºèª
            all_links = response.css('a::attr(href)').getall()
            dp_links = [link for link in all_links if link and '/dp/' in link]
            self.logger.debug(f'Total links: {len(all_links)}, DP links: {len(dp_links)}')

            # ãƒšãƒ¼ã‚¸ã®ä¸»è¦ãªè¦ç´ ã‚’ç¢ºèª
            main_content = response.css('body').get()
            if main_content and len(main_content) < 1000:
                self.logger.warning("Page content seems too small, possible blocking or redirect")

            # ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒšãƒ¼ã‚¸å†…å®¹ã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            try:
                with open('debug_page.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                self.logger.info("Page content saved to debug_page.html for inspection")
            except Exception as e:
                self.logger.warning(f"Failed to save debug page: {e}")

            # å¯èƒ½æ€§ã®ã‚ã‚‹ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ
            test_selectors = [
                'div.zg-item-immersion',
                'div[data-component-type="s-search-result"]',
                'div.s-result-item',
                'div.a-section.a-spacing-none',
                'span.zg-item'
            ]
            for selector in test_selectors:
                elements = response.css(selector)
                self.logger.debug(f'Selector "{selector}": {len(elements)} elements found')

        # å„å•†å“ãƒšãƒ¼ã‚¸ã‚’å‡¦ç†ï¼ˆã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«å¾Œï¼‰
        self.logger.info(f'ğŸš€ Processing {len(product_links)} product links from scrolled page {page_number}')
        for i, link in enumerate(product_links, 1):
            if link:
                # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                full_url = response.urljoin(link)
                self.logger.debug(f'ğŸ“¦ Queuing product {i}/{len(product_links)}: {full_url}')
                yield Request(
                    url=full_url,
                    callback=self.parse_product,
                    meta={'page_url': response.url, 'product_index': i, 'scrolled': True}
                )

        # ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³ãƒªãƒ³ã‚¯ã‚’å–å¾—ï¼ˆå†å¸°çš„ã«å·¡ã‚‹ï¼‰
        next_page_links = []

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼1: æ¨™æº–çš„ãªæ¬¡ãƒšãƒ¼ã‚¸ãƒªãƒ³ã‚¯
        next_page_links.extend(response.css('a[aria-label="æ¬¡ã®ãƒšãƒ¼ã‚¸ã«ç§»å‹•"]::attr(href)').getall())

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼2: æ¤œç´¢çµæœãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        next_page_links.extend(response.css('a.s-pagination-next::attr(href)').getall())

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼3: ä¸€èˆ¬çš„ãªãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        next_page_links.extend(response.css('li.a-last a::attr(href)').getall())

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼4: ãƒ™ã‚¹ãƒˆã‚»ãƒ©ãƒ¼ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        next_page_links.extend(response.css('a[aria-label*="æ¬¡"]::attr(href)').getall())

        # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼5: æ•°å­—ãƒšãƒ¼ã‚¸ãƒãƒ¼ã‚·ãƒ§ãƒ³
        next_page_links.extend(response.css('a[aria-label*="ãƒšãƒ¼ã‚¸"]::attr(href)').getall())

        # é‡è¤‡ã‚’å‰Šé™¤
        next_page_links = list(set(next_page_links))

        self.logger.info(f'ğŸ”— Found {len(next_page_links)} potential next page links')

        for next_link in next_page_links:
            if next_link:
                next_url = response.urljoin(next_link)
                self.logger.info(f'â¡ï¸  Following next page: {next_url}')
                yield Request(
                    url=next_url,
                    callback=self.parse,
                    meta={'page_url': response.url, 'from_scrolled_page': True}
                )

    def parse_product(self, response):
        """å•†å“è©³ç´°ãƒšãƒ¼ã‚¸ã®è§£æ"""
        product_index = response.meta.get('product_index', '?')
        scrolled = response.meta.get('scrolled', False)
        scroll_indicator = 'ğŸ“œ' if scrolled else 'ğŸ“„'
        self.logger.info(f'{scroll_indicator} Processing product {product_index}: {response.url}')

        # å•†å“ã‚¿ã‚¤ãƒˆãƒ« (id="title")
        title = response.css('#title span::text').get()
        if not title:
            title = response.css('#title::text').get()
        if title:
            title = title.strip()

        # è©•ä¾¡ã‚’å–å¾—
        rating = None
        rating_text = response.css('span.a-icon-alt::text').get()
        if rating_text:
            rating_match = re.search(r'(\d+\.?\d*)', rating_text)
            if rating_match:
                rating = rating_match.group(1)

        # ç¨è¾¼ä¾¡æ ¼ã‚’å–å¾—
        price = None
        price_selectors = [
            '.a-price-whole::text',
            '.a-price .a-offscreen::text',
            '.a-price-current .a-offscreen::text',
            '.a-price-range .a-offscreen::text'
        ]
        for selector in price_selectors:
            price_text = response.css(selector).get()
            if price_text:
                price = price_text.strip()
                break

        # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°ã‚’å–å¾—
        review_count = None
        review_selectors = [
            '#acrCustomerReviewText::text',
            'span[data-hook="total-review-count"]::text',
            '.a-size-base.a-link-normal::text'
        ]
        for selector in review_selectors:
            review_text = response.css(selector).get()
            if review_text and ('ãƒ¬ãƒ“ãƒ¥ãƒ¼' in review_text or 'review' in review_text.lower()):
                review_count = review_text.strip()
                break

        # ç”»åƒãƒ‘ã‚¹ (class="a-dynamic-image a-stretch-vertical")
        image_url = response.css('img.a-dynamic-image.a-stretch-vertical::attr(src)').get()
        if not image_url:
            # ä»£æ›¿ã‚»ãƒ¬ã‚¯ã‚¿
            image_url = response.css('#landingImage::attr(src)').get()

        # ã‚¢ã‚¤ãƒ†ãƒ æ•°ã‚’æ›´æ–°
        self.items_count += 1
        self.requests_count += 1

        # ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        item_data = {
            'title': title,
            'rating': rating,
            'price': price,
            'review_count': review_count,
            'image_url': image_url,
            'product_url': response.url,
            'source_page': response.meta.get('page_url', ''),
            'scraped_at': response.headers.get('Date', '').decode('utf-8') if response.headers.get('Date') else ''
        }

        # ãƒ­ã‚°ã§ã‚¢ã‚¤ãƒ†ãƒ æƒ…å ±ã‚’å‡ºåŠ›
        title_display = title[:50] + "..." if title and len(title) > 50 else (title or "No title")
        self.logger.info(f"âœ… Item {self.items_count}: {title_display} - Price: {price or 'N/A'} - Rating: {rating or 'N/A'}")

        yield item_data