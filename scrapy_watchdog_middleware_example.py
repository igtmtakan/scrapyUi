#!/usr/bin/env python3
"""
ScrapyãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã§watchdogç›£è¦–ã‚’è‡ªå‹•é–‹å§‹ã™ã‚‹ä¾‹
settings.pyã®DOWNLOADER_MIDDLEWARESã«è¿½åŠ 
"""
import asyncio
import threading
import os
import sys
from pathlib import Path
from scrapy import signals
from scrapy.exceptions import NotConfigured

class WatchdogMonitoringMiddleware:
    """watchdogç›£è¦–ã‚’è‡ªå‹•é–‹å§‹ã™ã‚‹ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢"""
    
    def __init__(self, crawler):
        self.crawler = crawler
        self.settings = crawler.settings
        self.monitor = None
        self.monitor_thread = None
        
        # ç›£è¦–è¨­å®šã‚’å–å¾—
        self.enable_watchdog = self.settings.getbool('WATCHDOG_MONITORING_ENABLED', False)
        self.task_id = self.settings.get('WATCHDOG_TASK_ID', f"auto_{int(time.time())}")
        self.db_path = self.settings.get('WATCHDOG_DB_PATH', 'backend/database/scrapy_ui.db')
        self.output_file = self.settings.get('FEED_URI', None)
        
        if not self.enable_watchdog:
            raise NotConfigured('Watchdog monitoring is disabled')
        
        if not self.output_file:
            raise NotConfigured('FEED_URI must be set for watchdog monitoring')
        
        print(f"ğŸ” Watchdog monitoring middleware initialized")
        print(f"   Task ID: {self.task_id}")
        print(f"   Output file: {self.output_file}")
        print(f"   DB path: {self.db_path}")

    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

    def spider_opened(self, spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼é–‹å§‹æ™‚ã«watchdogç›£è¦–ã‚’é–‹å§‹"""
        print(f"ğŸš€ Starting watchdog monitoring for spider: {spider.name}")
        
        try:
            # watchdogç›£è¦–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
            self.monitor_thread = threading.Thread(
                target=self._start_monitoring_thread,
                args=(spider.name,),
                daemon=True
            )
            self.monitor_thread.start()
            
            print(f"âœ… Watchdog monitoring thread started")
            
        except Exception as e:
            print(f"âŒ Failed to start watchdog monitoring: {e}")

    def spider_closed(self, spider):
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼çµ‚äº†æ™‚ã«ç›£è¦–ã‚’åœæ­¢"""
        print(f"ğŸ›‘ Stopping watchdog monitoring for spider: {spider.name}")
        
        if self.monitor:
            self.monitor.stop_monitoring()

    def _start_monitoring_thread(self, spider_name):
        """ç›£è¦–ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹"""
        try:
            # watchdogç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'backend', 'app', 'services'))
            from scrapy_watchdog_monitor import ScrapyWatchdogMonitor
            
            # ç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
            self.monitor = ScrapyWatchdogMonitor(
                task_id=self.task_id,
                project_path=os.getcwd(),
                spider_name=spider_name,
                db_path=self.db_path
            )
            
            # éåŒæœŸç›£è¦–ã‚’é–‹å§‹
            asyncio.run(self.monitor._start_jsonl_monitoring(self.output_file))
            
        except Exception as e:
            print(f"âŒ Watchdog monitoring thread error: {e}")


# settings.pyã«è¿½åŠ ã™ã‚‹è¨­å®šä¾‹:
"""
# Watchdogç›£è¦–ã‚’æœ‰åŠ¹åŒ–
WATCHDOG_MONITORING_ENABLED = True
WATCHDOG_TASK_ID = 'custom_task_123'
WATCHDOG_DB_PATH = 'backend/database/scrapy_ui.db'

# ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã‚’è¿½åŠ 
DOWNLOADER_MIDDLEWARES = {
    'myproject.middlewares.WatchdogMonitoringMiddleware': 543,
}

# JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã‚’è¨­å®š
FEED_URI = 'results_%(time)s.jsonl'
FEED_FORMAT = 'jsonlines'
"""
