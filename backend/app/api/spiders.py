from fastapi import APIRouter, Depends, HTTPException, status
from sqlalchemy.orm import Session
from typing import List
import uuid

from ..database import get_db, Spider as DBSpider, Project as DBProject
from ..models.schemas import Spider, SpiderCreate, SpiderUpdate
from ..services.scrapy_service import ScrapyPlaywrightService
from ..services.integrity_service import integrity_service
from .auth import get_current_active_user

router = APIRouter(
    responses={
        404: {"description": "Not found"},
        400: {"description": "Bad request"},
        500: {"description": "Internal server error"}
    }
)

@router.get(
    "/",
    response_model=List[Spider],
    summary="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä¸€è¦§å–å¾—",
    description="æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯å…¨ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆ"
)
async def get_spiders(project_id: str = None, db: Session = Depends(get_db)):
    """
    ## ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä¸€è¦§å–å¾—

    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯å…¨ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **project_id** (optional): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    query = db.query(DBSpider)
    if project_id:
        query = query.filter(DBSpider.project_id == project_id)

    spiders = query.all()
    return spiders

@router.get("/{spider_id}", response_model=Spider)
async def get_spider(spider_id: str, db: Session = Depends(get_db)):
    """ç‰¹å®šã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
    spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )
    return spider

def update_spider_name_in_code(code: str, spider_name: str) -> str:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®name=ã¨ã‚¯ãƒ©ã‚¹åã‚’æ›´æ–°ã™ã‚‹ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚’ä¿æŒï¼‰"""
    import re

    if not code:
        return code

    updated_code = code

    # 1. name = "..." ã¾ãŸã¯ name = '...' ã®å½¢å¼ã‚’æ¤œç´¢ã—ã¦ç½®æ›
    name_patterns = [
        r'name\s*=\s*["\'][^"\']*["\']',  # name = "old_name" ã¾ãŸã¯ name = 'old_name'
        r'name\s*=\s*"[^"]*"',           # name = "old_name"
        r"name\s*=\s*'[^']*'"            # name = 'old_name'
    ]

    name_updated = False
    for pattern in name_patterns:
        if re.search(pattern, updated_code):
            updated_code = re.sub(pattern, f'name = "{spider_name}"', updated_code)
            name_updated = True
            break

    # name = ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã€ã‚¯ãƒ©ã‚¹å®šç¾©ã®å¾Œã«è¿½åŠ 
    if not name_updated:
        class_pattern = r'(class\s+\w+.*?Spider.*?:)'
        if re.search(class_pattern, updated_code):
            updated_code = re.sub(class_pattern, f'\\1\n    name = "{spider_name}"', updated_code)

    # 2. ã‚¯ãƒ©ã‚¹åã‚’æ›´æ–°ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚’ç¢ºå®Ÿã«ä¿æŒï¼‰
    # ã‚ˆã‚Šè©³ç´°ãªã‚¯ãƒ©ã‚¹å®šç¾©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¤œå‡º
    class_patterns = [
        # ç¶™æ‰¿ã‚ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        r'class\s+(\w+)(\([^)]+\)):\s*',  # class ClassName(scrapy.Spider):
        # ç¶™æ‰¿ãªã—ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆã“ã‚Œã‚’ä¿®æ­£ã™ã‚‹ï¼‰
        r'class\s+(\w+):\s*',             # class ClassName:
    ]

    class_match = None
    inheritance_part = ""

    # ç¶™æ‰¿ã‚ã‚Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€åˆã«ãƒã‚§ãƒƒã‚¯
    for pattern in class_patterns:
        class_match = re.search(pattern, updated_code)
        if class_match:
            if len(class_match.groups()) > 1:
                inheritance_part = class_match.group(2)  # (scrapy.Spider) éƒ¨åˆ†
            break

    if class_match:
        original_class_name = class_match.group(1)

        # æ–°ã—ã„ã‚¯ãƒ©ã‚¹åã‚’ç”Ÿæˆï¼ˆCamelCaseï¼‰
        new_class_name = ''.join(word.capitalize() for word in spider_name.replace('_', ' ').replace('-', ' ').split())
        if not new_class_name.endswith('Spider'):
            new_class_name += 'Spider'

        # ç¶™æ‰¿é–¢ä¿‚ã‚’ç¢ºå®Ÿã«ä¿æŒ
        if not inheritance_part:
            # ç¶™æ‰¿ãŒãªã„å ´åˆã¯ scrapy.Spider ã‚’è¿½åŠ 
            inheritance_part = "(scrapy.Spider)"
            print(f"ğŸ”§ Added missing inheritance: scrapy.Spider")

        # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’ç½®æ›
        old_class_pattern = re.escape(class_match.group(0))
        new_class_definition = f'class {new_class_name}{inheritance_part}:\n'
        updated_code = re.sub(old_class_pattern, new_class_definition, updated_code)

        print(f"ğŸ”„ Updated class: {original_class_name} -> {new_class_name}{inheritance_part}")

    # 3. scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ç¢ºèªãƒ»è¿½åŠ 
    if 'import scrapy' not in updated_code and 'from scrapy' not in updated_code:
        # scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒãªã„å ´åˆã¯è¿½åŠ 
        import_lines = []
        if 'import scrapy' not in updated_code:
            import_lines.append('import scrapy')

        if import_lines:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®å…ˆé ­ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’è¿½åŠ 
            updated_code = '\n'.join(import_lines) + '\n' + updated_code
            print(f"ğŸ”§ Added missing imports: {', '.join(import_lines)}")

    return updated_code

def validate_spider_inheritance(code: str) -> dict:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®ç¶™æ‰¿é–¢ä¿‚ã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹"""
    import re

    validation_result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "fixes_applied": []
    }

    if not code:
        validation_result["valid"] = False
        validation_result["errors"].append("ã‚³ãƒ¼ãƒ‰ãŒç©ºã§ã™")
        return validation_result

    # 1. scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    has_scrapy_import = 'import scrapy' in code or 'from scrapy' in code
    if not has_scrapy_import:
        validation_result["warnings"].append("scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # 2. ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’ãƒã‚§ãƒƒã‚¯
    class_patterns = [
        r'class\s+(\w+)\(scrapy\.Spider\):\s*',  # class ClassName(scrapy.Spider):
        r'class\s+(\w+)\(Spider\):\s*',          # class ClassName(Spider):
        r'class\s+(\w+):\s*',                    # class ClassName: (ç¶™æ‰¿ãªã—)
    ]

    class_found = False
    has_inheritance = False
    class_name = None

    for i, pattern in enumerate(class_patterns):
        match = re.search(pattern, code)
        if match:
            class_found = True
            class_name = match.group(1)
            if i < 2:  # æœ€åˆã®2ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ç¶™æ‰¿ã‚ã‚Š
                has_inheritance = True
            break

    if not class_found:
        validation_result["valid"] = False
        validation_result["errors"].append("ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    elif not has_inheritance:
        validation_result["warnings"].append(f"ã‚¯ãƒ©ã‚¹ '{class_name}' ãŒ scrapy.Spider ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã›ã‚“")

    # 3. name å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    name_pattern = r'name\s*=\s*["\'][^"\']+["\']'
    if not re.search(name_pattern, code):
        validation_result["warnings"].append("name å±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # 4. parse ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
    parse_pattern = r'def\s+parse\s*\('
    if not re.search(parse_pattern, code):
        validation_result["warnings"].append("parse ãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return validation_result

def update_project_imports_in_code(code: str, old_project_name: str, new_project_name: str) -> str:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’æ›´æ–°ã™ã‚‹"""
    import re

    if not code or old_project_name == new_project_name:
        return code

    updated_code = code

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’æ¤œç´¢ã—ã¦ç½®æ›
    # ä¾‹: from old_project.items import -> from new_project.items import
    import_patterns = [
        rf'from\s+{re.escape(old_project_name)}\.(\w+)\s+import',
        rf'import\s+{re.escape(old_project_name)}\.(\w+)',
    ]

    for pattern in import_patterns:
        matches = re.finditer(pattern, updated_code)
        for match in matches:
            old_import = match.group(0)
            if 'from' in old_import:
                # from old_project.module import -> from new_project.module import
                new_import = re.sub(rf'{re.escape(old_project_name)}\.', f'{new_project_name}.', old_import)
            else:
                # import old_project.module -> import new_project.module
                new_import = re.sub(rf'{re.escape(old_project_name)}\.', f'{new_project_name}.', old_import)

            updated_code = updated_code.replace(old_import, new_import)
            print(f"ğŸ”„ Updated import: {old_import} -> {new_import}")

    return updated_code

@router.post("/", response_model=Spider, status_code=status.HTTP_201_CREATED)
async def create_spider(
    spider: SpiderCreate,
    db: Session = Depends(get_db)
    # current_user = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""

    print(f"DEBUG: Received spider creation request:")
    print(f"  name: {spider.name}")
    print(f"  project_id: {spider.project_id}")
    print(f"  template: {spider.template}")
    print(f"  code_length: {len(spider.code) if spider.code else 0}")
    print(f"  settings: {spider.settings}")

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
    project = db.query(DBProject).filter(DBProject.id == spider.project_id).first()
    if not project:
        print(f"DEBUG: Project not found: {spider.project_id}")
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == spider.project_id,
        DBSpider.name == spider.name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider with this name already exists in the project"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®name=ã‚’ç¢ºå®Ÿã«æ›´æ–°ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚‚ä¿æŒï¼‰
    updated_code = update_spider_name_in_code(spider.code, spider.name)
    print(f"DEBUG: Updated spider code name to: {spider.name}")
    print(f"DEBUG: Ensured scrapy.Spider inheritance")

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    validation_result = validate_spider_inheritance(updated_code)
    if validation_result["warnings"]:
        print(f"DEBUG: Validation warnings: {validation_result['warnings']}")
    if not validation_result["valid"]:
        print(f"DEBUG: Validation errors: {validation_result['errors']}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider code validation failed: {'; '.join(validation_result['errors'])}"
        )

    # ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼IDã‚’ä½¿ç”¨
    user_id = "admin-user-id"
    print(f"DEBUG: Using default user_id = {user_id}")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    scrapy_service = ScrapyPlaywrightService()
    spider_file_path = scrapy_service.base_projects_dir / project.path / project.path / "spiders" / f"{spider.name}.py"
    if spider_file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider file '{spider.name}.py' already exists in filesystem"
        )

    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
    db_spider = None
    file_created = False

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        db_spider = DBSpider(
            id=str(uuid.uuid4()),
            name=spider.name,
            code=updated_code,
            template=spider.template,
            settings=spider.settings,
            project_id=spider.project_id,
            user_id=user_id
        )

        db.add(db_spider)
        db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«flushï¼ˆcommitã¯ã¾ã ã—ãªã„ï¼‰

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
        scrapy_service.save_spider_code(project.path, spider.name, updated_code)
        file_created = True

        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if not spider_file_path.exists():
            raise Exception("File was not created successfully")

        # 4. å…¨ã¦æˆåŠŸã—ãŸå ´åˆã®ã¿commit
        db.commit()
        db.refresh(db_spider)

        print(f"âœ… Spider created successfully: {spider.name}")
        return db_spider

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"âŒ Error creating spider: {str(e)}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if db_spider:
            db.rollback()

        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if file_created and spider_file_path.exists():
            try:
                spider_file_path.unlink()
                print(f"ğŸ—‘ï¸ Cleaned up file: {spider_file_path}")
            except Exception as cleanup_error:
                print(f"âš ï¸ Failed to cleanup file: {cleanup_error}")

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create spider: {str(e)}"
        )

@router.put("/{spider_id}", response_model=Spider)
async def update_spider(
    spider_id: str,
    spider_update: SpiderUpdate,
    db: Session = Depends(get_db)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’æ›´æ–°"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®é©ç”¨
    update_data = spider_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_spider, field, value)

    db.commit()
    db.refresh(db_spider)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°
    if 'code' in update_data:
        try:
            scrapy_service = ScrapyPlaywrightService()
            scrapy_service.save_spider_code(project.path, db_spider.name, db_spider.code)
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to update spider file: {str(e)}"
            )

    return db_spider

@router.delete("/{spider_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_spider(spider_id: str, db: Session = Depends(get_db)):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å‰Šé™¤"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        scrapy_service = ScrapyPlaywrightService()
        spider_file_path = scrapy_service.base_projects_dir / project.path / "spiders" / f"{db_spider.name}.py"
        if spider_file_path.exists():
            spider_file_path.unlink()
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã¯å‰Šé™¤ã™ã‚‹
        pass

    db.delete(db_spider)
    db.commit()

    return None

@router.get("/{spider_id}/code")
async def get_spider_code(spider_id: str, db: Session = Depends(get_db)):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    try:
        scrapy_service = ScrapyPlaywrightService()
        code = scrapy_service.get_spider_code(project.path, db_spider.name)
        return {"code": code}
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
        return {"code": db_spider.code}

@router.post("/{spider_id}/save")
async def save_spider_code(
    spider_id: str,
    code_data: dict,
    db: Session = Depends(get_db)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    code = code_data.get("code", "")
    if not code:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Code is required"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
    db_spider.code = code
    db.commit()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
    try:
        scrapy_service = ScrapyPlaywrightService()
        scrapy_service.save_spider_code(project.path, db_spider.name, code)
        return {"message": "Code saved successfully"}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save spider code: {str(e)}"
        )

@router.post("/{spider_id}/validate")
async def validate_spider_code(spider_id: str, code_data: dict, db: Session = Depends(get_db)):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
    code = code_data.get("code", "")
    if not code:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Code is required"
        )

    try:
        scrapy_service = ScrapyPlaywrightService()
        result = scrapy_service.validate_spider_code(code)
        return result
    except Exception as e:
        return {
            "valid": False,
            "errors": [f"Validation error: {str(e)}"]
        }

@router.get(
    "/integrity/check",
    summary="æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚"
)
async def check_integrity():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        result = integrity_service.check_integrity()
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Integrity check failed: {str(e)}"
        )

@router.post(
    "/integrity/fix",
    summary="æ•´åˆæ€§ä¿®å¾©",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ä¿®å¾©ã—ã¾ã™ã€‚"
)
async def fix_integrity(auto_fix: bool = False):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ä¿®å¾©"""
    try:
        result = integrity_service.fix_integrity_issues(auto_fix=auto_fix)
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Integrity fix failed: {str(e)}"
        )

@router.get(
    "/integrity/report",
    summary="æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆ",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_integrity_report():
    """æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
    try:
        report = integrity_service.generate_integrity_report()
        return {"report": report}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Report generation failed: {str(e)}"
        )

@router.post(
    "/{spider_id}/copy",
    response_model=Spider,
    status_code=status.HTTP_201_CREATED,
    summary="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ”ãƒ¼",
    description="æ—¢å­˜ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚æ•´åˆæ€§ä¿è¨¼ä»˜ãã€‚"
)
async def copy_spider(
    spider_id: str,
    copy_data: dict,
    db: Session = Depends(get_db)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ•´åˆæ€§ä¿è¨¼ä»˜ãï¼‰"""

    new_name = copy_data.get("name", "").strip()
    if not new_name:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="New spider name is required"
        )

    # å…ƒã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
    original_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not original_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Original spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == original_spider.project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == original_spider.project_id,
        DBSpider.name == new_name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider with name '{new_name}' already exists in this project"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    scrapy_service = ScrapyPlaywrightService()
    new_spider_file_path = scrapy_service.base_projects_dir / project.path / project.path / "spiders" / f"{new_name}.py"
    if new_spider_file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider file '{new_name}.py' already exists in filesystem"
        )

    # ã‚³ãƒ¼ãƒ‰å†…ã®name=ã¨ã‚¯ãƒ©ã‚¹åã‚’æ–°ã—ã„åå‰ã«æ›´æ–°ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚‚ä¿æŒï¼‰
    updated_code = update_spider_name_in_code(original_spider.code, new_name)
    print(f"DEBUG: Updated copied spider code name to: {new_name}")
    print(f"DEBUG: Ensured scrapy.Spider inheritance in copied spider")

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ
    validation_result = validate_spider_inheritance(updated_code)
    if validation_result["warnings"]:
        print(f"DEBUG: Copy validation warnings: {validation_result['warnings']}")
    if not validation_result["valid"]:
        print(f"DEBUG: Copy validation errors: {validation_result['errors']}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Copied spider code validation failed: {'; '.join(validation_result['errors'])}"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã§ã®ã‚³ãƒ”ãƒ¼ã®å ´åˆã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚‚æ›´æ–°
    # ï¼ˆç¾åœ¨ã¯åŒä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ã‚³ãƒ”ãƒ¼ã®ã¿ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãŒã€å°†æ¥ã®æ‹¡å¼µã®ãŸã‚ï¼‰
    # updated_code = update_project_imports_in_code(updated_code, old_project_name, new_project_name)

    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
    new_spider = None
    file_created = False

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
        new_spider = DBSpider(
            id=str(uuid.uuid4()),
            name=new_name,
            code=updated_code,
            template=original_spider.template,
            settings=original_spider.settings,
            project_id=original_spider.project_id,
            user_id=original_spider.user_id
        )

        db.add(new_spider)
        db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«flushï¼ˆcommitã¯ã¾ã ã—ãªã„ï¼‰

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
        scrapy_service.save_spider_code(project.path, new_name, updated_code)
        file_created = True

        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if not new_spider_file_path.exists():
            raise Exception("File was not created successfully")

        # 4. å…¨ã¦æˆåŠŸã—ãŸå ´åˆã®ã¿commit
        db.commit()
        db.refresh(new_spider)

        print(f"âœ… Spider copied successfully: {original_spider.name} -> {new_name}")

        # 5. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
        integrity_result = integrity_service.check_integrity()
        if not integrity_result['summary']['integrity_ok']:
            print(f"âš ï¸ Integrity check failed after copy operation")

        return new_spider

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"âŒ Error copying spider: {str(e)}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if new_spider:
            db.rollback()

        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if file_created and new_spider_file_path.exists():
            try:
                new_spider_file_path.unlink()
                print(f"ğŸ—‘ï¸ Cleaned up file: {new_spider_file_path}")
            except Exception as cleanup_error:
                print(f"âš ï¸ Failed to cleanup file: {cleanup_error}")

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to copy spider: {str(e)}"
        )
