#!/usr/bin/env python3
"""
Scrapyã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒãƒ³ãƒ‰: crawlwithwatchdog
watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ

ä½¿ç”¨ä¾‹:
scrapy crawlwithwatchdog spider_name -o results.jsonl --task-id=test_123
"""
import asyncio
import threading
import time
import uuid
import json
import sqlite3
import os
import sys
from pathlib import Path
from datetime import datetime
from scrapy.commands import ScrapyCommand
from scrapy.utils.conf import arglist_to_dict
from scrapy.exceptions import UsageError

# watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    from watchdog.observers import Observer
    from watchdog.events import FileSystemEventHandler
    WATCHDOG_AVAILABLE = True
except ImportError:
    WATCHDOG_AVAILABLE = False


class JSONLWatchdogHandler(FileSystemEventHandler):
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«å°‚ç”¨ã®watchdogã‚¤ãƒ™ãƒ³ãƒˆãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    
    def __init__(self, monitor):
        self.monitor = monitor
        
    def on_modified(self, event):
        """ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ™‚ã®å‡¦ç†"""
        if event.is_directory:
            return
            
        # ç›£è¦–å¯¾è±¡ã®JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if event.src_path == str(self.monitor.jsonl_file_path):
            # éåŒæœŸå‡¦ç†ã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«å®Ÿè¡Œ
            threading.Thread(
                target=self.monitor.process_new_lines,
                daemon=True
            ).start()


class JSONLMonitor:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, task_id, spider_name, jsonl_file_path, db_path):
        self.task_id = task_id
        self.spider_name = spider_name
        self.jsonl_file_path = Path(jsonl_file_path)
        self.db_path = db_path
        self.processed_lines = 0
        self.last_file_size = 0
        self.is_monitoring = False
        self.observer = None
        
    def start_monitoring(self):
        """watchdogç›£è¦–ã‚’é–‹å§‹"""
        if not WATCHDOG_AVAILABLE:
            print("âš ï¸ watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            self._start_polling_monitoring()
            return
        
        self.is_monitoring = True
        
        # watchdogç›£è¦–ã‚’é–‹å§‹
        event_handler = JSONLWatchdogHandler(self)
        self.observer = Observer()
        self.observer.schedule(event_handler, str(self.jsonl_file_path.parent), recursive=False)
        self.observer.start()
        
        print(f"ğŸ” watchdogç›£è¦–é–‹å§‹: {self.jsonl_file_path}")
        
    def _start_polling_monitoring(self):
        """ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        self.is_monitoring = True
        
        def polling_loop():
            while self.is_monitoring:
                self.process_new_lines()
                time.sleep(1)
        
        polling_thread = threading.Thread(target=polling_loop, daemon=True)
        polling_thread.start()
        
        print(f"ğŸ”„ ãƒãƒ¼ãƒªãƒ³ã‚°ç›£è¦–é–‹å§‹: {self.jsonl_file_path}")
    
    def stop_monitoring(self):
        """ç›£è¦–ã‚’åœæ­¢"""
        self.is_monitoring = False
        
        if self.observer:
            self.observer.stop()
            self.observer.join()
            self.observer = None
        
        print(f"ğŸ›‘ ç›£è¦–åœæ­¢: å‡¦ç†æ¸ˆã¿è¡Œæ•° {self.processed_lines}")
    
    def process_new_lines(self):
        """æ–°ã—ã„è¡Œã‚’å‡¦ç†"""
        try:
            if not self.jsonl_file_path.exists():
                return
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’ãƒã‚§ãƒƒã‚¯
            current_size = self.jsonl_file_path.stat().st_size
            if current_size <= self.last_file_size:
                return
            
            # æ–°ã—ã„éƒ¨åˆ†ã®ã¿èª­ã¿å–ã‚Š
            with open(self.jsonl_file_path, 'r', encoding='utf-8') as f:
                f.seek(self.last_file_size)
                new_content = f.read()
            
            # æ–°ã—ã„è¡Œã‚’å‡¦ç†
            new_lines = [line.strip() for line in new_content.split('\n') if line.strip()]
            
            if new_lines:
                print(f"ğŸ“ æ–°ã—ã„è¡Œã‚’æ¤œå‡º: {len(new_lines)}ä»¶ï¼ˆãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã§å‡¦ç†äºˆå®šï¼‰")
                self.processed_lines += len(new_lines)
                print(f"ğŸ“Š ç·å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•°: {self.processed_lines}")
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã‚’æ›´æ–°
            self.last_file_size = current_size
            
        except Exception as e:
            print(f"âŒ æ–°ã—ã„è¡Œå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # å€‹åˆ¥ã‚¤ãƒ³ã‚µãƒ¼ãƒˆå‡¦ç†ã¯å‰Šé™¤ - ãƒãƒ«ã‚¯ã‚¤ãƒ³ã‚µãƒ¼ãƒˆã®ã¿ä½¿ç”¨


class Command(ScrapyCommand):
    """watchdogç›£è¦–ä»˜ãcrawlã‚³ãƒãƒ³ãƒ‰"""
    
    requires_project = True
    default_settings = {'LOG_LEVEL': 'INFO'}

    def syntax(self):
        return "<spider> [options]"

    def short_desc(self):
        return "Run a spider with watchdog monitoring for real-time DB insertion"

    def long_desc(self):
        return """
Run a spider with watchdog monitoring that automatically inserts
scraped items into the database in real-time as they are written
to the JSONL output file.

Examples:
  scrapy crawlwithwatchdog myspider -o results.jsonl --task-id=test_123
  scrapy crawlwithwatchdog myspider -o results.jsonl --db-path=/path/to/db.sqlite
        """

    def add_options(self, parser):
        ScrapyCommand.add_options(self, parser)
        parser.add_argument("-o", "--output", dest="output",
                           help="dump scraped items to JSONL file (required for watchdog monitoring)")
        parser.add_argument("-t", "--output-format", dest="output_format", default="jsonlines",
                           help="format to use for dumping items (default: jsonlines)")
        parser.add_argument("--task-id", dest="task_id",
                           help="task ID for monitoring (auto-generated if not provided)")
        parser.add_argument("--db-path", dest="db_path",
                           default=None,
                           help="database path for storing results (auto-detected from config if not specified)")

    def process_options(self, args, opts):
        ScrapyCommand.process_options(self, args, opts)
        try:
            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åä»¥é™ã®å¼•æ•°ã‚’å‡¦ç†
            spider_args = []
            for arg in args[1:]:
                if not arg.startswith('-'):
                    spider_args.append(arg)
            opts.spargs, opts.spkwargs = arglist_to_dict(spider_args)
        except ValueError:
            # å¼•æ•°è§£æã‚¨ãƒ©ãƒ¼ã‚’ç„¡è¦–ã—ã¦ç©ºã®è¾æ›¸ã‚’è¨­å®š
            opts.spargs, opts.spkwargs = [], {}

    def run(self, args, opts):
        if len(args) < 1:
            raise UsageError("Spider name is required")

        spider_name = args[0]
        
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        if not opts.output:
            raise UsageError("Output file (-o) is required for watchdog monitoring")
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‹ãƒã‚§ãƒƒã‚¯
        if not opts.output.endswith('.jsonl'):
            print("âš ï¸ Warning: Output file should be .jsonl for optimal monitoring")
        
        # ã‚¿ã‚¹ã‚¯IDã‚’ç”Ÿæˆã¾ãŸã¯å–å¾—
        task_id = opts.task_id or f"cmd_{spider_name}_{int(time.time())}"
        
        print(f"ğŸš€ Starting spider with watchdog monitoring")
        print(f"   Spider: {spider_name}")
        print(f"   Task ID: {task_id}")
        print(f"   Output: {opts.output}")
        print(f"   DB Path: {opts.db_path}")
        print(f"   Watchdog Available: {'Yes' if WATCHDOG_AVAILABLE else 'No (using polling)'}")
        
        # watchdogç›£è¦–ã‚’é–‹å§‹
        monitor = JSONLMonitor(
            task_id=task_id,
            spider_name=spider_name,
            jsonl_file_path=opts.output,
            db_path=opts.db_path
        )
        
        # ç›£è¦–ã‚’åˆ¥ã‚¹ãƒ¬ãƒƒãƒ‰ã§é–‹å§‹
        import threading
        monitor_thread = threading.Thread(target=monitor.start_monitoring)
        monitor_thread.daemon = True
        monitor_thread.start()

        # å°‘ã—å¾…ã£ã¦ç›£è¦–ãŒé–‹å§‹ã•ã‚Œã‚‹ã®ã‚’ç¢ºèª
        import time
        time.sleep(1)
        print(f"ğŸ” Monitoring started in background thread")

        try:
            # Scrapyã®è¨­å®šã‚’æ›´æ–°
            self.settings.set('FEED_URI', opts.output)
            self.settings.set('FEED_FORMAT', opts.output_format or 'jsonlines')

            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ
            print(f"ğŸ•·ï¸ Starting Scrapy crawler...")
            self.crawler_process.crawl(spider_name, **opts.spkwargs)
            self.crawler_process.start()

        except KeyboardInterrupt:
            print(f"\nâš ï¸ Interrupted by user")
        except Exception as e:
            print(f"âŒ Crawler error: {e}")
        finally:
            # ç›£è¦–ã‚’åœæ­¢
            print(f"ğŸ›‘ Stopping monitoring...")
            monitor.stop_monitoring()

            # æœ€çµ‚å‡¦ç†ã‚’å®Ÿè¡Œ
            print(f"ğŸ” Processing remaining lines...")
            monitor.process_new_lines()

            # æœ€çµ‚çš„ãªçµ±è¨ˆã‚’è¡¨ç¤º
            print(f"\nğŸ“Š Final Statistics:")
            print(f"   Total items processed: {monitor.processed_lines}")
            print(f"   Output file: {opts.output}")
            print(f"   Database: {opts.db_path}")
            print(f"âœ… crawlwithwatchdog completed")
