from fastapi import APIRouter, Depends, HTTPException, status, Query
from sqlalchemy.orm import Session
from typing import List, Dict, Any
from pydantic import BaseModel
import uuid

from ..database import get_db, Spider as DBSpider, Project as DBProject, User as DBUser, UserRole, ProjectFile
from ..models.schemas import Spider, SpiderCreate, SpiderUpdate
from ..services.scrapy_service import ScrapyPlaywrightService
from ..services.integrity_service import integrity_service
from ..services.default_settings_service import default_settings_service
from .auth import get_current_active_user

router = APIRouter(
    responses={
        404: {"description": "Not found"},
        400: {"description": "Bad request"},
        500: {"description": "Internal server error"}
    }
)

# Pydanticãƒ¢ãƒ‡ãƒ«å®šç¾©
class RunSpiderWithWatchdogRequest(BaseModel):
    settings: Dict[str, Any] = {}

class PuppeteerSpiderRequest(BaseModel):
    spider_name: str
    start_urls: List[str]
    spider_type: str = "spa"  # "spa" or "dynamic"
    puppeteer_config: Dict[str, Any] = {}
    extract_data: Dict[str, Any] = {}
    actions: List[Dict[str, Any]] = []  # for dynamic spiders
    custom_settings: Dict[str, Any] = {}


def sync_spider_file_to_database(db, project_id: str, project_path: str, spider_name: str, spider_code: str, user_id: str):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ"""
    from datetime import datetime

    try:
        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        spider_file_path = f"{project_path}/spiders/{spider_name}.py"

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ—¢å­˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        existing_file = db.query(ProjectFile).filter(
            ProjectFile.project_id == project_id,
            ProjectFile.path == spider_file_path
        ).first()

        if existing_file:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            existing_file.content = spider_code
            existing_file.updated_at = datetime.now()
            print(f"âœ… Updated spider file in database: {spider_file_path}")
        else:
            # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
            db_file = ProjectFile(
                id=str(uuid.uuid4()),
                name=f"{spider_name}.py",
                path=spider_file_path,
                content=spider_code,
                file_type="python",
                project_id=project_id,
                user_id=user_id
            )
            db.add(db_file)
            print(f"âœ… Added spider file to database: {spider_file_path}")

        # å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆ
        db.commit()
        print(f"âœ… Spider file synced to database: {spider_file_path}")
        return True

    except Exception as e:
        db.rollback()
        print(f"âŒ Failed to sync spider file to database: {str(e)}")
        return False

@router.get(
    "/",
    response_model=List[Spider],
    summary="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä¸€è¦§å–å¾—",
    description="æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯å…¨ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚",
    response_description="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆ"
)
async def get_spiders(
    project_id: str = None,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """
    ## ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ä¸€è¦§å–å¾—

    æŒ‡å®šã•ã‚ŒãŸãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¾ãŸã¯å…¨ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ä¸€è¦§ã‚’å–å¾—ã—ã¾ã™ã€‚

    ### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    - **project_id** (optional): ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°

    ### ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **200**: ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™
    - **500**: ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼
    """
    query = db.query(DBSpider)

    # ç®¡ç†è€…ã¯å…¨ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã€ä¸€èˆ¬ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è‡ªåˆ†ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã¿
    is_admin = (current_user.role == UserRole.ADMIN or
                current_user.role == "ADMIN" or
                current_user.role == "admin")
    if not is_admin:
        query = query.filter(DBSpider.user_id == current_user.id)

    if project_id:
        query = query.filter(DBSpider.project_id == project_id)

    spiders = query.all()

    # ç©ºã®codeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    for spider in spiders:
        if not spider.code or spider.code.strip() == "":
            spider.code = '''import scrapy

class DefaultSpider(scrapy.Spider):
    name = "default"

    def start_requests(self):
        urls = [
            "https://example.com",
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        yield {
            "title": response.css("title::text").get(),
            "url": response.url,
        }
'''

    return spiders

@router.get("/{spider_id}", response_model=Spider)
async def get_spider(
    spider_id: str,
    db: Session = Depends(get_db)
    # current_user = Depends(get_current_active_user)  # ä¸€æ™‚çš„ã«ç„¡åŠ¹åŒ–
):
    """ç‰¹å®šã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å–å¾—"""
    spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ä¸€æ™‚çš„ã«æ¨©é™ãƒã‚§ãƒƒã‚¯ã‚’ç„¡åŠ¹åŒ–
    print(f"ğŸ” Spider access check temporarily disabled for spider {spider_id}")

    # ç®¡ç†è€…ä»¥å¤–ã¯è‡ªåˆ†ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
    # is_admin = (current_user.role == UserRole.ADMIN or
    #             current_user.role == "ADMIN" or
    #             current_user.role == "admin")
    #
    # if not is_admin and spider.user_id != current_user.id:
    #     raise HTTPException(
    #         status_code=status.HTTP_403_FORBIDDEN,
    #         detail="Access denied"
    #     )

    # ç©ºã®codeãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’æŒã¤ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    if not spider.code or spider.code.strip() == "":
        spider.code = '''import scrapy

class DefaultSpider(scrapy.Spider):
    name = "default"

    def start_requests(self):
        urls = [
            "https://example.com",
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        yield {
            "title": response.css("title::text").get(),
            "url": response.url,
        }
'''

    return spider

def update_spider_name_in_code(code: str, spider_name: str) -> str:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®name=ã¨ã‚¯ãƒ©ã‚¹åã‚’æ›´æ–°ã™ã‚‹ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚’ä¿æŒï¼‰"""
    import re

    if not code:
        return code

    updated_code = code

    # 1. name = "..." ã¾ãŸã¯ name = '...' ã®å½¢å¼ã‚’æ¤œç´¢ã—ã¦ç½®æ›ï¼ˆã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆä¿æŒï¼‰
    name_patterns = [
        r'(\s*)name\s*=\s*["\'][^"\']*["\']',  # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’å«ã‚€name = "old_name"
        r'(\s*)name\s*=\s*"[^"]*"',           # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’å«ã‚€name = "old_name"
        r"(\s*)name\s*=\s*'[^']*'"            # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’å«ã‚€name = 'old_name'
    ]

    name_updated = False
    for pattern in name_patterns:
        match = re.search(pattern, updated_code)
        if match:
            # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ä¿æŒã—ã¦ç½®æ›
            indent = match.group(1)
            # æ—¢ã«æ­£ã—ã„åå‰ã®å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            current_name_match = re.search(r'name\s*=\s*["\']([^"\']*)["\']', match.group(0))
            if current_name_match and current_name_match.group(1) == spider_name:
                print(f"ğŸ”„ Name attribute already correct: {spider_name}")
                name_updated = True
                break

            updated_code = re.sub(pattern, f'{indent}name = "{spider_name}"', updated_code)
            name_updated = True

    # 2. ã‚¯ãƒ©ã‚¹åã‚’æ›´æ–°ï¼ˆé‡è¦ï¼šã“ã‚ŒãŒæ¬ ã‘ã¦ã„ãŸï¼‰
    class_name = spider_name_to_class_name(spider_name)

    # æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œç´¢
    class_pattern = r'class\s+(\w+)\s*\([^)]*\):'
    class_match = re.search(class_pattern, updated_code)

    if class_match:
        old_class_name = class_match.group(1)
        # ã‚¯ãƒ©ã‚¹åã‚’æ–°ã—ã„åå‰ã«ç½®æ›
        updated_code = re.sub(
            r'class\s+' + re.escape(old_class_name) + r'\s*\(',
            f'class {class_name}(',
            updated_code
        )
        print(f"ğŸ”„ Updated class name: {old_class_name} -> {class_name}")
    else:
        print(f"âš ï¸ No class definition found in spider code")

    # nameå±æ€§ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®å‡¦ç†ã‚’è¿½åŠ 
    if not name_updated:
        print(f"âš ï¸ Name attribute not found, adding it")
        # ã‚¯ãƒ©ã‚¹å®šç¾©ã®å¾Œã« name å±æ€§ã‚’è¿½åŠ 
        class_pattern = r'(class\s+\w+.*?:)'
        if re.search(class_pattern, updated_code):
            updated_code = re.sub(class_pattern, f'\\1\n    name = "{spider_name}"', updated_code)
            print(f"ğŸ”§ Added missing name attribute")

    return updated_code

def spider_name_to_class_name(spider_name: str) -> str:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’ç”Ÿæˆ"""
    # ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¹ã‚³ã‚¢ã‚’å‰Šé™¤ã—ã¦ã‚­ãƒ£ãƒ¡ãƒ«ã‚±ãƒ¼ã‚¹ã«å¤‰æ›
    parts = spider_name.split('_')
    class_name = ''.join(word.capitalize() for word in parts)

    # Spiderã‚µãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ ï¼ˆã¾ã ãªã„å ´åˆï¼‰
    if not class_name.endswith('Spider'):
        class_name += 'Spider'

    return class_name

def auto_fix_spider_indentation(code: str) -> tuple[str, list[str]]:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚¨ãƒ©ãƒ¼ã‚’è‡ªå‹•ä¿®æ­£ã™ã‚‹"""
    import re

    if not code:
        return code, []

    lines = code.split('\n')
    fixed_lines = []
    fixes_applied = []
    in_class = False
    in_method = False
    current_method_indent = 0

    for i, line in enumerate(lines):
        line_num = i + 1

        # ç©ºè¡Œã‚„ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã¯ãã®ã¾ã¾
        if not line.strip() or line.strip().startswith('#'):
            fixed_lines.append(line)
            continue

        # importæ–‡ã‚„fromæ–‡ã¯ã‚¯ãƒ©ã‚¹å¤–
        if re.match(r'^(import|from)\s+', line):
            in_class = False
            in_method = False
            fixed_lines.append(line)
            continue

        # é–¢æ•°å®šç¾©ï¼ˆã‚¯ãƒ©ã‚¹å¤–ï¼‰
        if re.match(r'^def\s+', line):
            in_class = False
            in_method = False
            fixed_lines.append(line)
            continue

        # ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’æ¤œå‡º
        if re.match(r'^class\s+\w+.*?:', line):
            in_class = True
            in_method = False
            fixed_lines.append(line)
            print(f"ğŸ” Found class definition at line {line_num}: {line.strip()}")
            continue

        # æ–°ã—ã„ã‚¯ãƒ©ã‚¹å®šç¾©ï¼ˆå‰ã®ã‚¯ãƒ©ã‚¹çµ‚äº†ï¼‰
        if re.match(r'^class\s+', line):
            in_class = True
            in_method = False
            fixed_lines.append(line)
            continue

        # ã‚¯ãƒ©ã‚¹å†…ã®å‡¦ç†
        if in_class:
            # ãƒ¡ã‚½ãƒƒãƒ‰å®šç¾©
            if re.match(r'^\s*def\s+', line):
                in_method = True
                stripped_line = line.lstrip()
                expected_indent = '    '  # 4ã‚¹ãƒšãƒ¼ã‚¹

                # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒæ­£ã—ããªã„å ´åˆã®ã¿ä¿®æ­£
                if not line.startswith(expected_indent) and line.strip().startswith('def '):
                    fixed_line = expected_indent + stripped_line
                    fixed_lines.append(fixed_line)
                    fixes_applied.append(f"Line {line_num}: Fixed indentation for method definition")
                    print(f"ğŸ”§ Fixed line {line_num}: method definition")
                    continue
                else:
                    # æ—¢ã«æ­£ã—ã„ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
                    fixed_lines.append(line)
                    continue

            # ã‚¯ãƒ©ã‚¹å±æ€§ï¼ˆname, allowed_domains, start_urls, custom_settings ãªã©ï¼‰
            elif re.match(r'^\s*(name|allowed_domains|start_urls|custom_settings|handle_httpstatus_list|target_items_per_page|target_pages|total_target_items)\s*=', line):
                in_method = False
                stripped_line = line.lstrip()
                expected_indent = '    '  # 4ã‚¹ãƒšãƒ¼ã‚¹

                # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒæ­£ã—ããªã„å ´åˆã®ã¿ä¿®æ­£
                if not line.startswith(expected_indent) and not line.startswith('        '):
                    fixed_line = expected_indent + stripped_line
                    fixed_lines.append(fixed_line)
                    fixes_applied.append(f"Line {line_num}: Fixed indentation for class attribute: {stripped_line.split('=')[0].strip()}")
                    print(f"ğŸ”§ Fixed line {line_num}: '{line.strip()}' -> '{fixed_line.strip()}'")
                    continue
                else:
                    # æ—¢ã«æ­£ã—ã„ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ
                    fixed_lines.append(line)
                    continue

            # ãã®ä»–ã®ã‚¯ãƒ©ã‚¹å†…ã‚³ãƒ¼ãƒ‰ï¼ˆæ—¢ã«é©åˆ‡ã«ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã®ã¾ã¾ï¼‰
            elif line.strip():
                # æ—¢ã«é©åˆ‡ã«ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã•ã‚Œã¦ã„ã‚‹è¡Œã¯ãã®ã¾ã¾ä¿æŒ
                if line.startswith('    ') or line.startswith('        '):
                    fixed_lines.append(line)
                    continue

                # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã•ã‚Œã¦ã„ãªã„è¡Œï¼ˆãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«ï¼‰ãŒè¦‹ã¤ã‹ã£ãŸå ´åˆã€ã‚¯ãƒ©ã‚¹å¤–ã«å‡ºãŸã¨åˆ¤æ–­
                elif not line.startswith(' ') and not line.startswith('\t'):
                    in_class = False
                    in_method = False
                    fixed_lines.append(line)
                    continue

                # ãã®ä»–ã®å ´åˆã¯ãã®ã¾ã¾
                else:
                    fixed_lines.append(line)
                    continue

        # ã‚¯ãƒ©ã‚¹å¤–ã®ã‚³ãƒ¼ãƒ‰
        else:
            fixed_lines.append(line)

    fixed_code = '\n'.join(fixed_lines)
    return fixed_code, fixes_applied

def validate_spider_inheritance(code: str, auto_fix: bool = False) -> dict:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®ç¶™æ‰¿é–¢ä¿‚ã¨ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ï¼ˆè‡ªå‹•ä¿®æ­£ã‚ªãƒ—ã‚·ãƒ§ãƒ³ä»˜ãï¼‰"""
    import re

    validation_result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "fixes_applied": [],
        "fixed_code": code
    }

    if not code:
        validation_result["valid"] = False
        validation_result["errors"].append("ã‚³ãƒ¼ãƒ‰ãŒç©ºã§ã™")
        return validation_result

    # ã¾ãšå…ƒã®ã‚³ãƒ¼ãƒ‰ã§ã‚¨ãƒ©ãƒ¼ã‚’æ¤œå‡º
    original_lines = code.split('\n')
    for i, line in enumerate(original_lines, 1):
        # name = ã®è¡Œã‚’ãƒã‚§ãƒƒã‚¯
        if re.match(r'^\s*name\s*=', line):
            # ã‚¯ãƒ©ã‚¹å†…ã®å±æ€§ãªã®ã§ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãŒå¿…è¦
            if not line.startswith('    ') and not line.startswith('\t'):
                validation_result["errors"].append(f"Line {i}: 'name' attribute must be indented (class attribute)")

        # ã‚¯ãƒ©ã‚¹å®šç¾©å†…ã®ä»–ã®å±æ€§ã‚‚ãƒã‚§ãƒƒã‚¯
        if re.match(r'^\s*(allowed_domains|start_urls|custom_settings)\s*=', line):
            if not line.startswith('    ') and not line.startswith('\t'):
                validation_result["warnings"].append(f"Line {i}: Class attribute should be indented")

    # è‡ªå‹•ä¿®æ­£ã‚’å®Ÿè¡Œ
    if auto_fix:
        fixed_code, fixes_applied = auto_fix_spider_indentation(code)
        validation_result["fixed_code"] = fixed_code
        validation_result["fixes_applied"].extend(fixes_applied)

        # ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ã§å†æ¤œè¨¼
        if fixes_applied:
            fixed_lines = fixed_code.split('\n')
            remaining_errors = []
            remaining_warnings = []

            for i, line in enumerate(fixed_lines, 1):
                # name = ã®è¡Œã‚’ãƒã‚§ãƒƒã‚¯
                if re.match(r'^\s*name\s*=', line):
                    if not line.startswith('    ') and not line.startswith('\t'):
                        remaining_errors.append(f"Line {i}: 'name' attribute must be indented (class attribute)")

                # ã‚¯ãƒ©ã‚¹å®šç¾©å†…ã®ä»–ã®å±æ€§ã‚‚ãƒã‚§ãƒƒã‚¯
                if re.match(r'^\s*(allowed_domains|start_urls|custom_settings)\s*=', line):
                    if not line.startswith('    ') and not line.startswith('\t'):
                        remaining_warnings.append(f"Line {i}: Class attribute should be indented")

            # ä¿®æ­£å¾Œã«ã‚¨ãƒ©ãƒ¼ãŒæ®‹ã£ã¦ã„ãªã„å ´åˆã¯æœ‰åŠ¹ã¨ã™ã‚‹
            if not remaining_errors:
                validation_result["valid"] = True
                validation_result["errors"] = []
            else:
                validation_result["valid"] = False
                validation_result["errors"] = remaining_errors

            validation_result["warnings"] = remaining_warnings
    else:
        # è‡ªå‹•ä¿®æ­£ã—ãªã„å ´åˆã€ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚Œã°ç„¡åŠ¹
        if validation_result["errors"]:
            validation_result["valid"] = False

    # 1. scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯
    has_scrapy_import = 'import scrapy' in code or 'from scrapy' in code
    if not has_scrapy_import:
        validation_result["warnings"].append("scrapy ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # 2. ã‚¯ãƒ©ã‚¹å®šç¾©ã‚’ãƒã‚§ãƒƒã‚¯
    class_patterns = [
        r'class\s+(\w+)\(scrapy\.Spider\):\s*',  # class ClassName(scrapy.Spider):
        r'class\s+(\w+)\(Spider\):\s*',          # class ClassName(Spider):
        r'class\s+(\w+):\s*',                    # class ClassName: (ç¶™æ‰¿ãªã—)
    ]

    class_found = False
    has_inheritance = False
    class_name = None

    for i, pattern in enumerate(class_patterns):
        match = re.search(pattern, code)
        if match:
            class_found = True
            class_name = match.group(1)
            if i < 2:  # æœ€åˆã®2ã¤ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ç¶™æ‰¿ã‚ã‚Š
                has_inheritance = True
            break

    if not class_found:
        validation_result["valid"] = False
        validation_result["errors"].append("ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚¯ãƒ©ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    elif not has_inheritance:
        validation_result["warnings"].append(f"ã‚¯ãƒ©ã‚¹ '{class_name}' ãŒ scrapy.Spider ã‚’ç¶™æ‰¿ã—ã¦ã„ã¾ã›ã‚“")

    # 3. name å±æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    name_pattern = r'name\s*=\s*["\'][^"\']+["\']'
    if not re.search(name_pattern, code):
        validation_result["warnings"].append("name å±æ€§ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # 4. parse ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ãƒã‚§ãƒƒã‚¯
    parse_pattern = r'def\s+parse\s*\('
    if not re.search(parse_pattern, code):
        validation_result["warnings"].append("parse ãƒ¡ã‚½ãƒƒãƒ‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    return validation_result

def update_project_imports_in_code(code: str, old_project_name: str, new_project_name: str) -> str:
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’æ›´æ–°ã™ã‚‹"""
    import re

    if not code or old_project_name == new_project_name:
        return code

    updated_code = code

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‚’å«ã‚€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚’æ¤œç´¢ã—ã¦ç½®æ›
    # ä¾‹: from old_project.items import -> from new_project.items import
    import_patterns = [
        rf'from\s+{re.escape(old_project_name)}\.(\w+)\s+import',
        rf'import\s+{re.escape(old_project_name)}\.(\w+)',
    ]

    for pattern in import_patterns:
        matches = re.finditer(pattern, updated_code)
        for match in matches:
            old_import = match.group(0)
            if 'from' in old_import:
                # from old_project.module import -> from new_project.module import
                new_import = re.sub(rf'{re.escape(old_project_name)}\.', f'{new_project_name}.', old_import)
            else:
                # import old_project.module -> import new_project.module
                new_import = re.sub(rf'{re.escape(old_project_name)}\.', f'{new_project_name}.', old_import)

            updated_code = updated_code.replace(old_import, new_import)
            print(f"ğŸ”„ Updated import: {old_import} -> {new_import}")

    return updated_code

@router.post("/puppeteer", response_model=Spider, status_code=status.HTTP_201_CREATED)
async def create_puppeteer_spider(
    request: PuppeteerSpiderRequest,
    project_id: str,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
    project = db.query(DBProject).filter(DBProject.id == project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # ç®¡ç†è€…ä»¥å¤–ã¯è‡ªåˆ†ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ã¿ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½
    is_admin = (current_user.role == UserRole.ADMIN or
                current_user.role == "ADMIN" or
                current_user.role == "admin")
    if not is_admin and project.user_id != current_user.id:
        raise HTTPException(
            status_code=status.HTTP_403_FORBIDDEN,
            detail="Access denied"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == project_id,
        DBSpider.name == request.spider_name
    ).first()

    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider '{request.spider_name}' already exists in this project"
        )

    try:
        # Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
        try:
            from backend.app.templates.advanced_puppeteer_spider import get_puppeteer_spider_template
            spider_code = get_puppeteer_spider_template(
                request.spider_name,
                project.path,
                request.start_urls
            )
        except ImportError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¾“æ¥ã®æ–¹æ³•
            spider_code = generate_puppeteer_spider_code(request)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
        db_spider = DBSpider(
            id=str(uuid.uuid4()),
            name=request.spider_name,
            code=spider_code,
            template="puppeteer",
            project_id=project_id,
            user_id=current_user.id
        )

        db.add(db_spider)
        db.commit()
        db.refresh(db_spider)

        print(f"âœ… Puppeteer spider created successfully: {request.spider_name}")
        return db_spider

    except Exception as e:
        db.rollback()
        print(f"âŒ Error creating Puppeteer spider: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to create Puppeteer spider: {str(e)}"
        )

def generate_puppeteer_spider_code(request: PuppeteerSpiderRequest) -> str:
    """Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ"""

    # ã‚¯ãƒ©ã‚¹åã‚’ç”Ÿæˆ
    class_name = ''.join(word.capitalize() for word in request.spider_name.replace('_', ' ').replace('-', ' ').split())
    if not class_name.endswith('Spider'):
        class_name += 'Spider'

    # start_urlsã®æ–‡å­—åˆ—ã‚’ç”Ÿæˆ
    start_urls_str = ',\n        '.join([f'"{url}"' for url in request.start_urls])

    # extractDataã®è¨­å®šã‚’ç”Ÿæˆ
    extract_data_str = ""
    if request.extract_data:
        import json
        extract_data_str = f"""
                extractData={json.dumps(request.extract_data, indent=16).replace('    ', '')},"""

    # actionsã®è¨­å®šã‚’ç”Ÿæˆï¼ˆdynamicã‚¿ã‚¤ãƒ—ã®å ´åˆï¼‰
    actions_str = ""
    if request.spider_type == "dynamic" and request.actions:
        import json
        actions_str = f"""
            actions = {json.dumps(request.actions, indent=12).replace('    ', '')}

            yield self.make_dynamic_request(
                url=url,
                actions=actions,
                extract_after={json.dumps(request.extract_data, indent=16).replace('    ', '') if request.extract_data else 'None'}
            )"""
    else:
        actions_str = f"""yield self.make_puppeteer_request(
                url=url,{extract_data_str}
                screenshot=False,
                waitFor=3000
            )"""

    # ã‚«ã‚¹ã‚¿ãƒ è¨­å®šã‚’ç”Ÿæˆ
    custom_settings_str = ""
    if request.custom_settings:
        import json
        custom_settings_str = f"""
    custom_settings = {json.dumps(request.custom_settings, indent=8).replace('    ', '')}"""

    # Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
    spider_code = f'''"""
{request.spider_name} - Puppeteerã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼
Generated by ScrapyUI
"""

import scrapy
import json
from datetime import datetime


class {class_name}(scrapy.Spider):
    """
    Puppeteerã‚’ä½¿ç”¨ã—ãŸã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼
    JavaScripté‡è¦ãªSPAã‚µã‚¤ãƒˆã‚„ãƒ€ã‚¤ãƒŠãƒŸãƒƒã‚¯ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å–å¾—
    """

    name = "{request.spider_name}"
    start_urls = [
        {start_urls_str}
    ]

    # Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã®è¨­å®š
    puppeteer_service_url = 'http://localhost:3001'{custom_settings_str}

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.puppeteer_config = {json.dumps(request.puppeteer_config, indent=12).replace('    ', '') if request.puppeteer_config else '{}'}

    def start_requests(self):
        """é–‹å§‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        for url in self.start_urls:
            {actions_str}

    def make_puppeteer_request(self, url, **kwargs):
        """Puppeteerã‚’ä½¿ç”¨ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ"""
        config = {{**self.puppeteer_config, **kwargs}}

        puppeteer_data = {{
            'url': url,
            'viewport': config.get('viewport', {{'width': 1920, 'height': 1080}}),
            'userAgent': config.get('userAgent', 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'),
            'timeout': config.get('timeout', 30000),
            'waitFor': config.get('waitFor', 3000),
            'extractData': config.get('extractData'),
            'screenshot': config.get('screenshot', False),
        }}

        puppeteer_data = {{k: v for k, v in puppeteer_data.items() if v is not None}}

        return scrapy.Request(
            url=f"{{self.puppeteer_service_url}}/api/scraping/spa",
            method='POST',
            headers={{'Content-Type': 'application/json'}},
            body=json.dumps(puppeteer_data),
            callback=self.parse_puppeteer_response,
            meta={{'original_url': url, 'puppeteer_data': puppeteer_data}}
        )

    def make_dynamic_request(self, url, actions, extract_after=None, **kwargs):
        """å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”¨ã®Puppeteerãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ"""
        config = {{**self.puppeteer_config, **kwargs}}

        puppeteer_data = {{
            'url': url,
            'actions': actions,
            'extractAfter': extract_after,
            'timeout': config.get('timeout', 30000),
        }}

        return scrapy.Request(
            url=f"{{self.puppeteer_service_url}}/api/scraping/dynamic",
            method='POST',
            headers={{'Content-Type': 'application/json'}},
            body=json.dumps(puppeteer_data),
            callback=self.parse_dynamic_response,
            meta={{'original_url': url, 'puppeteer_data': puppeteer_data}}
        )

    def parse_puppeteer_response(self, response):
        """Puppeteerã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        try:
            data = json.loads(response.text)

            if not data.get('success'):
                self.logger.error(f"Puppeteer scraping failed: {{data.get('message', 'Unknown error')}}")
                return

            scraping_data = data.get('data', {{}})
            original_url = response.meta.get('original_url')

            item = {{
                'url': original_url,
                'scraped_url': scraping_data.get('url'),
                'title': scraping_data.get('pageInfo', {{}}).get('title'),
                'timestamp': scraping_data.get('timestamp'),
                'scraped_at': datetime.now().isoformat(),
            }}

            # æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            if 'extractedData' in scraping_data:
                item.update(scraping_data['extractedData'])

            # ã‚«ã‚¹ã‚¿ãƒ JavaScriptã®çµæœã‚’è¿½åŠ 
            if 'customData' in scraping_data:
                item['custom_data'] = scraping_data['customData']

            yield item

        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse Puppeteer response: {{e}}")
        except Exception as e:
            self.logger.error(f"Error processing Puppeteer response: {{e}}")

    def parse_dynamic_response(self, response):
        """å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è§£æ"""
        try:
            data = json.loads(response.text)

            if not data.get('success'):
                self.logger.error(f"Dynamic scraping failed: {{data.get('message', 'Unknown error')}}")
                return

            original_url = response.meta.get('original_url')

            item = {{
                'url': original_url,
                'scraped_url': data.get('url'),
                'title': data.get('pageInfo', {{}}).get('title'),
                'timestamp': data.get('timestamp'),
                'actions_executed': data.get('actionsExecuted', 0),
                'scraped_at': datetime.now().isoformat(),
            }}

            # æŠ½å‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            if 'data' in data:
                item.update(data['data'])

            # ã‚«ã‚¹ã‚¿ãƒ JavaScriptã®çµæœã‚’è¿½åŠ 
            if 'customData' in data:
                item['custom_data'] = data['customData']

            yield item

        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse dynamic response: {{e}}")
        except Exception as e:
            self.logger.error(f"Error processing dynamic response: {{e}}")
'''

    return spider_code

@router.post("/", response_model=Spider, status_code=status.HTTP_201_CREATED)
async def create_spider(
    spider: SpiderCreate,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ"""

    try:
        print(f"DEBUG: Received spider creation request:")
        print(f"  name: {spider.name}")
        print(f"  project_id: {spider.project_id}")
        print(f"  template: {spider.template}")
        print(f"  code_length: {len(spider.code) if spider.code else 0}")
        print(f"  settings: {spider.settings}")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
        project = db.query(DBProject).filter(DBProject.id == spider.project_id).first()
        if not project:
            print(f"DEBUG: Project not found: {spider.project_id}")
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found"
            )
    except Exception as e:
        print(f"âŒ Error in spider creation initial validation: {str(e)}")
        import traceback
        traceback.print_exc()
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider creation validation failed: {str(e)}"
        )

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼åã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == spider.project_id,
        DBSpider.name == spider.name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Spider with this name already exists in the project"
        )

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨ï¼ˆJSONLå½¢å¼å¯¾å¿œï¼‰
    spider_type = spider.template or "basic"
    default_settings = default_settings_service.get_spider_default_settings(spider_type)

    # æ—¢å­˜ã®è¨­å®šã¨ãƒãƒ¼ã‚¸
    merged_settings = default_settings.copy()
    if spider.settings:
        merged_settings.update(spider.settings)

    print(f"DEBUG: Applied default settings for spider type: {spider_type}")
    print(f"DEBUG: Default feed format: {merged_settings.get('FEEDS', {}).get('results.jsonl', {}).get('format', 'unknown')}")

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰å†…ã®name=ã‚’ç¢ºå®Ÿã«æ›´æ–°ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚‚ä¿æŒï¼‰
    updated_code = update_spider_name_in_code(spider.code, spider.name)
    print(f"DEBUG: Updated spider code name to: {spider.name}")
    print(f"DEBUG: Ensured scrapy.Spider inheritance")

    # ã¾ãšåŸºæœ¬çš„ãªãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œï¼ˆè‡ªå‹•ä¿®æ­£ãªã—ï¼‰
    validation_result = validate_spider_inheritance(updated_code, auto_fix=False)

    # é‡å¤§ãªã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã®ã¿è‡ªå‹•ä¿®æ­£ã‚’å®Ÿè¡Œ
    if validation_result["errors"]:
        print(f"DEBUG: Found validation errors, attempting auto-fix: {validation_result['errors']}")
        validation_result = validate_spider_inheritance(updated_code, auto_fix=True)
        if validation_result["fixes_applied"]:
            print(f"DEBUG: Auto-fixed issues: {validation_result['fixes_applied']}")
            updated_code = validation_result["fixed_code"]

    if validation_result["warnings"]:
        print(f"DEBUG: Validation warnings: {validation_result['warnings']}")

    if not validation_result["valid"]:
        print(f"DEBUG: Validation errors: {validation_result['errors']}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider code validation failed: {'; '.join(validation_result['errors'])}"
        )

    # user_idã®è¨­å®šï¼ˆèªè¨¼ã•ã‚ŒãŸãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰å–å¾—ï¼‰
    user_id = current_user.id
    print(f"DEBUG: Using authenticated user_id = {user_id} (user: {current_user.email})")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    scrapy_service = ScrapyPlaywrightService()
    spider_file_path = scrapy_service.base_projects_dir / project.path / project.path / "spiders" / f"{spider.name}.py"
    if spider_file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider file '{spider.name}.py' already exists in filesystem"
        )

    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
    db_spider = None
    file_created = False

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šé©ç”¨ï¼‰
        db_spider = DBSpider(
            id=str(uuid.uuid4()),
            name=spider.name,
            code=updated_code,
            template=spider.template,
            settings=merged_settings,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨
            project_id=spider.project_id,
            user_id=user_id
        )

        db.add(db_spider)
        db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«flushï¼ˆcommitã¯ã¾ã ã—ãªã„ï¼‰

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
        scrapy_service.save_spider_code(project.path, spider.name, updated_code)
        file_created = True

        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if not spider_file_path.exists():
            raise Exception("File was not created successfully")

        # 4. ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ
        try:
            sync_spider_file_to_database(
                db, spider.project_id, project.path, spider.name, updated_code, user_id
            )
            print(f"âœ… Spider file synced to database: {spider.name}")
        except Exception as sync_error:
            print(f"âš ï¸ Failed to sync spider file to database: {sync_error}")
            # ãƒ•ã‚¡ã‚¤ãƒ«åŒæœŸå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³å‡¦ç†ã¯ç¶™ç¶šï¼‰

        # 5. å…¨ã¦æˆåŠŸã—ãŸå ´åˆã®ã¿commit
        db.commit()
        db.refresh(db_spider)

        print(f"âœ… Spider created successfully: {spider.name}")
        return db_spider

    except HTTPException:
        # HTTPExceptionã¯ãã®ã¾ã¾å†ç™ºç”Ÿ
        raise
    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"âŒ Error creating spider: {str(e)}")
        import traceback
        traceback.print_exc()

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        try:
            if db_spider:
                db.rollback()
        except Exception as rollback_error:
            print(f"âš ï¸ Failed to rollback database: {rollback_error}")

        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        try:
            if file_created and spider_file_path.exists():
                spider_file_path.unlink()
                print(f"ğŸ—‘ï¸ Cleaned up file: {spider_file_path}")
        except Exception as cleanup_error:
            print(f"âš ï¸ Failed to cleanup file: {cleanup_error}")

        # è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’å«ã‚ã¦å†ç™ºç”Ÿ
        error_detail = f"Failed to create spider: {str(e)}"
        if "validation" in str(e).lower():
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=error_detail
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=error_detail
            )

@router.put("/{spider_id}", response_model=Spider)
async def update_spider(
    spider_id: str,
    spider_update: SpiderUpdate,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’æ›´æ–°"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã®é©ç”¨
    update_data = spider_update.model_dump(exclude_unset=True)
    for field, value in update_data.items():
        setattr(db_spider, field, value)

    db.commit()
    db.refresh(db_spider)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°
    if 'code' in update_data:
        try:
            scrapy_service = ScrapyPlaywrightService()
            scrapy_service.save_spider_code(project.path, db_spider.name, db_spider.code)

            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ProjectFileãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚åŒæœŸ
            sync_spider_file_to_database(
                db, db_spider.project_id, project.path, db_spider.name, db_spider.code, db_spider.user_id
            )
            print(f"âœ… Spider file updated and synced: {db_spider.name}")
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"Failed to update spider file: {str(e)}"
            )

    return db_spider

@router.delete("/{spider_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_spider(
    spider_id: str,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å‰Šé™¤"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã«é–¢é€£ã™ã‚‹ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å‰Šé™¤
    try:
        from ..database import Schedule as DBSchedule
        related_schedules = db.query(DBSchedule).filter(DBSchedule.spider_id == spider_id).all()

        if related_schedules:
            print(f"ğŸ—‘ï¸ Deleting {len(related_schedules)} schedules related to spider {db_spider.name}")
            for schedule in related_schedules:
                print(f"  - Deleting schedule: {schedule.name} (ID: {schedule.id})")
                db.delete(schedule)

        # å¤‰æ›´ã‚’ã‚³ãƒŸãƒƒãƒˆï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ï¼‰
        db.commit()
        print(f"âœ… Successfully deleted {len(related_schedules)} related schedules")

    except Exception as e:
        print(f"âš ï¸ Error deleting related schedules: {str(e)}")
        db.rollback()
        # ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å‰Šé™¤ã¯ç¶šè¡Œ

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        scrapy_service = ScrapyPlaywrightService()
        spider_file_path = scrapy_service.base_projects_dir / project.path / "spiders" / f"{db_spider.name}.py"
        if spider_file_path.exists():
            spider_file_path.unlink()
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã«å¤±æ•—ã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã¯å‰Šé™¤ã™ã‚‹
        pass

    # ProjectFileãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ã‚‚ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
    try:
        spider_file_path_db = f"{project.path}/spiders/{db_spider.name}.py"
        project_file = db.query(ProjectFile).filter(
            ProjectFile.project_id == db_spider.project_id,
            ProjectFile.path == spider_file_path_db
        ).first()
        if project_file:
            db.delete(project_file)
            print(f"âœ… Removed spider file from database: {spider_file_path_db}")
    except Exception as e:
        print(f"âš ï¸ Failed to remove spider file from database: {str(e)}")

    db.delete(db_spider)
    db.commit()

    return None

@router.get("/{spider_id}/sync-from-filesystem")
async def sync_spider_code_from_filesystem(spider_id: str, db: Session = Depends(get_db)):
    """å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‹ã‚‰ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
    try:
        db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
        if not db_spider:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Spider not found"
            )

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
        project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()
        if not project:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail="Project not found"
            )

        # ScrapyPlaywrightServiceã‚’ä½¿ç”¨ã—ã¦å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿å–ã‚Š
        scrapy_service = ScrapyPlaywrightService()
        code = scrapy_service.get_spider_code(project.path, db_spider.name)

        print(f"âœ… Read spider code from filesystem: {db_spider.name}.py ({len(code)} chars)")

        return {
            "spider_id": spider_id,
            "spider_name": db_spider.name,
            "project_path": project.path,
            "code": code,
            "size": len(code.encode('utf-8'))
        }

    except Exception as e:
        print(f"âŒ Failed to read spider code from filesystem: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to read spider code from filesystem: {str(e)}"
        )

@router.get("/{spider_id}/code")
async def get_spider_code(spider_id: str, db: Session = Depends(get_db)):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    try:
        scrapy_service = ScrapyPlaywrightService()
        code = scrapy_service.get_spider_code(project.path, db_spider.name)
        return {"code": code}
    except Exception as e:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’è¿”ã™
        return {"code": db_spider.code}

@router.post("/{spider_id}/save")
async def save_spider_code(
    spider_id: str,
    code_data: dict,
    db: Session = Depends(get_db)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¿å­˜"""
    db_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not db_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Spider not found"
        )

    code = code_data.get("code", "")
    if not code:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Code is required"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == db_spider.project_id).first()

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ›´æ–°
    db_spider.code = code
    db.commit()

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
    try:
        scrapy_service = ScrapyPlaywrightService()
        scrapy_service.save_spider_code(project.path, db_spider.name, code)

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ProjectFileãƒ†ãƒ¼ãƒ–ãƒ«ã‚‚åŒæœŸ
        sync_spider_file_to_database(
            db, db_spider.project_id, project.path, db_spider.name, code, db_spider.user_id
        )
        print(f"âœ… Spider code saved and synced: {db_spider.name}")

        return {"message": "Code saved successfully"}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to save spider code: {str(e)}"
        )

@router.post("/{spider_id}/validate")
async def validate_spider_code(spider_id: str, code_data: dict, db: Session = Depends(get_db)):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ¼ãƒ‰ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    code = code_data.get("code", "")
    if not code:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="Code is required"
        )

    try:
        scrapy_service = ScrapyPlaywrightService()

        # åŸºæœ¬çš„ãªæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
        basic_result = scrapy_service.validate_spider_code(code)

        # è¿½åŠ ã®æ§‹æ–‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ‹¬å¼§ã®ä¸ä¸€è‡´ãªã©ï¼‰
        enhanced_result = validate_enhanced_syntax(code)

        # çµæœã‚’ãƒãƒ¼ã‚¸
        result = {
            "valid": basic_result["valid"] and enhanced_result["valid"],
            "errors": basic_result["errors"] + enhanced_result["errors"],
            "warnings": enhanced_result.get("warnings", []),
            "suggestions": enhanced_result.get("suggestions", [])
        }

        return result
    except Exception as e:
        return {
            "valid": False,
            "errors": [f"Validation error: {str(e)}"]
        }

def validate_enhanced_syntax(code: str) -> dict:
    """å¼·åŒ–ã•ã‚ŒãŸæ§‹æ–‡ãƒã‚§ãƒƒã‚¯"""
    result = {
        "valid": True,
        "errors": [],
        "warnings": [],
        "suggestions": []
    }

    if not code.strip():
        result["valid"] = False
        result["errors"].append("Code is empty")
        return result

    lines = code.split('\n')

    # æ‹¬å¼§ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    bracket_stack = []
    bracket_pairs = {'(': ')', '[': ']', '{': '}'}

    for line_num, line in enumerate(lines, 1):
        for char_pos, char in enumerate(line):
            if char in bracket_pairs:
                bracket_stack.append((char, line_num, char_pos))
            elif char in bracket_pairs.values():
                if not bracket_stack:
                    result["valid"] = False
                    result["errors"].append(f"Line {line_num}: Unmatched closing bracket '{char}'")
                else:
                    open_bracket, _, _ = bracket_stack.pop()
                    if bracket_pairs[open_bracket] != char:
                        result["valid"] = False
                        result["errors"].append(f"Line {line_num}: Mismatched bracket. Expected '{bracket_pairs[open_bracket]}' but found '{char}'")

    # æœªé–‰ã˜ã®æ‹¬å¼§ãƒã‚§ãƒƒã‚¯
    if bracket_stack:
        for bracket, line_num, _ in bracket_stack:
            result["valid"] = False
            result["errors"].append(f"Line {line_num}: Unclosed bracket '{bracket}'")

    # å¼•ç”¨ç¬¦ã®ä¸ä¸€è‡´ãƒã‚§ãƒƒã‚¯
    for line_num, line in enumerate(lines, 1):
        in_string = False
        quote_char = None
        i = 0
        while i < len(line):
            char = line[i]
            if char in ['"', "'"]:
                if not in_string:
                    in_string = True
                    quote_char = char
                elif char == quote_char:
                    # ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã•ã‚Œã¦ã„ãªã„å ´åˆã®ã¿
                    if i == 0 or line[i-1] != '\\':
                        in_string = False
                        quote_char = None
            i += 1

        if in_string:
            result["warnings"].append(f"Line {line_num}: Unclosed string literal")

    # ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã®ä¸€è²«æ€§ãƒã‚§ãƒƒã‚¯
    indent_levels = []
    for line_num, line in enumerate(lines, 1):
        if line.strip():  # ç©ºè¡Œã¯ç„¡è¦–
            leading_spaces = len(line) - len(line.lstrip())
            if leading_spaces > 0:
                indent_levels.append((line_num, leading_spaces))

    # 4ã®å€æ•°ã§ãªã„ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’è­¦å‘Š
    for line_num, indent in indent_levels:
        if indent % 4 != 0:
            result["warnings"].append(f"Line {line_num}: Indentation is not a multiple of 4 spaces")

    # åŸºæœ¬çš„ãªPythonæ§‹æ–‡ãƒã‚§ãƒƒã‚¯
    try:
        compile(code, '<string>', 'exec')
    except SyntaxError as e:
        result["valid"] = False
        result["errors"].append(f"Line {e.lineno}: {e.msg}")
    except Exception as e:
        result["warnings"].append(f"Compilation warning: {str(e)}")

    return result

@router.get(
    "/integrity/check",
    summary="æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚"
)
async def check_integrity():
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ãƒã‚§ãƒƒã‚¯"""
    try:
        result = integrity_service.check_integrity()
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Integrity check failed: {str(e)}"
        )

@router.post(
    "/integrity/fix",
    summary="æ•´åˆæ€§ä¿®å¾©",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ä¿®å¾©ã—ã¾ã™ã€‚"
)
async def fix_integrity(auto_fix: bool = False):
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ã‚’ä¿®å¾©"""
    try:
        result = integrity_service.fix_integrity_issues(auto_fix=auto_fix)
        return result
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Integrity fix failed: {str(e)}"
        )

@router.get(
    "/integrity/report",
    summary="æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆ",
    description="ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚"
)
async def get_integrity_report():
    """æ•´åˆæ€§ãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
    try:
        report = integrity_service.generate_integrity_report()
        return {"report": report}
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Report generation failed: {str(e)}"
        )

@router.post(
    "/{spider_id}/copy",
    response_model=Spider,
    status_code=status.HTTP_201_CREATED,
    summary="ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚³ãƒ”ãƒ¼",
    description="æ—¢å­˜ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚æ•´åˆæ€§ä¿è¨¼ä»˜ãã€‚"
)
async def copy_spider(
    spider_id: str,
    copy_data: dict,
    db: Session = Depends(get_db)
):
    """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ã‚³ãƒ”ãƒ¼ï¼ˆæ•´åˆæ€§ä¿è¨¼ä»˜ãï¼‰"""

    new_name = copy_data.get("name", "").strip()
    if not new_name:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail="New spider name is required"
        )

    # å…ƒã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
    original_spider = db.query(DBSpider).filter(DBSpider.id == spider_id).first()
    if not original_spider:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Original spider not found"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
    project = db.query(DBProject).filter(DBProject.id == original_spider.project_id).first()
    if not project:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Project not found"
        )

    # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ï¼‰
    existing_spider = db.query(DBSpider).filter(
        DBSpider.project_id == original_spider.project_id,
        DBSpider.name == new_name
    ).first()
    if existing_spider:
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider with name '{new_name}' already exists in this project"
        )

    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
    scrapy_service = ScrapyPlaywrightService()
    new_spider_file_path = scrapy_service.base_projects_dir / project.path / project.path / "spiders" / f"{new_name}.py"
    if new_spider_file_path.exists():
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Spider file '{new_name}.py' already exists in filesystem"
        )

    # ã‚³ãƒ¼ãƒ‰å†…ã®name=ã¨ã‚¯ãƒ©ã‚¹åã‚’æ–°ã—ã„åå‰ã«æ›´æ–°ï¼ˆç¶™æ‰¿é–¢ä¿‚ã‚‚ä¿æŒï¼‰
    updated_code = update_spider_name_in_code(original_spider.code, new_name)
    print(f"DEBUG: Updated copied spider code name to: {new_name}")
    print(f"DEBUG: Ensured scrapy.Spider inheritance in copied spider")

    # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨è‡ªå‹•ä¿®æ­£ã‚’å®Ÿè¡Œ
    validation_result = validate_spider_inheritance(updated_code, auto_fix=True)
    if validation_result["fixes_applied"]:
        print(f"DEBUG: Auto-fixed issues in copied spider: {validation_result['fixes_applied']}")
        updated_code = validation_result["fixed_code"]

    if validation_result["warnings"]:
        print(f"DEBUG: Copy validation warnings: {validation_result['warnings']}")
    if not validation_result["valid"]:
        print(f"DEBUG: Copy validation errors: {validation_result['errors']}")
        raise HTTPException(
            status_code=status.HTTP_400_BAD_REQUEST,
            detail=f"Copied spider code validation failed: {'; '.join(validation_result['errors'])}"
        )

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–“ã§ã®ã‚³ãƒ”ãƒ¼ã®å ´åˆã€ã‚¤ãƒ³ãƒãƒ¼ãƒˆæ–‡ã‚‚æ›´æ–°
    # ï¼ˆç¾åœ¨ã¯åŒä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå†…ã®ã‚³ãƒ”ãƒ¼ã®ã¿ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹ãŒã€å°†æ¥ã®æ‹¡å¼µã®ãŸã‚ï¼‰
    # updated_code = update_project_imports_in_code(updated_code, old_project_name, new_project_name)

    # ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³é–‹å§‹
    new_spider = None
    file_created = False

    try:
        # 1. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ–°ã—ã„ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’ä½œæˆ
        new_spider = DBSpider(
            id=str(uuid.uuid4()),
            name=new_name,
            code=updated_code,
            template=original_spider.template,
            settings=original_spider.settings,
            project_id=original_spider.project_id,
            user_id=original_spider.user_id
        )

        db.add(new_spider)
        db.flush()  # IDã‚’å–å¾—ã™ã‚‹ãŸã‚ã«flushï¼ˆcommitã¯ã¾ã ã—ãªã„ï¼‰

        # 2. ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
        scrapy_service.save_spider_code(project.path, new_name, updated_code)
        file_created = True

        # 3. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
        if not new_spider_file_path.exists():
            raise Exception("File was not created successfully")

        # 4. ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«åŒæœŸ
        try:
            sync_spider_file_to_database(
                db, original_spider.project_id, project.path, new_name, updated_code, original_spider.user_id
            )
            print(f"âœ… Copied spider file synced to database: {new_name}")

            # åŒæœŸç¢ºèª
            spider_file_path_db = f"{project.path}/spiders/{new_name}.py"
            synced_file = db.query(ProjectFile).filter(
                ProjectFile.project_id == original_spider.project_id,
                ProjectFile.path == spider_file_path_db
            ).first()
            if synced_file:
                print(f"âœ… Copied spider file sync confirmed: {spider_file_path_db}")
            else:
                print(f"âš ï¸ Copied spider file sync verification failed: {spider_file_path_db}")
                # åŒæœŸç¢ºèªå¤±æ•—æ™‚ã¯å†è©¦è¡Œ
                sync_spider_file_to_database(
                    db, original_spider.project_id, project.path, new_name, updated_code, original_spider.user_id
                )
                print(f"ğŸ”„ Retried spider file sync for: {new_name}")
        except Exception as sync_error:
            print(f"âŒ Failed to sync copied spider file to database: {sync_error}")
            # ã‚³ãƒ”ãƒ¼æ™‚ã®åŒæœŸå¤±æ•—ã¯é‡è¦ãªã®ã§ã€ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†
            raise Exception(f"Database sync failed for copied spider: {sync_error}")

        # 5. å…¨ã¦æˆåŠŸã—ãŸå ´åˆã®ã¿commit
        db.commit()
        db.refresh(new_spider)

        print(f"âœ… Spider copied successfully: {original_spider.name} -> {new_name}")

        # 5. æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œ
        integrity_result = integrity_service.check_integrity()
        if not integrity_result['summary']['integrity_ok']:
            print(f"âš ï¸ Integrity check failed after copy operation")

        return new_spider

    except Exception as e:
        # ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        print(f"âŒ Error copying spider: {str(e)}")

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if new_spider:
            db.rollback()

        # ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        if file_created and new_spider_file_path.exists():
            try:
                new_spider_file_path.unlink()
                print(f"ğŸ—‘ï¸ Cleaned up file: {new_spider_file_path}")
            except Exception as cleanup_error:
                print(f"âš ï¸ Failed to cleanup file: {cleanup_error}")

        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to copy spider: {str(e)}"
        )

@router.post(
    "/check-all-indentation",
    summary="å…¨ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯",
    description="ã™ã¹ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦è‡ªå‹•ä¿®æ­£ã—ã¾ã™ã€‚"
)
async def check_all_spider_indentation(
    auto_fix: bool = False,
    db: Session = Depends(get_db)
):
    """å…¨ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•ä¿®æ­£"""

    try:
        scrapy_service = ScrapyPlaywrightService()
        results = {
            "total_spiders": 0,
            "checked_spiders": 0,
            "spiders_with_issues": 0,
            "spiders_fixed": 0,
            "details": []
        }

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰ã™ã¹ã¦ã®ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å–å¾—
        all_spiders = db.query(DBSpider).all()
        results["total_spiders"] = len(all_spiders)

        for spider in all_spiders:
            try:
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±ã‚’å–å¾—
                project = db.query(DBProject).filter(DBProject.id == spider.project_id).first()
                if not project:
                    continue

                # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’æ§‹ç¯‰
                spider_file_path = scrapy_service.base_projects_dir / project.path / project.path / "spiders" / f"{spider.name}.py"

                spider_result = {
                    "spider_name": spider.name,
                    "project_name": project.name,
                    "file_path": str(spider_file_path),
                    "exists": spider_file_path.exists(),
                    "issues_found": [],
                    "fixes_applied": [],
                    "status": "checked"
                }

                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹å ´åˆã®ã¿ãƒã‚§ãƒƒã‚¯
                if spider_file_path.exists():
                    try:
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã‚³ãƒ¼ãƒ‰ã‚’èª­ã¿å–ã‚Š
                        with open(spider_file_path, 'r', encoding='utf-8') as f:
                            file_code = f.read()

                        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨è‡ªå‹•ä¿®æ­£ã‚’å®Ÿè¡Œ
                        validation_result = validate_spider_inheritance(file_code, auto_fix=auto_fix)

                        spider_result["issues_found"] = validation_result["errors"] + validation_result["warnings"]
                        spider_result["fixes_applied"] = validation_result["fixes_applied"]

                        if validation_result["errors"] or validation_result["warnings"]:
                            results["spiders_with_issues"] += 1

                        # è‡ªå‹•ä¿®æ­£ãŒé©ç”¨ã•ã‚ŒãŸå ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
                        if auto_fix and validation_result["fixes_applied"]:
                            try:
                                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ä¿å­˜
                                scrapy_service.save_spider_code(project.path, spider.name, validation_result["fixed_code"])

                                # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚‚æ›´æ–°
                                spider.code = validation_result["fixed_code"]
                                db.commit()

                                results["spiders_fixed"] += 1
                                spider_result["status"] = "fixed"

                                print(f"âœ… Fixed spider: {spider.name} ({len(validation_result['fixes_applied'])} fixes)")

                            except Exception as save_error:
                                spider_result["status"] = "fix_failed"
                                spider_result["error"] = f"Failed to save fixes: {str(save_error)}"
                                print(f"âŒ Failed to save fixes for {spider.name}: {save_error}")

                    except Exception as read_error:
                        spider_result["status"] = "read_error"
                        spider_result["error"] = f"Failed to read file: {str(read_error)}"
                        print(f"âŒ Failed to read {spider.name}: {read_error}")

                else:
                    spider_result["status"] = "file_not_found"
                    spider_result["error"] = "Spider file not found in filesystem"

                results["details"].append(spider_result)
                results["checked_spiders"] += 1

            except Exception as spider_error:
                print(f"âŒ Error processing spider {spider.name}: {spider_error}")
                results["details"].append({
                    "spider_name": spider.name,
                    "status": "error",
                    "error": str(spider_error)
                })

        # ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
        print(f"ğŸ“Š Indentation Check Summary:")
        print(f"   Total spiders: {results['total_spiders']}")
        print(f"   Checked: {results['checked_spiders']}")
        print(f"   With issues: {results['spiders_with_issues']}")
        print(f"   Fixed: {results['spiders_fixed']}")

        return results

    except Exception as e:
        print(f"âŒ Error in check_all_spider_indentation: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to check spider indentation: {str(e)}"
        )

@router.post("/{spider_id}/run-with-watchdog")
async def run_spider_with_watchdog(
    spider_id: str,
    request: RunSpiderWithWatchdogRequest,
    project_id: str = Query(..., description="Project ID"),
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """watchdogç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ"""
    try:
        print(f"ğŸš€ watchdogç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {spider_id}")
        print(f"ğŸ“‹ Project ID: {project_id}")
        print(f"ğŸ‘¤ User: {current_user.email}")
        print(f"ğŸ“¦ Request data: {request}")
        print(f"âš™ï¸ Settings: {request.settings}")

        # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã®å­˜åœ¨ç¢ºèª
        spider = db.query(DBSpider).filter(
            DBSpider.id == spider_id,
            DBSpider.project_id == project_id,
            DBSpider.user_id == current_user.id
        ).first()

        if not spider:
            raise HTTPException(status_code=404, detail="Spider not found")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
        project = db.query(DBProject).filter(
            DBProject.id == project_id,
            DBProject.user_id == current_user.id
        ).first()

        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # watchdogç›£è¦–ä»˜ãã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        # Celeryã‚¿ã‚¹ã‚¯ã®ä»£ã‚ã‚Šã«ç›´æ¥å®Ÿè¡Œï¼ˆä¸€æ™‚çš„ãªè§£æ±ºç­–ï¼‰
        import uuid
        import asyncio
        from ..services.scrapy_watchdog_monitor import ScrapyWatchdogMonitor
        from pathlib import Path

        # ã‚¿ã‚¹ã‚¯IDã‚’ç”Ÿæˆ
        task_id = str(uuid.uuid4())

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        scrapy_service = ScrapyPlaywrightService()
        project_path = scrapy_service.base_projects_dir / project.path

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã§æŒ‡å®š
        import os
        # backend/app/api/spiders.py ã‹ã‚‰ backend/database/scrapy_ui.db ã¸ã®ãƒ‘ã‚¹
        current_file = os.path.abspath(__file__)  # backend/app/api/spiders.py
        app_dir = os.path.dirname(current_file)  # backend/app/api
        backend_dir = os.path.dirname(os.path.dirname(app_dir))  # backend/app -> backend
        db_path = os.path.join(backend_dir, "database", "scrapy_ui.db")

        # watchdogç›£è¦–ã‚¯ãƒ©ã‚¹ã‚’ä½œæˆ
        monitor = ScrapyWatchdogMonitor(
            task_id=task_id,
            project_path=str(project_path),
            spider_name=spider.name,
            db_path=db_path
        )

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã§å®Ÿè¡Œé–‹å§‹
        import threading
        def run_spider_background():
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                result = loop.run_until_complete(
                    monitor.execute_spider_with_monitoring(request.settings)
                )
                print(f"âœ… Spider execution completed: {result}")
            except Exception as e:
                print(f"âŒ Background spider execution error: {e}")
            finally:
                loop.close()

        # ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã§å®Ÿè¡Œ
        thread = threading.Thread(target=run_spider_background)
        thread.daemon = True
        thread.start()

        return {
            "task_id": task_id,
            "celery_task_id": task_id,
            "status": "started_with_watchdog",
            "monitoring": "jsonl_file_watchdog",
            "spider_name": spider.name,
            "project_name": project.name,
            "message": f"Spider {spider.name} started with watchdog monitoring"
        }

    except Exception as e:
        print(f"âŒ Error running spider with watchdog: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/commands/available")
async def get_available_commands(
    project_id: str,
    db: Session = Depends(get_db),
    current_user = Depends(get_current_active_user)
):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§åˆ©ç”¨å¯èƒ½ãªScrapyã‚³ãƒãƒ³ãƒ‰ã‚’å–å¾—"""
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å­˜åœ¨ç¢ºèª
        project = db.query(DBProject).filter(
            DBProject.id == project_id,
            DBProject.user_id == current_user.id
        ).first()

        if not project:
            raise HTTPException(status_code=404, detail="Project not found")

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’æ§‹ç¯‰
        from pathlib import Path
        scrapy_service = ScrapyPlaywrightService()
        project_path = scrapy_service.base_projects_dir / project.path
        commands_dir = project_path / project.path / "commands"

        available_commands = {
            "standard_commands": [
                {
                    "name": "crawl",
                    "description": "Run a spider",
                    "usage": "scrapy crawl <spider_name>",
                    "watchdog_support": False
                }
            ],
            "custom_commands": [],
            "watchdog_available": False
        }

        # watchdogãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ç¢ºèª
        try:
            import watchdog
            available_commands["watchdog_available"] = True
        except ImportError:
            available_commands["watchdog_available"] = False

        # ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒãƒ³ãƒ‰ã®ç¢ºèª
        if commands_dir.exists():
            crawlwithwatchdog_file = commands_dir / "crawlwithwatchdog.py"
            if crawlwithwatchdog_file.exists():
                available_commands["custom_commands"].append({
                    "name": "crawlwithwatchdog",
                    "description": "Run a spider with watchdog monitoring for real-time DB insertion",
                    "usage": "scrapy crawlwithwatchdog <spider_name> -o results.jsonl --task-id=<task_id>",
                    "watchdog_support": True,
                    "file_path": str(crawlwithwatchdog_file),
                    "requirements": ["watchdog"] if not available_commands["watchdog_available"] else []
                })

        return available_commands

    except Exception as e:
        print(f"âŒ Error getting available commands: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))
