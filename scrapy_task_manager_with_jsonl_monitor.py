#!/usr/bin/env python3
"""
ScrapyTaskManagerã«JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ã‚’çµ±åˆã—ãŸä¾‹
"""
import asyncio
import subprocess
import json
import sqlite3
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Callable
import uuid

class ScrapyTaskManagerWithJSONLMonitor:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ä»˜ãScrapyTaskManager"""
    
    def __init__(self, 
                 task_id: str,
                 spider_config: Dict[str, Any],
                 progress_callback: Optional[Callable] = None,
                 websocket_callback: Optional[Callable] = None):
        self.task_id = task_id
        self.spider_config = spider_config
        self.progress_callback = progress_callback
        self.websocket_callback = websocket_callback
        
        # JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ç”¨
        self.jsonl_monitor = None
        self.processed_items = 0
        self.is_monitoring = False
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¨­å®š
        self.db_path = "backend/database/scrapy_ui.db"
    
    async def execute(self) -> Dict[str, Any]:
        """ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œã—ã¦JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–"""
        try:
            print(f"ğŸš€ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œé–‹å§‹: {self.spider_config['spider_name']}")
            
            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
            project_path = Path(self.spider_config['project_path'])
            jsonl_file = project_path / f"results_{self.task_id}.jsonl"
            
            # JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹
            monitor_task = asyncio.create_task(
                self._start_jsonl_monitoring(str(jsonl_file))
            )
            
            # Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚’é–‹å§‹
            spider_task = asyncio.create_task(
                self._run_scrapy_process(project_path, jsonl_file)
            )
            
            # ä¸¡æ–¹ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦è¡Œå®Ÿè¡Œ
            spider_result, _ = await asyncio.gather(
                spider_task,
                monitor_task,
                return_exceptions=True
            )
            
            # ç›£è¦–åœæ­¢
            self.is_monitoring = False
            
            return {
                'success': True,
                'task_id': self.task_id,
                'items_processed': self.processed_items,
                'spider_result': spider_result
            }
            
        except Exception as e:
            self.is_monitoring = False
            return {
                'success': False,
                'task_id': self.task_id,
                'error': str(e)
            }
    
    async def _run_scrapy_process(self, project_path: Path, jsonl_file: Path) -> Dict[str, Any]:
        """Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚’å®Ÿè¡Œ"""
        try:
            spider_name = self.spider_config['spider_name']
            
            # Scrapyã‚³ãƒãƒ³ãƒ‰ã‚’æ§‹ç¯‰
            cmd = [
                'python', '-m', 'scrapy', 'crawl', spider_name,
                '-o', str(jsonl_file),  # JSONLãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                '-s', 'FEED_FORMAT=jsonlines',  # JSONLå½¢å¼æŒ‡å®š
                '-s', 'LOG_LEVEL=INFO'
            ]
            
            print(f"ğŸ“‹ Scrapyã‚³ãƒãƒ³ãƒ‰: {' '.join(cmd)}")
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            process = await asyncio.create_subprocess_exec(
                *cmd,
                cwd=str(project_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            # ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã‚’å¾…æ©Ÿ
            stdout, stderr = await process.communicate()
            
            result = {
                'return_code': process.returncode,
                'stdout': stdout.decode('utf-8'),
                'stderr': stderr.decode('utf-8')
            }
            
            if process.returncode == 0:
                print(f"âœ… Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Œäº†")
            else:
                print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {result['stderr']}")
            
            return result
            
        except Exception as e:
            print(f"âŒ Scrapyãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    async def _start_jsonl_monitoring(self, jsonl_file_path: str):
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ã‚’é–‹å§‹"""
        self.is_monitoring = True
        jsonl_path = Path(jsonl_file_path)
        
        print(f"ğŸ” JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–é–‹å§‹: {jsonl_path}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
        while not jsonl_path.exists() and self.is_monitoring:
            await asyncio.sleep(0.5)
        
        if not self.is_monitoring:
            return
        
        # tail -fæ–¹å¼ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›£è¦–
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ«å°¾ã«ç§»å‹•
            f.seek(0, 2)
            
            while self.is_monitoring:
                line = f.readline()
                if line:
                    line = line.strip()
                    if line:
                        await self._process_jsonl_line(line)
                else:
                    await asyncio.sleep(0.1)
        
        print(f"ğŸ›‘ JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–çµ‚äº†: å‡¦ç†æ¸ˆã¿ã‚¢ã‚¤ãƒ†ãƒ æ•° {self.processed_items}")
    
    async def _process_jsonl_line(self, json_line: str):
        """JSONLã®1è¡Œã‚’å‡¦ç†ã—ã¦DBã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            # JSONè§£æ
            item_data = json.loads(json_line)
            
            # DBã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            await self._insert_item_to_db(item_data)
            
            # ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
            self.processed_items += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹é€šçŸ¥
            if self.progress_callback:
                await self.progress_callback({
                    'task_id': self.task_id,
                    'items_processed': self.processed_items,
                    'latest_item': item_data
                })
            
            # WebSocketé€šçŸ¥
            if self.websocket_callback:
                await self.websocket_callback({
                    'type': 'item_scraped',
                    'task_id': self.task_id,
                    'item_count': self.processed_items,
                    'item_data': item_data
                })
            
            print(f"ğŸ“ ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†å®Œäº†: {self.processed_items}ä»¶ç›®")
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æã‚¨ãƒ©ãƒ¼: {e}")
        except Exception as e:
            print(f"âŒ ã‚¢ã‚¤ãƒ†ãƒ å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    async def _insert_item_to_db(self, item_data: Dict[str, Any]):
        """ã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ"""
        try:
            # éåŒæœŸã§DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._sync_insert_item, item_data)
            
        except Exception as e:
            print(f"âŒ DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise
    
    def _sync_insert_item(self, item_data: Dict[str, Any]):
        """åŒæœŸçš„ã«DBã‚¤ãƒ³ã‚µãƒ¼ãƒˆï¼ˆã‚¨ã‚°ã‚¼ã‚­ãƒ¥ãƒ¼ã‚¿ãƒ¼ã§å®Ÿè¡Œï¼‰"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # scraped_itemsãƒ†ãƒ¼ãƒ–ãƒ«ã«ã‚¤ãƒ³ã‚µãƒ¼ãƒˆ
            item_id = str(uuid.uuid4())
            cursor.execute("""
                INSERT INTO scraped_items 
                (id, task_id, project_id, spider_name, data, scraped_at, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                item_id,
                self.task_id,
                self.spider_config.get('project_id', 'unknown'),
                self.spider_config['spider_name'],
                json.dumps(item_data, ensure_ascii=False),
                datetime.now().isoformat(),
                datetime.now().isoformat()
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âŒ åŒæœŸDBã‚¤ãƒ³ã‚µãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            raise


# ScrapyServiceã¸ã®çµ±åˆä¾‹
class ScrapyServiceWithJSONLMonitor:
    """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–æ©Ÿèƒ½ä»˜ãScrapyService"""
    
    def __init__(self):
        self.base_projects_dir = Path("scrapy_projects")
    
    async def run_spider_with_jsonl_monitor(self, 
                                          project_path: str, 
                                          spider_name: str, 
                                          task_id: str,
                                          settings: Optional[Dict[str, Any]] = None,
                                          websocket_callback: Optional[Callable] = None) -> Dict[str, Any]:
        """JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ä»˜ãã§ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼ã‚’å®Ÿè¡Œ"""
        
        try:
            print(f"ğŸ¯ JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ä»˜ãã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ: {spider_name}")
            
            # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼è¨­å®š
            spider_config = {
                'project_path': str(self.base_projects_dir / project_path),
                'project_id': project_path,
                'spider_name': spider_name,
                'settings': settings or {}
            }
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            async def progress_callback(progress_data):
                print(f"ğŸ“Š é€²è¡ŒçŠ¶æ³: {progress_data}")
                # ã‚¿ã‚¹ã‚¯ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’DBã«æ›´æ–°
                await self._update_task_progress(task_id, progress_data)
            
            # TaskManagerã‚’ä½œæˆã—ã¦å®Ÿè¡Œ
            task_manager = ScrapyTaskManagerWithJSONLMonitor(
                task_id=task_id,
                spider_config=spider_config,
                progress_callback=progress_callback,
                websocket_callback=websocket_callback
            )
            
            result = await task_manager.execute()
            
            print(f"ğŸ‰ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œå®Œäº†: {result}")
            return result
            
        except Exception as e:
            print(f"âŒ ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'task_id': task_id,
                'error': str(e)
            }
    
    async def _update_task_progress(self, task_id: str, progress_data: Dict[str, Any]):
        """ã‚¿ã‚¹ã‚¯ã®é€²è¡ŒçŠ¶æ³ã‚’DBã«æ›´æ–°"""
        try:
            # éåŒæœŸã§DBæ›´æ–°
            loop = asyncio.get_event_loop()
            await loop.run_in_executor(None, self._sync_update_task, task_id, progress_data)
            
        except Exception as e:
            print(f"âŒ ã‚¿ã‚¹ã‚¯é€²è¡ŒçŠ¶æ³æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
    
    def _sync_update_task(self, task_id: str, progress_data: Dict[str, Any]):
        """åŒæœŸçš„ã«ã‚¿ã‚¹ã‚¯é€²è¡ŒçŠ¶æ³ã‚’æ›´æ–°"""
        try:
            conn = sqlite3.connect("backend/database/scrapy_ui.db")
            cursor = conn.cursor()
            
            cursor.execute("""
                UPDATE tasks 
                SET items_scraped = ?, updated_at = ?
                WHERE id = ?
            """, (
                progress_data.get('items_processed', 0),
                datetime.now().isoformat(),
                task_id
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âŒ åŒæœŸã‚¿ã‚¹ã‚¯æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")


# ä½¿ç”¨ä¾‹
async def example_usage():
    """ä½¿ç”¨ä¾‹"""
    
    print("ğŸ¯ JSONLãƒ•ã‚¡ã‚¤ãƒ«ç›£è¦–ä»˜ãScrapyServiceä½¿ç”¨ä¾‹")
    
    service = ScrapyServiceWithJSONLMonitor()
    
    # WebSocketã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¾‹
    async def websocket_callback(data):
        print(f"ğŸ“¡ WebSocketé€ä¿¡: {data}")
    
    # ã‚¹ãƒ‘ã‚¤ãƒ€ãƒ¼å®Ÿè¡Œ
    result = await service.run_spider_with_jsonl_monitor(
        project_path="test_project",
        spider_name="test_spider",
        task_id="test_task_123",
        settings={'LOG_LEVEL': 'INFO'},
        websocket_callback=websocket_callback
    )
    
    print(f"ğŸ‰ å®Ÿè¡Œçµæœ: {result}")

if __name__ == "__main__":
    asyncio.run(example_usage())
