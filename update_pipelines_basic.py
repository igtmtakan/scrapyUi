#!/usr/bin/env python3
"""
å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ›´æ–°
"""
import sqlite3
import uuid
from pathlib import Path
from datetime import datetime

def update_all_pipelines_to_basic():
    """å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ›´æ–°"""
    
    print("ğŸ”„ å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyåŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°é–‹å§‹\n")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    db_path = Path("backend/database/scrapy_ui.db")
    
    if not db_path.exists():
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {db_path}")
        return False
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—
        cursor.execute("SELECT id, name, path, user_id FROM projects")
        projects = cursor.fetchall()
        
        print(f"ğŸ“Š ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ•°: {len(projects)}ä»¶")
        
        success_count = 0
        error_count = 0
        
        for project_id, project_name, project_path, user_id in projects:
            print(f"\nğŸ”„ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ›´æ–°ä¸­: {project_name}")
            print(f"   ID: {project_id}")
            print(f"   ãƒ‘ã‚¹: {project_path}")
            
            try:
                success = update_single_project_pipeline(cursor, project_id, project_name, project_path, user_id)
                if success:
                    success_count += 1
                    print(f"   âœ… æ›´æ–°æˆåŠŸ")
                else:
                    error_count += 1
                    print(f"   âŒ æ›´æ–°å¤±æ•—")
            except Exception as e:
                error_count += 1
                print(f"   âŒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")
        
        conn.commit()
        
        print(f"\nğŸ‰ æ›´æ–°å®Œäº†!")
        print(f"   æˆåŠŸ: {success_count}ä»¶")
        print(f"   å¤±æ•—: {error_count}ä»¶")
        print(f"   åˆè¨ˆ: {len(projects)}ä»¶")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    finally:
        if 'conn' in locals():
            conn.close()

def update_single_project_pipeline(cursor, project_id: str, project_name: str, project_path: str, user_id: str):
    """å˜ä¸€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyã‚’åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«æ›´æ–°"""
    
    try:
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã‹ã‚‰ã‚¯ãƒ©ã‚¹åã‚’ç”Ÿæˆï¼ˆæœ€åˆã®æ–‡å­—ã‚’å¤§æ–‡å­—ã«ï¼‰
        class_name = project_name.replace('_', ' ').title().replace(' ', '')
        if not class_name.endswith('Pipeline'):
            class_name += 'Pipeline'
        
        # åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®pipelines.pyå†…å®¹ã‚’ç”Ÿæˆ
        basic_pipelines_content = f'''# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class {class_name}:
    def process_item(self, item, spider):
        return item
'''
        
        print(f"     ğŸ“ åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç”Ÿæˆ: {len(basic_pipelines_content)}æ–‡å­—")
        print(f"     ğŸ“ ã‚¯ãƒ©ã‚¹å: {class_name}")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚‚æ›´æ–°
        update_filesystem_pipeline(project_path, basic_pipelines_content)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰æ—¢å­˜ã®pipelines.pyã‚’å‰Šé™¤
        cursor.execute("""
            DELETE FROM project_files 
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))
        
        deleted_count = cursor.rowcount
        if deleted_count > 0:
            print(f"     ğŸ—‘ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {deleted_count}ä»¶")
        
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        file_id = str(uuid.uuid4())
        cursor.execute("""
            INSERT INTO project_files 
            (id, name, path, content, file_type, project_id, user_id, created_at, updated_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            file_id,
            "pipelines.py",
            "pipelines.py",
            basic_pipelines_content,
            "python",
            project_id,
            user_id,
            datetime.now().isoformat(),
            datetime.now().isoformat()
        ))
        
        print(f"     â• æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«è¿½åŠ ")
        
        # æ¤œè¨¼
        cursor.execute("""
            SELECT content FROM project_files 
            WHERE project_id = ? AND name = 'pipelines.py'
        """, (project_id,))
        
        result = cursor.fetchone()
        if result:
            db_content = result[0]
            print(f"     âœ… ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œè¨¼: {len(db_content)}æ–‡å­—")
            
            # åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ãƒã‚§ãƒƒã‚¯
            is_basic = 'ScrapyUIDatabasePipeline' not in db_content and 'def process_item' in db_content
            print(f"     âœ… åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {'ã¯ã„' if is_basic else 'ã„ã„ãˆ'}")
            
            return is_basic
        else:
            print(f"     âŒ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¤œè¨¼å¤±æ•—")
            return False
        
    except Exception as e:
        print(f"     âŒ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def update_filesystem_pipeline(project_path: str, pipelines_content: str):
    """ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®pipelines.pyã‚‚æ›´æ–°"""
    
    try:
        scrapy_projects_dir = Path("scrapy_projects")
        
        # å¯èƒ½ãªãƒ‘ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦ã™
        possible_paths = [
            scrapy_projects_dir / project_path / project_path / "pipelines.py",
            scrapy_projects_dir / project_path / "pipelines.py",
        ]
        
        updated_files = []
        for path in possible_paths:
            if path.exists():
                try:
                    with open(path, 'w', encoding='utf-8') as f:
                        f.write(pipelines_content)
                    updated_files.append(str(path))
                    print(f"     ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°: {path}")
                except Exception as e:
                    print(f"     âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°å¤±æ•—: {path} - {e}")
        
        if not updated_files:
            print(f"     âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«pipelines.pyãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
    except Exception as e:
        print(f"     âš ï¸ ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ æ›´æ–°ã‚¨ãƒ©ãƒ¼: {e}")

def verify_basic_format_results():
    """åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°çµæœã‚’æ¤œè¨¼"""
    
    print("\nğŸ” åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°çµæœæ¤œè¨¼")
    
    db_path = Path("backend/database/scrapy_ui.db")
    
    try:
        conn = sqlite3.connect(str(db_path))
        cursor = conn.cursor()
        
        # pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ã®çµ±è¨ˆ
        cursor.execute("""
            SELECT 
                p.name as project_name,
                CASE WHEN pf.content LIKE '%ScrapyUIDatabasePipeline%' THEN 1 ELSE 0 END as has_scrapy_ui,
                CASE WHEN pf.content LIKE '%def process_item%' THEN 1 ELSE 0 END as has_process_item,
                length(pf.content) as content_length
            FROM projects p
            LEFT JOIN project_files pf ON p.id = pf.project_id AND pf.name = 'pipelines.py'
            ORDER BY p.name
        """)
        
        results = cursor.fetchall()
        
        print(f"\nğŸ“Š pipelines.pyåŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆçµæœ:")
        print(f"{'ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå':<30} {'ScrapyUI':<10} {'process_item':<12} {'ã‚µã‚¤ã‚º':<8} {'çŠ¶æ…‹':<6}")
        print("-" * 75)
        
        basic_count = 0
        total_count = 0
        
        for project_name, has_scrapy_ui, has_process_item, content_length in results:
            scrapy_ui_status = "ã‚ã‚Š" if has_scrapy_ui else "ãªã—"
            process_item_status = "ã‚ã‚Š" if has_process_item else "ãªã—"
            size_str = f"{content_length}æ–‡å­—" if content_length else "ãªã—"
            
            # åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‹ãƒã‚§ãƒƒã‚¯
            is_basic = not has_scrapy_ui and has_process_item
            status = "âœ…" if is_basic else "âŒ"
            
            if is_basic:
                basic_count += 1
            total_count += 1
            
            print(f"{project_name:<30} {scrapy_ui_status:<10} {process_item_status:<12} {size_str:<8} {status:<6}")
        
        print("-" * 75)
        print(f"åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {basic_count}/{total_count}ä»¶ ({basic_count/total_count*100:.1f}%)")
        
        conn.close()
        
        return basic_count == total_count
        
    except Exception as e:
        print(f"âŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
        return False

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    print("ğŸ¯ å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®pipelines.pyåŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°ãƒ„ãƒ¼ãƒ«")
    
    # åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°å®Ÿè¡Œ
    success = update_all_pipelines_to_basic()
    
    if success:
        # çµæœæ¤œè¨¼
        all_basic = verify_basic_format_results()
        
        print("\nğŸ‰ åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°å®Œäº†ï¼")
        
        if all_basic:
            print("\nâœ… å…¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒåŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§ã™")
        else:
            print("\nâš ï¸ ä¸€éƒ¨ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å•é¡ŒãŒã‚ã‚Šã¾ã™")
        
        print("\nğŸ”§ å®Ÿè¡Œå†…å®¹:")
        print("  âœ… ScrapyUIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢é€£ã‚³ãƒ¼ãƒ‰ã‚’å‰Šé™¤")
        print("  âœ… åŸºæœ¬çš„ãªprocess_itemãƒ¡ã‚½ãƒƒãƒ‰ã®ã¿ã«ç°¡ç´ åŒ–")
        print("  âœ… ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®ä¸¡æ–¹ã‚’æ›´æ–°")
        print("  âœ… ã‚¯ãƒ©ã‚¹åã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã«åŸºã¥ã„ã¦ç”Ÿæˆ")
        
        print("\nğŸŒ WebUIç¢ºèª:")
        print("  1. http://localhost:4000 ã«ã‚¢ã‚¯ã‚»ã‚¹")
        print("  2. ä»»æ„ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’é¸æŠ")
        print("  3. pipelines.pyãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã")
        print("  4. ã‚·ãƒ³ãƒ—ãƒ«ãªåŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª")
        
        print("\nğŸ“ æœŸå¾…ã•ã‚Œã‚‹çµæœ:")
        print("  - ScrapyUIãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–¢é€£ã‚³ãƒ¼ãƒ‰ãªã—")
        print("  - åŸºæœ¬çš„ãªprocess_itemãƒ¡ã‚½ãƒƒãƒ‰ã®ã¿")
        print("  - ã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„æ§‹é€ ")
    else:
        print("\nâŒ åŸºæœ¬ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ›´æ–°å¤±æ•—")

if __name__ == "__main__":
    main()
