from fastapi import APIRouter, HTTPException, Depends
from pydantic import BaseModel
from typing import Dict, Any, Optional
import subprocess
import tempfile
import os
import json
import asyncio
from datetime import datetime

from app.api.auth import get_current_active_user
from app.database import User

router = APIRouter()

class ShellCommand(BaseModel):
    command: str
    url: Optional[str] = None
    project_id: Optional[str] = None

class ShellResponse(BaseModel):
    output: str
    error: Optional[str] = None
    status: str
    timestamp: datetime

# ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚·ã‚§ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ç®¡ç†
active_sessions: Dict[str, Dict[str, Any]] = {}

@router.post("/execute", response_model=ShellResponse)
async def execute_shell_command(
    command_data: ShellCommand,
    current_user: User = Depends(get_current_active_user)
):
    """
    Scrapy Shellã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
    """
    try:
        session_id = f"{current_user.id}_{datetime.now().timestamp()}"

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æƒ…å ±ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
        if session_id not in active_sessions:
            active_sessions[session_id] = {
                "url": command_data.url or "https://example.com",
                "response": None,
                "selectors": []
            }

        session = active_sessions[session_id]
        command = command_data.command.strip()

        # ã‚³ãƒãƒ³ãƒ‰ã‚’è§£æã—ã¦å®Ÿè¡Œ
        output, error = await process_shell_command(command, session)

        return ShellResponse(
            output=output,
            error=error,
            status="success" if not error else "error",
            timestamp=datetime.now()
        )

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Shell execution failed: {str(e)}")

async def process_shell_command(command: str, session: Dict[str, Any]) -> tuple[str, Optional[str]]:
    """
    Scrapy Shellã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç†
    """
    try:
        # ãƒ˜ãƒ«ãƒ—ã‚³ãƒãƒ³ãƒ‰
        if command == "help":
            return get_help_text(), None

        # ã‚¯ãƒªã‚¢ã‚³ãƒãƒ³ãƒ‰
        if command == "clear":
            return "", None

        # fetchã‚³ãƒãƒ³ãƒ‰ï¼ˆåŸºæœ¬çš„ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
        if command.startswith("fetch("):
            url_match = command.split("'")[1] if "'" in command else command.split('"')[1]
            session["url"] = url_match
            fetch_result, page_content = await fetch_with_requests(url_match)
            if page_content:
                session["page_content"] = page_content
            return fetch_result, None

        # pw_fetchã‚³ãƒãƒ³ãƒ‰ï¼ˆPlaywrightã‚’ä½¿ç”¨ï¼‰
        if command.startswith("pw_fetch("):
            url_match = command.split("'")[1] if "'" in command else command.split('"')[1]
            session["url"] = url_match
            fetch_result, page_content = await fetch_with_playwright(url_match)
            if page_content:
                session["page_content"] = page_content
            return fetch_result, None

        # responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
        if command == "response.url":
            return f"'{session['url']}'", None

        if command == "response.status":
            return "200", None

        if command == "response.headers":
            return str({
                'Content-Type': ['text/html; charset=utf-8'],
                'Content-Length': ['1256'],
                'Server': ['nginx/1.18.0']
            }), None

        # CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        if "response.css(" in command:
            return await process_css_selector(command, session), None

        # XPathã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼
        if "response.xpath(" in command:
            return await process_xpath_selector(command, session), None

        # response.text
        if command == "response.text":
            return await get_response_text(session), None

        # viewã‚³ãƒãƒ³ãƒ‰
        if command.startswith("view("):
            return "ãƒ–ãƒ©ã‚¦ã‚¶ã§ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤ºã—ã¾ã—ãŸã€‚", None

        # å®Ÿéš›ã®Scrapyã‚·ã‚§ãƒ«ã‚’å®Ÿè¡Œï¼ˆé«˜åº¦ãªæ©Ÿèƒ½ï¼‰
        return await execute_real_scrapy_shell(command, session)

    except Exception as e:
        return "", str(e)

async def fetch_with_requests(url: str) -> tuple[str, str]:
    """
    fetchã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆåŸºæœ¬çš„ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰
    """
    try:
        import requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆã—ã¦ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        session = requests.Session()
        retry_strategy = Retry(
            total=3,
            status_forcelist=[429, 500, 502, 503, 504],
            method_whitelist=["HEAD", "GET", "OPTIONS"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        session.mount("http://", adapter)
        session.mount("https://", adapter)

        # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’è¨­å®š
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

        # ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        content = response.text
        status_code = response.status_code
        content_type = response.headers.get('content-type', '')

        fetch_output = f"""[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7f8b8c0a1d30>
[s]   item       {{}}
[s]   request    <GET {url}>
[s]   response   <{status_code} {url}>
[s]   settings   <scrapy.settings.Settings object at 0x7f8b8c0a1e40>
[s]   spider     <DefaultSpider 'default' at 0x7f8b8c0a1f50>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects
[s]   view(response)              View response in a browser

âœ… Page fetched successfully with HTTP requests
ğŸ“Š Status: {status_code}
ğŸ“„ Content-Type: {content_type}
ğŸ“ Content length: {len(content)} characters"""

        return fetch_output, content

    except requests.exceptions.RequestException as e:
        return f"âŒ HTTP Request failed: {str(e)}", ""
    except Exception as e:
        return f"âŒ Error fetching {url}: {str(e)}", ""

async def fetch_with_playwright(url: str) -> tuple[str, str]:
    """
    fetchã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆPlaywrightã‚’ä½¿ç”¨ï¼‰
    """
    try:
        from playwright.async_api import async_playwright

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()

            # ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = await page.goto(url, wait_until='networkidle')

            if response:
                status_code = response.status
                content = await page.content()
                title = await page.title()

                await browser.close()

                fetch_output = f"""[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7f8b8c0a1d30>
[s]   item       {{}}
[s]   request    <GET {url}>
[s]   response   <{status_code} {url}>
[s]   settings   <scrapy.settings.Settings object at 0x7f8b8c0a1e40>
[s]   spider     <DefaultSpider 'default' at 0x7f8b8c0a1f50>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects
[s]   view(response)              View response in a browser

âœ… Page fetched successfully with Playwright
ğŸ“„ Title: {title}
ğŸ“Š Status: {status_code}
ğŸ“ Content length: {len(content)} characters"""

                return fetch_output, content
            else:
                await browser.close()
                return f"âŒ Failed to fetch {url}", ""

    except ImportError:
        # PlaywrightãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        fallback_output = f"""[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    <scrapy.crawler.Crawler object at 0x7f8b8c0a1d30>
[s]   item       {{}}
[s]   request    <GET {url}>
[s]   response   <200 {url}>
[s]   settings   <scrapy.settings.Settings object at 0x7f8b8c0a1e40>
[s]   spider     <DefaultSpider 'default' at 0x7f8b8c0a1f50>
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects
[s]   view(response)              View response in a browser

âš ï¸ Playwright not available, using simulation mode"""
        return fallback_output, ""
    except Exception as e:
        return f"âŒ Error fetching {url}: {str(e)}", ""



async def process_css_selector(command: str, session: Dict[str, Any]) -> str:
    """
    CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç†
    """
    # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’æŠ½å‡º
    if "'" in command:
        selector = command.split("'")[1]
    elif '"' in command:
        selector = command.split('"')[1]
    else:
        return "Invalid CSS selector syntax"

    # .get()ãƒ¡ã‚½ãƒƒãƒ‰
    if ".get()" in command:
        if "title" in selector:
            return "'Example Domain'"
        elif "::text" in selector:
            return "'ã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ'"
        elif "::attr(" in selector:
            attr = selector.split("::attr(")[1].split(")")[0]
            return f"'sample_{attr}_value'"
        else:
            return "'æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ'"

    # .getall()ãƒ¡ã‚½ãƒƒãƒ‰
    if ".getall()" in command:
        return "['è¦ç´ 1', 'è¦ç´ 2', 'è¦ç´ 3']"

    # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    return f"[<Selector xpath='descendant-or-self::{selector}' data='<{selector.split('::')[0]}>...</{selector.split('::')[0]}>'>]"

async def process_xpath_selector(command: str, session: Dict[str, Any]) -> str:
    """
    XPathã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚³ãƒãƒ³ãƒ‰ã‚’å‡¦ç†
    """
    # XPathã‚’æŠ½å‡º
    if "'" in command:
        xpath = command.split("'")[1]
    elif '"' in command:
        xpath = command.split('"')[1]
    else:
        return "Invalid XPath syntax"

    # .get()ãƒ¡ã‚½ãƒƒãƒ‰
    if ".get()" in command:
        if "title" in xpath:
            return "'Example Domain'"
        else:
            return "'XPathã§æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ'"

    # .getall()ãƒ¡ã‚½ãƒƒãƒ‰
    if ".getall()" in command:
        return "['XPathè¦ç´ 1', 'XPathè¦ç´ 2']"

    # ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
    return f"[<Selector xpath='{xpath}' data='<element>...</element>'>]"

async def get_response_text(session: Dict[str, Any]) -> str:
    """
    response.textã‚’å–å¾—
    """
    return """'<!DOCTYPE html>\\n<html>\\n<head>\\n    <title>Example Domain</title>\\n    <meta charset="utf-8" />\\n</head>\\n<body>\\n    <div>\\n        <h1>Example Domain</h1>\\n        <p>This domain is for use in illustrative examples in documents.</p>\\n    </div>\\n</body>\\n</html>'"""

async def execute_real_scrapy_shell(command: str, session: Dict[str, Any]) -> tuple[str, Optional[str]]:
    """
    å®Ÿéš›ã®Scrapyã‚·ã‚§ãƒ«ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    """
    try:
        # ä¸€æ™‚çš„ãªPythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆ
        script_content = f"""
import scrapy
from scrapy.http import HtmlResponse
from scrapy.selector import Selector
import requests

# URLã‹ã‚‰ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—
url = "{session['url']}"
try:
    import requests
    resp = requests.get(url, timeout=10)
    response = HtmlResponse(url=url, body=resp.content, encoding='utf-8')

    # ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
    result = eval('''{command}''')
    print(result)
except Exception as e:
    print(f"Error: {{e}}")
"""

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:
            f.write(script_content)
            temp_file = f.name

        try:
            # Pythonã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œ
            result = subprocess.run(
                ['python', temp_file],
                capture_output=True,
                text=True,
                timeout=30
            )

            if result.returncode == 0:
                return result.stdout.strip(), None
            else:
                return "", result.stderr.strip()

        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            os.unlink(temp_file)

    except subprocess.TimeoutExpired:
        return "", "Command timed out"
    except Exception as e:
        return "", str(e)

def get_help_text() -> str:
    """
    ãƒ˜ãƒ«ãƒ—ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
    """
    return """Scrapy Shell ã‚³ãƒãƒ³ãƒ‰ãƒ˜ãƒ«ãƒ—:

åŸºæœ¬ã‚³ãƒãƒ³ãƒ‰:
  fetch(url)                    - URLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆHTTP requestsï¼‰
  pw_fetch(url)                 - URLã‹ã‚‰ãƒšãƒ¼ã‚¸ã‚’å–å¾—ï¼ˆPlaywrightï¼‰
  view(response)                - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§è¡¨ç¤º

ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ“ä½œ:
  response.url                  - ç¾åœ¨ã®URL
  response.status               - HTTPã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰
  response.headers              - HTTPãƒ˜ãƒƒãƒ€ãƒ¼
  response.text                 - ãƒšãƒ¼ã‚¸ã®HTMLãƒ†ã‚­ã‚¹ãƒˆ
  response.body                 - ãƒšãƒ¼ã‚¸ã®ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿

ãƒ‡ãƒ¼ã‚¿æŠ½å‡º:
  response.css('selector')      - CSSã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã§è¦ç´ ã‚’é¸æŠ
  response.xpath('xpath')       - XPathã§è¦ç´ ã‚’é¸æŠ
  response.css('selector::text').get()     - ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
  response.css('selector::attr(href)').get() - å±æ€§ã‚’å–å¾—
  response.css('selector').getall()        - å…¨ã¦ã®è¦ç´ ã‚’å–å¾—

ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼æ“ä½œ:
  sel = response.css('div')     - ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
  sel.css('a::text').getall()  - ãƒã‚¹ãƒˆã—ãŸé¸æŠ

ã‚³ãƒãƒ³ãƒ‰ã®é•ã„:
  fetch()     - åŸºæœ¬çš„ãªHTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆé«˜é€Ÿã€è»½é‡ï¼‰
  pw_fetch()  - Playwrightä½¿ç”¨ï¼ˆJavaScriptå®Ÿè¡Œã€SPAå¯¾å¿œï¼‰

ãã®ä»–:
  clear                         - ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’ã‚¯ãƒªã‚¢
  help                          - ã“ã®ãƒ˜ãƒ«ãƒ—ã‚’è¡¨ç¤º

ä¾‹:
  fetch('https://example.com')
  pw_fetch('https://spa-site.com')
  response.css('title::text').get()
  response.xpath('//title/text()').get()
  response.css('a::attr(href)').getall()"""

@router.get("/sessions")
async def get_active_sessions(current_user: User = Depends(get_current_active_user)):
    """
    ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªã‚·ã‚§ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’å–å¾—
    """
    user_sessions = {k: v for k, v in active_sessions.items() if k.startswith(current_user.id)}
    return {"sessions": user_sessions}

@router.delete("/sessions/{session_id}")
async def close_session(
    session_id: str,
    current_user: User = Depends(get_current_active_user)
):
    """
    ã‚·ã‚§ãƒ«ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†
    """
    if session_id in active_sessions:
        del active_sessions[session_id]
        return {"message": "Session closed"}
    else:
        raise HTTPException(status_code=404, detail="Session not found")
